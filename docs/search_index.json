[["index.html", "ADPStudy 1 R기초", " ADPStudy tingyuan 2021-10-02 1 R기초 R 기초 "],["데이터-전처리.html", "2 데이터 전처리 2.1 제어문 2.2 데이터 변환 2.3 데이터 결합 및 요약 2.4 패키지를 활용한 데이터 전처리 2.5 결측치 2.6 이상치 인식 2.7 날짜 데이터 전처리", " 2 데이터 전처리 2.1 제어문 2.2 데이터 변환 2.2.1 파생변수 생성 2.2.2 변수 축소 2.2.2.1 주성분분석 2.2.2.1.1 개념 주성분분석이란 데이터에 여러 변수들이 있을 때 서로 상관성이 높은 변수들의 선형결합으로 이루어진 ‘주성분’ 이라는 새로운 변수를 만들어 변수들을 요약하고 축소하는 기법이다. 예를 들어, 변수 x와 z로 y를 예측하고자 할 때 x=3a, z=a+1, y=2x+z와 같은 관계가 성립된다면 굳이 x와 z라는 두 변수를 사용하지 않고, 변수 a로만 y를 예측하는 것이 더 좋을 것이다. 이와 같이 여러 변수의 선형조합으로 만들어진 주성분을 통해 변수들을 축소할 수 있다. 주성분분석을 할 때, 첫 번째 주성분으로 전체 변동을 가장 많이 설명할 수 있도록 하고, 두 번째 주성분으로는 첫 번째 주성분이 설명하지 못하는 나머지 변동을 정보의 손실 없이 가장 많이 설명할 수 있도록 변수들의 선형조합을 만든다. 각 주성분은 서로 독립인 것(상관계수 = 0)을 원칙으로 한다. 2.2.2.1.2 목적 소수의 주성분으로 차원을 축소함. 다중공선성이 존재하는 경우, 상관성이 없는 (적은) 주성분으로 변수들을 축소 주성분분석을 통해 변수 차원을 축소한 후 군집분석을 수행하면 군집화 결과와 연산속도를 개선할 수 있다. 2.2.2.1.3 주성분의 선택 기여율 ’주성분 기여율’을 사용하여 주성분이 데이터를 얼마나 잘 설명하는지 평가함. 주성분 기여율 : 원변수의 총변동 (각 변수들의 분산값 총합) 분의 주성분 변수의 분산으로, 총변동에 대한 주성분의 설명력을 의미한다. 기여율은 1에 가까울수록 적절하고 0에 가까울수록 데이터에 대한 설명력이 떨어진다고 판단한다. 첫번째 주성분부터 차례대로 기여율을 합한 누적 기여율(cumulative proportion)이 85% 이상이 되면 해당 지점까지를 주성분의 수로 결정한다. 아래의 사진에서는 두 번째 주성분까지의 누적 기여율이 약 87%이므로, 주성분의 수를 두 개로 결정한다. 스크리 산점도 (Scree Plot) 주성분을 x축, 각 주성분의 고유값(주성분의 분산)을 y축에 둔 그래프이다. 고유치가 급격히 완만해지는 지점의 바로 전 단계로 주성분의 수를 선택한다. 2.2.2.1.4 R을 이용한 주성분 분석 prcomp(data, center=TRUE, scale.=FALSE, ...) princomp(data, cor=FALSE, scores=TRUE, ...) Q. R에 내장된 USArrests 데이터는 1973년 미국 50개주 100,000명의 인구 당 체포된 세가지 강력범죄수(assault, murder, rape)와 각 주마다 도시에 거주하는 인구의 비율(%)로 구성되어 있다. 주성분 분석을 수행하여 해당 데이터이ㅡ 변수들을 가장 잘 요약하는 주성분을 구하고 해석해 보자. USArrests 데이터는 변수들 간의 척도 차이가 상당히 크기 때문에 상관행렬을 사용하여 분석한다. 특이치 분해를 사용하는 경우 자료 행렬 각 변수의 평균과 제곱의 합이 1로 표준화되었다고 가정할 수 있다. (1) 데이터 확인 및 산점도를 통한 변수 간 상관관계 파악 library(datasets) data(USArrests) head(USArrests) ## Murder Assault UrbanPop Rape ## Alabama 13.2 236 58 21.2 ## Alaska 10.0 263 48 44.5 ## Arizona 8.1 294 80 31.0 ## Arkansas 8.8 190 50 19.5 ## California 9.0 276 91 40.6 ## Colorado 7.9 204 78 38.7 pairs(USArrests, panel=panel.smooth, main=&quot;USArrests data&quot;) (2) 주성분분석 수행 US.prin &lt;- princomp(USArrests, cor=TRUE) summary(US.prin) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 ## Standard deviation 1.5748783 0.9948694 0.5971291 0.41644938 ## Proportion of Variance 0.6200604 0.2474413 0.0891408 0.04335752 ## Cumulative Proportion 0.6200604 0.8675017 0.9566425 1.00000000 주성분 분석 결과에 summary함수를 적용하면 결과에 대한 요약 설명이 나온다. summary 결과로 나오는 Standard deviation은 주성분의 표준편차, Proportion of Variance는 주성분의 기여율, Cumulative Proportion은 누적기여율을 의미한다. 제1주성분과 제2주성분까지의 누적 기여율은 대략 86.8%로 2개의 주성분변수를 활용하여 전체 데이터의 약 86.8%를 설명할 수 있다. plot(US.prin, type=&#39;l&#39;) - 주성분들에 의해 설명되는 변동의 비율은 Screeplot을 통해 시각적으로도 확인이 가능하다. 그래프의 3번재 주성분에서 기울기가 급격하게 줄어드는 형태를 보이므로, 그 이전 주성분인 2번째 주성분까지 선택하는 것이 적절하다. (3) Loading US.prin$loadings ## ## Loadings: ## Comp.1 Comp.2 Comp.3 Comp.4 ## Murder 0.536 0.418 0.341 0.649 ## Assault 0.583 0.188 0.268 -0.743 ## UrbanPop 0.278 -0.873 0.378 0.134 ## Rape 0.543 -0.167 -0.818 ## ## Comp.1 Comp.2 Comp.3 Comp.4 ## SS loadings 1.00 1.00 1.00 1.00 ## Proportion Var 0.25 0.25 0.25 0.25 ## Cumulative Var 0.25 0.50 0.75 1.00 주성분분석 결과의 loading을 통해 주성분계수 즉, 네 개의 변수가 각 주성분에 기여하는 가중치가 제시된다. 제 1주성분은 0.536 * Murder + 0.583 * Assault + 0.278 * UrbanPop + 0.543 * Rape의 선형결합식으로 이루어져 있음을 파악할 수 있다. (4) Scores 주성분분석 결과, 차원축소로 얻어지는 주성분점수는 scores 인자를 통해 확인할 수 있다. 주성분 점수는 주성분들의 선형식을 통해 새롭게 계산된 각 행별 좌표를 나타낸다. head(US.prin$scores) ## Comp.1 Comp.2 Comp.3 Comp.4 ## Alabama 0.9855659 1.1333924 0.44426879 0.156267145 ## Alaska 1.9501378 1.0732133 -2.04000333 -0.438583440 ## Arizona 1.7631635 -0.7459568 -0.05478082 -0.834652924 ## Arkansas -0.1414203 1.1197968 -0.11457369 -0.182810896 ## California 2.5239801 -1.5429340 -0.59855680 -0.341996478 ## Colorado 1.5145629 -0.9875551 -1.09500699 0.001464887 (5) 제 1-2 주성분에 의한 행렬도 biplot(US.prin, scale=0) biplot 함수는 제1주성분과 제2주성분으로 이루어진 좌표평면상에 원데이터 행들의 주성분점수를 산점도의 형태로 나타내고, 각 변수에 대한 주성분계수를 화살표로 시각화하여 그래프로 표현해 준다. 제1주성분의 모든 주성분계수는 양수이므로 가로축(PC1)을 기준으로 모든 변수가 0 이상의 값을 가리키는 축을 나타내고 있다. PC1을 이루는 선형 결합식에서 상대적 부하량의 절대값이 가장 큰 Assault 변수는 가장 수평의 형태를 나타내고 있으며, 상대적 부하량의 절대값이 가장 작은 UrbanPop 변수는 가장 수직에 가까운 형태를 나타내고 있다. 2.2.2.2 요인분석 2.2.2.2.1 개념 여러개의 변수들로 이루어진 데이터에서 변수들 간의 상관관계를 고려하여 서로 유사한 변수들을 묶어 새로운 잠재요인들을 추출해내는 분석방법으로, 변수를 축소하고 데이터를 요약하는데 사용 예를 들어, 시험성적에 대한 데이터가 ’국어, 영어, 중국어, 수학, 물리, 음악, 미술’에 해당하는 7개의 변수로 이루어져 있다고 하자. 이 7개가 아닌 공통의 변수들을 파악해 국어, 영어, 중국어를 (언어능력), 수락, 물리를 (수리능력), 음악, 미술을 (예술적 재능) 등과 같이 새로운 요인들로 구성해낼 때 요인분석을 사용한다. 요인분석을 수행하기 위해서는 변수가 간격척도 혹은 비율척도로 측정되어야 하며, 표본(관측치)의 크기는 100개 이상이 바람직하며 최소 50개 이상이 되어야 한다. 2.2.2.2.2 주성분분석 .vs. 요인분석 주성분분석 요인분석 공통점 * 원데이터를 활용하여 몇개의 새로운 변수를 생성* 변수축소 및 데이터 요약에 사용됨 생성되는 변수의 수 통상적으로 2개 지정된 개수 없음 생성되는 변수의 이름 제1, 2주성분과 같이 표현됨 분석가가 변수의 이름을 지정함 생성되는 변수들의 관계 제1주성분이 가장중요. 2주성분이 그다음 대등한 관계 분석방법의 의미 목표변수를 잘 예측/분류하기 위해 기존 변수들의 선형결합으로 이루어진 몇 개의 주성분을 찾아냄 목표변수를 고려하지 않고 주어진 변수들을 비슷한 성격으로 묶어서 새로운 (잠재)변수를 생성 2.2.2.2.3 요인추출방법 주성분분석 : 변수들로부터 요인을 추출하는 방식으로, 전체분산을 토대로 요인을 추출. 가장 많이 사용 공통요인분석 : 잠재요인으로부터 변수들이 산출된 것으로 보는 방식, 공통분산만을 토대로 요인을 추출. 2.2.2.2.4 요인의 수 결정 고유값을 기준으로 할 때는, 고유값이 1이상인 요인들을 추출한다. 스크리 도표에서 요인의 설명력이 하락하다가 완만한 하락으로 추세가 바뀌기 직전의 요인수를 기준으로 요인을 추출한다. 경우에 따라 추출할 요인의 수를 사전에 정의한 후 요인분석을 수행할 수도 있다. 2.2.2.2.5 R을 이용한 요인분석 R에서 요인추출법으로 주성분분석을 사용할 때는 prcomp 혹은 principal 함수를 활용하며, 요인추출법으로 공통요인분석을 사용할 때는 factanal 함수를 사용한다. factanal(data, factors=n, rotation=&quot;varimax&quot;, scores=&quot;regression&quot;, ...) 인자 설명 data 요인분석을 수행할 숫자형 행렬 혹은 데이터프레임 factors 요인의 개수 지정 rotation 요인 회전방법을 선택 (“varimax”, “promax”, “none”이 있음) scores 요인점수 계산방법을 선택 (“regression”, “Bartlett”가 있음) Q. R의 내장 데이터 swiss는 1888년 경 스위스 내 47개주의 사회 경제적 지표(교육, 농업 종사자 비율 등)와 출산율에 대한 데이터이다. 원활한 분석을 위해 먼저 해당 데이터의 6가지 변수들을 min-max 정규화한 뒤 (2장 3절 표준화와 정규화 참고), 요인분석을 실시하여 변수들을 3개의 요인으로 축소해 보자. (factanal 함수 사용) (1) swiss 데이터 확인 data(swiss) str(swiss) ## &#39;data.frame&#39;: 47 obs. of 6 variables: ## $ Fertility : num 80.2 83.1 92.5 85.8 76.9 76.1 83.8 92.4 82.4 82.9 ... ## $ Agriculture : num 17 45.1 39.7 36.5 43.5 35.3 70.2 67.8 53.3 45.2 ... ## $ Examination : int 15 6 5 12 17 9 16 14 12 16 ... ## $ Education : int 12 9 5 7 15 7 7 8 7 13 ... ## $ Catholic : num 9.96 84.84 93.4 33.77 5.16 ... ## $ Infant.Mortality: num 22.2 22.2 20.2 20.3 20.6 26.6 23.6 24.9 21 24.4 ... (2) 정규화 수행 및 실습용 데이터 생성 Min &lt;- apply(swiss, 2, min) Max &lt;- apply(swiss, 2, max) swiss_fa &lt;- scale(swiss, center=Min, scale=(Max-Min)) head(swiss_fa) ## Fertility Agriculture Examination Education Catholic ## Courtelary 0.7860870 0.1785311 0.35294118 0.21153846 0.07981604 ## Delemont 0.8365217 0.4960452 0.08823529 0.15384615 0.84506898 ## Franches-Mnt 1.0000000 0.4350282 0.05882353 0.07692308 0.93254982 ## Moutier 0.8834783 0.3988701 0.26470588 0.11538462 0.32314768 ## Neuveville 0.7286957 0.4779661 0.41176471 0.26923077 0.03076137 ## Porrentruy 0.7147826 0.3853107 0.17647059 0.11538462 0.90362800 ## Infant.Mortality ## Courtelary 0.7215190 ## Delemont 0.7215190 ## Franches-Mnt 0.5949367 ## Moutier 0.6012658 ## Neuveville 0.6202532 ## Porrentruy 1.0000000 (3) 요인분석 수행 factanal(x = swiss_fa, factors=3) ## ## Call: ## factanal(x = swiss_fa, factors = 3) ## ## Uniquenesses: ## Fertility Agriculture Examination Education ## 0.005 0.286 0.213 0.114 ## Catholic Infant.Mortality ## 0.083 0.743 ## ## Loadings: ## Factor1 Factor2 Factor3 ## Fertility -0.512 0.203 0.832 ## Agriculture -0.774 0.312 -0.129 ## Examination 0.751 -0.423 -0.211 ## Education 0.901 -0.262 ## Catholic -0.186 0.913 0.220 ## Infant.Mortality 0.500 ## ## Factor1 Factor2 Factor3 ## SS loadings 2.273 1.164 1.120 ## Proportion Var 0.379 0.194 0.187 ## Cumulative Var 0.379 0.573 0.759 ## ## The degrees of freedom for the model is 0 and the fit was 1e-04 요인분석 결과의 Proportion Var는 각 요인이 설명하는 분산의 비율이며 Cumulative Var는 요인별 해당값의 누적치이다. 세번째 요인에 대한 Cumulative Var 값이 0.759이므로 세 요인은 전체 데이터 분산의 약 76%를 설명할 수 있다고 해석이 가능한다. 2.2.3 표준화와 정규화 2.2.3.1 표준화 (standardization) 각 개체들이 평균을 기준으로 얼마나 떨어져 있는지를 나타내는 값으로 변환하는 과정을 의미하여, 표준화한 후 특정 범위를 벗어난 데이터를 확인하여 이상치 판별에 활용할 수도 있다. Z-Score 표준화는 각 요소의 값에서 평균을 뺀 후 표준편차로 나누어 수행한다. 변환 후 데이터의 평균은 0, 표준편차는 1의 값을 갖게 된다. scale 함수 혹은 사용자 정의 함수를 이용하여 R에서 구현할 수 있다. scale(data, center=TRUE, scale=TRUE) 인자 설명 data 숫자형벡터 center TRUE이면 데이터에서 해당 벡터의 평균을 뺌 Q. R의 내장 데이터 ’mtcars’의 mpg(마일)변수와 hp(총 마력)변수로만 이루어진 데이터프레임(test, cars)을 생성하고, 각 변수를 표준화한 새로운 변수를 추가해 보자. (mpg를 표준화한 변수의 이름은 mpg_scale, hp를 표준화한 변수의 이름은 hp_scale로 지정) data(&quot;mtcars&quot;) str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... test.cars &lt;- mtcars[,c(&quot;mpg&quot;, &quot;hp&quot;)] head(test.cars) ## mpg hp ## Mazda RX4 21.0 110 ## Mazda RX4 Wag 21.0 110 ## Datsun 710 22.8 93 ## Hornet 4 Drive 21.4 110 ## Hornet Sportabout 18.7 175 ## Valiant 18.1 105 test.cars &lt;- transform(test.cars, mpg_scale=scale(test.cars$mpg), hp_scale=scale(test.cars$hp)) head(test.cars) ## mpg hp mpg_scale hp_scale ## Mazda RX4 21.0 110 0.1508848 -0.5350928 ## Mazda RX4 Wag 21.0 110 0.1508848 -0.5350928 ## Datsun 710 22.8 93 0.4495434 -0.7830405 ## Hornet 4 Drive 21.4 110 0.2172534 -0.5350928 ## Hornet Sportabout 18.7 175 -0.2307345 0.4129422 ## Valiant 18.1 105 -0.3302874 -0.6080186 2.2.3.2 정규화 (Normalization) 정규화란 데이터의 범위를 0과 1사이로 변환하여 데이터의 분포를 조정하는 방법으로, 데이터군 내에서 특정 개체가 가지는 위치를 파악하고 비교할 때 유용하게 사용할 수 있다. 일반적으로 많이 사용되는 min-max 정규화는 ’(xi - minx)/(maxi-minx)’의 공식을 이용하며, scale함수 혹은 사용자 정의 함수를 이용하는 등 다양한 방법을 사용할 수 있다. 2.2.3.3 scale 함수 이용 Min &lt;- min(iris$Sepal.Length) Max &lt;- max(iris$Sepal.Length) iris$SL_new &lt;- scale(iris$Sepal.Length, center=Min, scale=Max-Min) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species SL_new ## 1 5.1 3.5 1.4 0.2 setosa 0.22222222 ## 2 4.9 3.0 1.4 0.2 setosa 0.16666667 ## 3 4.7 3.2 1.3 0.2 setosa 0.11111111 ## 4 4.6 3.1 1.5 0.2 setosa 0.08333333 ## 5 5.0 3.6 1.4 0.2 setosa 0.19444444 ## 6 5.4 3.9 1.7 0.4 setosa 0.30555556 2.3 데이터 결합 및 요약 2.3.1 데이터 결합 2.3.1.1 rbind 2.3.1.2 cbind 2.3.1.3 merge merge는 두 데이터프레임에서 기준이 되는 특정 칼럼의 값이 같은 행끼리 묶어 병합하는 함수이다. 이는 데이터베이스에서 join과 같은 역할을 한다. merge(x, y, by, by.x, by.y, all=FALSE, all.x,) (id_name &lt;- data.frame(id=c(&quot;c01&quot;, &quot;c02&quot;, &quot;c03&quot;, &quot;c04&quot;, &quot;c05&quot;, &quot;c06&quot;, &quot;c07&quot;), last_name=c(&quot;Lee&quot;, &quot;Kim&quot;, &quot;Choi&quot;, &quot;park&quot;, &quot;Lim&quot;, &quot;Bae&quot;, &quot;Kim&quot;))) ## id last_name ## 1 c01 Lee ## 2 c02 Kim ## 3 c03 Choi ## 4 c04 park ## 5 c05 Lim ## 6 c06 Bae ## 7 c07 Kim (id_number &lt;- data.frame(id=c(&quot;c03&quot;, &quot;c04&quot;, &quot;c05&quot;, &quot;c06&quot;, &quot;c07&quot;, &quot;c08&quot;, &quot;c09&quot;), number=c(3, 1, 0, 7, 3, 4, 1))) ## id number ## 1 c03 3 ## 2 c04 1 ## 3 c05 0 ## 4 c06 7 ## 5 c07 3 ## 6 c08 4 ## 7 c09 1 Q-1. Inner Join merge(id_name, id_number, by=&quot;id&quot;) ## id last_name number ## 1 c03 Choi 3 ## 2 c04 park 1 ## 3 c05 Lim 0 ## 4 c06 Bae 7 ## 5 c07 Kim 3 Q-2. Outer Join merge(id_name, id_number, by=&quot;id&quot;, all=TRUE) ## id last_name number ## 1 c01 Lee NA ## 2 c02 Kim NA ## 3 c03 Choi 3 ## 4 c04 park 1 ## 5 c05 Lim 0 ## 6 c06 Bae 7 ## 7 c07 Kim 3 ## 8 c08 &lt;NA&gt; 4 ## 9 c09 &lt;NA&gt; 1 Q-3. Left Outer Join merge(id_name, id_number, by=&quot;id&quot;, all.x=TRUE) ## id last_name number ## 1 c01 Lee NA ## 2 c02 Kim NA ## 3 c03 Choi 3 ## 4 c04 park 1 ## 5 c05 Lim 0 ## 6 c06 Bae 7 ## 7 c07 Kim 3 Q-4. Right Outer Join merge(id_name, id_number, by=&quot;id&quot;, all.y=TRUE) ## id last_name number ## 1 c03 Choi 3 ## 2 c04 park 1 ## 3 c05 Lim 0 ## 4 c06 Bae 7 ## 5 c07 Kim 3 ## 6 c08 &lt;NA&gt; 4 ## 7 c09 &lt;NA&gt; 1 2.3.2 데이터 요약 2.3.2.1 aggregate aggregate(x, by, FUN) aggregate(formula, data, FUN) Q1. iris 데이터에서 종별 Sepal.width의 평균을 구해보자. aggregate(Sepal.Width~Species, iris, mean) ## Species Sepal.Width ## 1 setosa 3.428 ## 2 versicolor 2.770 ## 3 virginica 2.974 Q2. iris 데이터에서 종별 Sepal.Width와 Petal.Width의 평균을 구해보자. aggregate(cbind(Sepal.Width, Petal.Width)~Species, iris, mean) ## Species Sepal.Width Petal.Width ## 1 setosa 3.428 0.246 ## 2 versicolor 2.770 1.326 ## 3 virginica 2.974 2.026 2.3.2.2 table Q1. 내장 데이터 Titanic은 타이타닉호 탑승자들의 특성에 따른 생존여부를 기록해놓은 데이터이다. Titanic 데이터에서 좌석등급을 의미하는 Class 변수에 대해서 도수분포표를 생성해 보자. 내장데이터 Titanic의 구조 확인 str(Titanic) ## &#39;table&#39; num [1:4, 1:2, 1:2, 1:2] 0 0 35 0 0 0 17 0 118 154 ... ## - attr(*, &quot;dimnames&quot;)=List of 4 ## ..$ Class : chr [1:4] &quot;1st&quot; &quot;2nd&quot; &quot;3rd&quot; &quot;Crew&quot; ## ..$ Sex : chr [1:2] &quot;Male&quot; &quot;Female&quot; ## ..$ Age : chr [1:2] &quot;Child&quot; &quot;Adult&quot; ## ..$ Survived: chr [1:2] &quot;No&quot; &quot;Yes&quot; 데이터프레임으로 변환한 뒤 다시 구조를 확인 Titanic&lt;-as.data.frame(Titanic) str(Titanic) ## &#39;data.frame&#39;: 32 obs. of 5 variables: ## $ Class : Factor w/ 4 levels &quot;1st&quot;,&quot;2nd&quot;,&quot;3rd&quot;,..: 1 2 3 4 1 2 3 4 1 2 ... ## $ Sex : Factor w/ 2 levels &quot;Male&quot;,&quot;Female&quot;: 1 1 1 1 2 2 2 2 1 1 ... ## $ Age : Factor w/ 2 levels &quot;Child&quot;,&quot;Adult&quot;: 1 1 1 1 1 1 1 1 2 2 ... ## $ Survived: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Freq : num 0 0 35 0 0 0 17 0 118 154 ... table 함수를 이용하여 범주형 변수 Class에 대한 도수분포표를 생성 table(Titanic$Class) ## ## 1st 2nd 3rd Crew ## 8 8 8 8 Q2. 내장데이터 Titanic에서 Survived 변수는 승객의 생존여부를 의미한다. 좌석등급과 생존여부의 관계를 살펴보기 위해 Class 변수에 따른 Survived 변수의 도수를 표 형태로 나타내 보자. table(Titanic$Class, Titanic$Survived) ## ## No Yes ## 1st 4 4 ## 2nd 4 4 ## 3rd 4 4 ## Crew 4 4 2.3.2.3 prop.table prop.table(table) prop.table(table, 1) prop.table(table, 2) Q. Titanic 데이터에서 Age 변수는 해당 승객이 어른인지 아이인지의 여부를 나타낸다. Age 변수에 따른 생존여부의 관계를 전체에 대한 비율, 행별 비율, 열별 비율로 살펴보자. prop.table(table(Titanic$Age, Titanic$Survived)) ## ## No Yes ## Child 0.25 0.25 ## Adult 0.25 0.25 prop.table(table(Titanic$Age, Titanic$Survived), 1) ## ## No Yes ## Child 0.5 0.5 ## Adult 0.5 0.5 prop.table(table(Titanic$Age, Titanic$Survived), 2) ## ## No Yes ## Child 0.5 0.5 ## Adult 0.5 0.5 2.3.3 apply 계열 함수 2.3.3.1 apply apply는 데이터의 행 혹은 열 방향으로 주어진 함수를 한번에 적용한 뒤 그 결과를 벡터, 배열, 리스트로 반환하는 함수이다. apply(X, MARGIN, FUN) a &lt;- matrix(1:12, nrow=4, ncol=3) apply(a, 1, max) ## [1] 9 10 11 12 apply(iris[, 1:4], 2, mean) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 5.843333 3.057333 3.758000 1.199333 2.3.3.2 lapply lapply(X, FUN, ...) a &lt;- c(1, 2, 3) lapply(a, FUN=function(x){x ^ 2}) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 4 ## ## [[3]] ## [1] 9 class(lapply(a, FUN=function(x){x^2})) ## [1] &quot;list&quot; b&lt;- lapply(a, FUN=function(x){x^2}) unlist(b) ## [1] 1 4 9 2.3.3.3 sapply sapply(X, FUN, simplify=TRUE, ...) Q1. iris 데이터에서 각 칼럼별 데이터 타입을 구해보자. sapply(iris, class) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species SL_new ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;factor&quot; &quot;matrix&quot; Q2. iris 데이터에서 각 칼럼에 summary 함수를 적용해 보자. sapply(iris, summary) ## $Sepal.Length ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.300 5.100 5.800 5.843 6.400 7.900 ## ## $Sepal.Width ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.000 2.800 3.000 3.057 3.300 4.400 ## ## $Petal.Length ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.600 4.350 3.758 5.100 6.900 ## ## $Petal.Width ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.100 0.300 1.300 1.199 1.800 2.500 ## ## $Species ## setosa versicolor virginica ## 50 50 50 ## ## $SL_new ## V1 ## Min. :0.0000 ## 1st Qu.:0.2222 ## Median :0.4167 ## Mean :0.4287 ## 3rd Qu.:0.5833 ## Max. :1.0000 2.3.3.4 vapply vapply(X, FUN, FUN.VALUE, ...) Q. 1~100까지의 숫자가 저장된 리스트에 fivenum 함수를 적용한 후, 각 값에 이름을 부여하여 리스트 형태로 출력해 보자. test &lt;- c(1:100) fivenum(test) ## [1] 1.0 25.5 50.5 75.5 100.0 test &lt;- list(test) (test2 &lt;- vapply(test, fivenum, c(&quot;Min&quot;=0, &quot;Q1&quot;=0, &quot;Median&quot;=0, &quot;Q3&quot;=0, &quot;Max&quot;=0))) ## [,1] ## Min 1.0 ## Q1 25.5 ## Median 50.5 ## Q3 75.5 ## Max 100.0 2.3.3.5 mapply mapply(FUN, arg1, arg2, ..., argn, ...) Q. 1을 4번, 2를 3번, 3을 2번, 4를 1번 반복하는 4개의 수열을 구해보자. 이 때 rep 함수를 이용할 때와 mapply 함수를 이용 할 때를 비교해 보자. mapply(rep, c(1:4), c(4:1)) ## [[1]] ## [1] 1 1 1 1 ## ## [[2]] ## [1] 2 2 2 ## ## [[3]] ## [1] 3 3 ## ## [[4]] ## [1] 4 2.3.3.6 tapply tapply 함수를 이용하면 데이터를 특정 기준에 따라 그룹으로 나눈 뒤 각 그룹별로 함수를 적용하여 그 결과를 반환할 수 있다. tapply(DATA, INDEX, FUN, ...) Q1. R의 googleVis 패키지에 있는 Fruits 데이터에서 과일종류별 판매량의 평균을 구해보자. install.packages(setdiff(&quot;googleVis&quot;, rownames(installed.packages()))) library(googleVis) ## Creating a generic function for &#39;toJSON&#39; from package &#39;jsonlite&#39; in package &#39;googleVis&#39; ## ## Welcome to googleVis version 0.6.10 ## ## Please read Google&#39;s Terms of Use ## before you start using the package: ## https://developers.google.com/terms/ ## ## Note, the plot method of googleVis will by default use ## the standard browser to display its output. ## ## See the googleVis package vignettes for more details, ## or visit https://github.com/mages/googleVis. ## ## To suppress this message use: ## suppressPackageStartupMessages(library(googleVis)) head(Fruits) ## Fruit Year Location Sales Expenses Profit Date ## 1 Apples 2008 West 98 78 20 2008-12-31 ## 2 Apples 2009 West 111 79 32 2009-12-31 ## 3 Apples 2010 West 89 76 13 2010-12-31 ## 4 Oranges 2008 East 96 81 15 2008-12-31 ## 5 Bananas 2008 East 85 76 9 2008-12-31 ## 6 Oranges 2009 East 93 80 13 2009-12-31 tapply(Fruits$Sales, Fruits$Fruit, mean) ## Apples Bananas Oranges ## 99.33333 86.66667 95.66667 Q2. Fruits 데이터에서 Location이 West인 것과 아닌 것으로 그룹을 지정하여 Profit의 평균을 구해보자. tapply(Fruits$Profit, Fruits$Location==&quot;West&quot;, mean) ## FALSE TRUE ## 11.66667 21.66667 2.4 패키지를 활용한 데이터 전처리 2.4.1 plyr 2.4.1.1 plyr 패키지 plyr 패키지의 함수들은 데이터를 분할(split)한 뒤 원하는 방향으로 특정 함수를 적용하고 (apply), 그 결과를 재조합 (combine) 하여 반환해 준다. 2.4.1.2 adply adply(data, margins, fun) Q. R의 iris 데이터에서 Petal.Length 변수가 1.5 미만이면서 Species 변수 값이 ’setosa’인 조건을 만족하는 경우 ’1’을 그렇지 않은 경우 ’0’을 부여한 칼럼을 생성하여, 원래의 iris 데이터와 함께 데이터프레임 형태로 출력해 보자. install.packages(setdiff(&quot;plyr&quot;, rownames(installed.packages()))) library(plyr) head(adply(iris, 1, function(row) {ifelse(row$Petal.Length &lt; 1.5 &amp; row$Species == &quot;setosa&quot;, &quot;1&quot;, &quot;0&quot;)})) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species SL_new V1 ## 1 5.1 3.5 1.4 0.2 setosa 0.22222222 1 ## 2 4.9 3.0 1.4 0.2 setosa 0.16666667 1 ## 3 4.7 3.2 1.3 0.2 setosa 0.11111111 1 ## 4 4.6 3.1 1.5 0.2 setosa 0.08333333 0 ## 5 5.0 3.6 1.4 0.2 setosa 0.19444444 1 ## 6 5.4 3.9 1.7 0.4 setosa 0.30555556 0 2.4.1.3 ddply ddply(data, .variables, ddply-func, fun) Q1. iris 데이터에서 Species별로 나머지 네 개 변수의 평균을 출력해 보자. ddply(iris, .(Species), function(sub) { data.frame( mean_SL = mean(sub$Sepal.Length), mean_SW = mean(sub$Sepal.Width), mean_PL = mean(sub$Petal.Length), mean_PW = mean(sub$Petal.Width) ) }) ## Species mean_SL mean_SW mean_PL mean_PW ## 1 setosa 5.006 3.428 1.462 0.246 ## 2 versicolor 5.936 2.770 4.260 1.326 ## 3 virginica 6.588 2.974 5.552 2.026 Q2. iris 데이터에서 Species와 Petal.Length가 1.5 미만인지 여부로 데이터를 그룹지어 네 개 변수의 평균을 출력해 보자. ddply(iris, .(Species, Petal.Length&lt;1.5), function(sub) { data.frame( mean_SL = mean(sub$Sepal.Length), mean_SW = mean(sub$Sepal.Width), mean_PL = mean(sub$Petal.Length), mean_PW = mean(sub$Petal.Width)) }) ## Species Petal.Length &lt; 1.5 mean_SL mean_SW mean_PL mean_PW ## 1 setosa FALSE 5.107692 3.515385 1.588462 0.2730769 ## 2 setosa TRUE 4.895833 3.333333 1.325000 0.2166667 ## 3 versicolor FALSE 5.936000 2.770000 4.260000 1.3260000 ## 4 virginica FALSE 6.588000 2.974000 5.552000 2.0260000 2.4.1.3.1 transform head(ddply(baseball, .(id), transform, avgG=sum(g)/length(year))) ## id year stint team lg g ab r h X2b X3b hr rbi sb cs bb so ibb ## 1 aaronha01 1954 1 ML1 NL 122 468 58 131 27 6 13 69 2 2 28 39 NA ## 2 aaronha01 1955 1 ML1 NL 153 602 105 189 37 9 27 106 3 1 49 61 5 ## 3 aaronha01 1956 1 ML1 NL 153 609 106 200 34 14 26 92 2 4 37 54 6 ## 4 aaronha01 1957 1 ML1 NL 151 615 118 198 27 6 44 132 1 1 57 58 15 ## 5 aaronha01 1958 1 ML1 NL 153 601 109 196 34 4 30 95 4 1 59 49 16 ## 6 aaronha01 1959 1 ML1 NL 154 629 116 223 46 7 39 123 8 0 51 54 17 ## hbp sh sf gidp avgG ## 1 3 6 4 13 143.3913 ## 2 3 7 4 20 143.3913 ## 3 2 5 7 21 143.3913 ## 4 0 0 3 13 143.3913 ## 5 1 0 3 21 143.3913 ## 6 4 0 9 19 143.3913 2.4.1.3.2 mutate # avgG 칼럼과 avgG_RND 칼럼을 한번에 추가하여 출력. # 이 경우, mutate가 아닌 tansform을 사용하면 에러가 발생함. head(ddply(baseball, .(id), mutate, avgG=sum(g)/length(year), avgG_RND=round(avgG))) ## id year stint team lg g ab r h X2b X3b hr rbi sb cs bb so ibb ## 1 aaronha01 1954 1 ML1 NL 122 468 58 131 27 6 13 69 2 2 28 39 NA ## 2 aaronha01 1955 1 ML1 NL 153 602 105 189 37 9 27 106 3 1 49 61 5 ## 3 aaronha01 1956 1 ML1 NL 153 609 106 200 34 14 26 92 2 4 37 54 6 ## 4 aaronha01 1957 1 ML1 NL 151 615 118 198 27 6 44 132 1 1 57 58 15 ## 5 aaronha01 1958 1 ML1 NL 153 601 109 196 34 4 30 95 4 1 59 49 16 ## 6 aaronha01 1959 1 ML1 NL 154 629 116 223 46 7 39 123 8 0 51 54 17 ## hbp sh sf gidp avgG avgG_RND ## 1 3 6 4 13 143.3913 143 ## 2 3 7 4 20 143.3913 143 ## 3 2 5 7 21 143.3913 143 ## 4 0 0 3 13 143.3913 143 ## 5 1 0 3 21 143.3913 143 ## 6 4 0 9 19 143.3913 143 2.4.1.3.3 summarise head(ddply(baseball, .(id), summarise, year_fin=max(year))) ## id year_fin ## 1 aaronha01 1976 ## 2 abernte02 1972 ## 3 adairje01 1970 ## 4 adamsba01 1926 ## 5 adamsbo03 1959 ## 6 adcocjo01 1966 head(ddply(baseball, .(team), summarise, hr_sum=sum(hr))) ## team hr_sum ## 1 ALT 0 ## 2 ANA 134 ## 3 ARI 809 ## 4 ATL 3272 ## 5 BAL 4243 ## 6 BFN 74 2.4.2 dplyr 2.4.2.1 dplyr 패키지 2.4.2.2 filter dataframe name %&gt;% filter(condition) Q. Cars93 데이터에서 제조사가 “Audi” 혹은 “BMW”이면서, 엔진크기가 2.4 이상인 행들만 추출해 보자. install.packages(setdiff(&quot;dplyr&quot;, rownames(installed.packages()))) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:plyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, ## summarize ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select Cars93 %&gt;% filter((Manufacturer==&quot;Audi&quot;|Manufacturer==&quot;BMW&quot;) &amp; EngineSize &gt;= 2.4) ## Manufacturer Model Type Min.Price Price Max.Price MPG.city MPG.highway ## 1 Audi 90 Compact 25.9 29.1 32.3 20 26 ## 2 Audi 100 Midsize 30.8 37.7 44.6 19 26 ## 3 BMW 535i Midsize 23.7 30.0 36.2 22 30 ## AirBags DriveTrain Cylinders EngineSize Horsepower RPM ## 1 Driver only Front 6 2.8 172 5500 ## 2 Driver &amp; Passenger Front 6 2.8 172 5500 ## 3 Driver only Rear 4 3.5 208 5700 ## Rev.per.mile Man.trans.avail Fuel.tank.capacity Passengers Length Wheelbase ## 1 2280 Yes 16.9 5 180 102 ## 2 2535 Yes 21.1 6 193 106 ## 3 2545 Yes 21.1 4 186 109 ## Width Turn.circle Rear.seat.room Luggage.room Weight Origin Make ## 1 67 37 28 14 3375 non-USA Audi 90 ## 2 70 37 31 17 3405 non-USA Audi 100 ## 3 69 39 27 13 3640 non-USA BMW 535i 2.4.2.3 select Q1. Cars93 데이터의 모델번호, 종류, 가격 변수들만 추출해 보자. # Cars93 %&gt;% select(Model, Type, Price) # Error in select(., Model, Type, Price) : unused arguments (Model, Type, Price) Calls: &lt;Anomymous&gt; ... # withCallingHandlers -&gt; withVisible -&gt; eval -&gt; eval -&gt; %&gt;% Execution halted # 에러 발생 이유 : MASS 패키지의 select()와 dplyr의 select()가 충돌하기 때문 head(Cars93 %&gt;% dplyr::select(Model, Type, Price)) ## Model Type Price ## 1 Integra Small 15.9 ## 2 Legend Midsize 33.9 ## 3 90 Compact 29.1 ## 4 100 Midsize 37.7 ## 5 535i Midsize 30.0 ## 6 Century Midsize 15.7 Q2. 제조사가 “Chevrolet” 혹은 “Volkswagen” 이면서, 가격이 10 이상인 행들의 제조사, 모델, 종류, 가격 변수들만 추출해 보자. Cars93 %&gt;% filter((Manufacturer==&quot;Chevrolet&quot;|Manufacturer==&quot;Volkswagen&quot;) &amp; Price &gt;= 10) %&gt;% dplyr::select(Manufacturer, Model, Type, Price) ## Manufacturer Model Type Price ## 1 Chevrolet Cavalier Compact 13.4 ## 2 Chevrolet Corsica Compact 11.4 ## 3 Chevrolet Camaro Sporty 15.1 ## 4 Chevrolet Lumina Midsize 15.9 ## 5 Chevrolet Lumina_APV Van 16.3 ## 6 Chevrolet Astro Van 16.6 ## 7 Chevrolet Caprice Large 18.8 ## 8 Chevrolet Corvette Sporty 38.0 ## 9 Volkswagen Eurovan Van 19.7 ## 10 Volkswagen Passat Compact 20.0 ## 11 Volkswagen Corrado Sporty 23.3 2.4.2.4 group_by 와 summarise Q1. Cars93 데이터의 제조사별 가격의 평균과 무게의 최댓값을 산출한 뒤 변수명을 각각 mean_Price, max_Weight로 지정하여 출력해 보자. Cars93 %&gt;% group_by(Manufacturer) %&gt;% summarise(mean_Price=mean(Price), max_Weight=max(Weight)) ## # A tibble: 32 x 3 ## Manufacturer mean_Price max_Weight ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Acura 24.9 3560 ## 2 Audi 33.4 3405 ## 3 BMW 30 3640 ## 4 Buick 21.6 4105 ## 5 Cadillac 37.4 3935 ## 6 Chevrolet 18.2 4025 ## 7 Chrylser 18.4 3515 ## 8 Chrysler 22.6 3570 ## 9 Dodge 15.7 3805 ## 10 Eagle 15.8 3490 ## # … with 22 more rows Q2. 종류와 에어백을 기준으로 데이터를 그룹화한 뒤, 자동차 평균 무게를 구해 보자. Cars93 %&gt;% group_by(Type, AirBags) %&gt;% summarise(mean_Weight=mean(Weight)) ## `summarise()` has grouped output by &#39;Type&#39;. You can override using the `.groups` argument. ## # A tibble: 15 x 3 ## # Groups: Type [6] ## Type AirBags mean_Weight ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Compact Driver &amp; Passenger 3062. ## 2 Compact Driver only 2991. ## 3 Compact None 2730 ## 4 Large Driver &amp; Passenger 3639. ## 5 Large Driver only 3728. ## 6 Midsize Driver &amp; Passenger 3554. ## 7 Midsize Driver only 3344. ## 8 Midsize None 3285 ## 9 Small Driver only 2423 ## 10 Small None 2278. ## 11 Sporty Driver &amp; Passenger 3115 ## 12 Sporty Driver only 2939. ## 13 Sporty None 2578. ## 14 Van Driver only 3742. ## 15 Van None 3875 2.4.2.5 mutate mutate는 데이터에 새로운 파생변수를 추가해 주는 함수이며, 사용법은 아래와 같다. Q1. Cars93 데이터에서 가격이 12미만이면 “low”, 12 이상 23 미만이면 “middle”, 23 이상이면 “high” 값을 가지는 Pr_level 변수를 생성한 뒤, 모델, 가격, 새로운 파생변수 Pr_level만 출력해 보자. head( Cars93 %&gt;% mutate(Pr_level=ifelse(Price &lt; 12, &quot;low&quot;, ifelse(Price &gt;= 12 &amp; Price &lt; 23, &quot;middle&quot;, &quot;high&quot;))) %&gt;% dplyr::select(Model, Price, Pr_level) ) ## Model Price Pr_level ## 1 Integra 15.9 middle ## 2 Legend 33.9 high ## 3 90 29.1 high ## 4 100 37.7 high ## 5 535i 30.0 high ## 6 Century 15.7 middle 2.4.2.6 arrange arrange는 특정열 기준으로 데이터를 정렬해주는 함수이며, 사용법은 아래와 같다. Q. Cars93 데이터에서 종류가 “Midsize” 혹은 “Small”인 데이터의 Model, Type, Weight, Price 변수들만 추출한 뒤, 종류별로 Weight 변수값들이 Weight의 중앙값보다 작은 경우는 “low”, 중앙값 이상인 경우 “high” 값을 갖는 Weight_lv 변수를 생성하라. 그리고 Price 변수를 기준으로 데이터를 오름차순으로 정렬하여 출력하라. Cars93 %&gt;% filter(Type %in% c(&quot;Midsize&quot;, &quot;Small&quot;)) %&gt;% dplyr::select(Model, Type, Weight, Price) %&gt;% group_by(Type) %&gt;% mutate(Weight_lv = ifelse(Weight &lt; median(Weight), &quot;low&quot;, &quot;high&quot;)) %&gt;% arrange(Price) ## # A tibble: 43 x 5 ## # Groups: Type [2] ## Model Type Weight Price Weight_lv ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Festiva Small 1845 7.4 low ## 2 Excel Small 2345 8 high ## 3 323 Small 2325 8.3 low ## 4 Metro Small 1695 8.4 low ## 5 Justy Small 2045 8.4 low ## 6 Swift Small 1965 8.6 low ## 7 LeMans Small 2350 9 high ## 8 Fox Small 2240 9.1 low ## 9 Colt Small 2270 9.2 low ## 10 Tercel Small 2055 9.8 low ## # … with 33 more rows 2.4.2.7 {left, right, inner, full}_join Q. 카페에서 판매하는 메뉴 코드, 이름을 담은 데이터 ’NAME’과 메뉴코드 해당 메뉴의 가격을 담은 데이터 ’PRICE’를 생성해보자. 그 후 각 메뉴의 고유코드를 의미하는 code 변수를 기준으로 left, right, inner, full_join을 수행하여 결과를 확인해 보자. NAME&lt;-data.frame(code=c(&quot;A01&quot;, &quot;A02&quot;, &quot;A03&quot;), name=c(&quot;coffee&quot;, &quot;cake&quot;, &quot;cookie&quot;)) PRICE&lt;-data.frame(code=c(&quot;A01&quot;, &quot;A02&quot;, &quot;A04&quot;), price=c(3000, 4000, 3000)) (cafe_left &lt;- left_join(NAME, PRICE, by=&quot;code&quot;)) ## code name price ## 1 A01 coffee 3000 ## 2 A02 cake 4000 ## 3 A03 cookie NA (cafe_right &lt;- right_join(NAME, PRICE, by=&quot;code&quot;)) ## code name price ## 1 A01 coffee 3000 ## 2 A02 cake 4000 ## 3 A04 &lt;NA&gt; 3000 (cafe_inner &lt;- inner_join(NAME, PRICE, by=&quot;code&quot;)) ## code name price ## 1 A01 coffee 3000 ## 2 A02 cake 4000 (cafe_full &lt;- full_join(NAME, PRICE, by=&quot;code&quot;)) ## code name price ## 1 A01 coffee 3000 ## 2 A02 cake 4000 ## 3 A03 cookie NA ## 4 A04 &lt;NA&gt; 3000 2.4.2.8 bind_rows과 bind_cols # rbind(NAME, PRICE) # 변수명이 다르므로 에러가 발생함 bind_rows(NAME,PRICE) ## code name price ## 1 A01 coffee NA ## 2 A02 cake NA ## 3 A03 cookie NA ## 4 A01 &lt;NA&gt; 3000 ## 5 A02 &lt;NA&gt; 4000 ## 6 A04 &lt;NA&gt; 3000 2.4.3 reshape2 2.4.3.1 reshape2 패키지 2.4.3.2 melt Q. R의 airquality는 1973년 5월~9월 동안 뉴욕이ㅡ 일일 대기 질 측정량에 대한 데이터로, 153개의 행과 6개의 변수로 이루어져 있다. 6개의 변수 중 Month와 Day을 식별자로 두고, 나머지 변수와 변수값은 모두 데이터 내에 포함되는 형태로 변환해 보자. install.packages(setdiff(&quot;reshape2&quot;, rownames(installed.packages()))) library(reshape2) head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 head(melt(airquality, id.vars=c(&quot;Month&quot;, &quot;Day&quot;), na.rm=TRUE)) ## Month Day variable value ## 1 5 1 Ozone 41 ## 2 5 2 Ozone 36 ## 3 5 3 Ozone 12 ## 4 5 4 Ozone 18 ## 6 5 6 Ozone 28 ## 7 5 7 Ozone 23 2.4.3.3 dcast air_melt&lt;-melt(airquality, id.vars=c(&quot;Month&quot;, &quot;Day&quot;), na.rm=TRUE) head(air_melt) ## Month Day variable value ## 1 5 1 Ozone 41 ## 2 5 2 Ozone 36 ## 3 5 3 Ozone 12 ## 4 5 4 Ozone 18 ## 6 5 6 Ozone 28 ## 7 5 7 Ozone 23 air_dcast&lt;-dcast(air_melt, Month + Day ~ ...) head(air_dcast) ## Month Day Ozone Solar.R Wind Temp ## 1 5 1 41 190 7.4 67 ## 2 5 2 36 118 8.0 72 ## 3 5 3 12 149 12.6 74 ## 4 5 4 18 313 11.5 62 ## 5 5 5 NA NA 14.3 56 ## 6 5 6 28 NA 14.9 66 2.4.4 data.table 2.4.4.1 data.table 패키지 2.4.4.2 데이터 테이블 생성 Q1. install.packages(setdiff(&quot;data.table&quot;, rownames(installed.packages()))) library(data.table) ## ## Attaching package: &#39;data.table&#39; ## The following objects are masked from &#39;package:reshape2&#39;: ## ## dcast, melt ## The following objects are masked from &#39;package:dplyr&#39;: ## ## between, first, last (mydata&lt;-data.table(x=c(1:3), y=c(&quot;가&quot;, &quot;나&quot;, &quot;다&quot;))) ## x y ## 1: 1 가 ## 2: 2 나 ## 3: 3 다 class(mydata) ## [1] &quot;data.table&quot; &quot;data.frame&quot; 2.4.4.3 데이터 접근 iris_dt&lt;-as.data.table(iris) iris_dt[,mean(Petal.Length), by=Species] ## Species V1 ## 1: setosa 1.462 ## 2: versicolor 4.260 ## 3: virginica 5.552 # Petal.Length값이 1이상인 행들을 Species로 그룹화한 뒤, # Sepal.Length와 Sepal.Width의 평균을 각각 mean.SL과 mean.SW를 변수명으로 하여 출력 iris_dt[Petal.Length&gt;=1, .(mean.SL=mean(Sepal.Length), mean.SW=mean(Sepal.Width)), by=Species] ## Species mean.SL mean.SW ## 1: setosa 5.006 3.428 ## 2: versicolor 5.936 2.770 ## 3: virginica 6.588 2.974 2.4.4.4 새로운 변수 생성 #하나의 변수 추가 데이터테이블[행, 새로운 칼럼명:=값, by=&quot;그룹화 기준 변수&quot;] #여러개의 변수 추가 데이터테이블[행, c(&quot;칼럼명1&quot;, ..., &quot;칼럼명n&quot;):=list(값1, ..., 값n), by=&quot;그룹화 기준 변수&quot;] air&lt;-as.data.table(airquality) air[, Wind_class:=ifelse(Wind&gt;=mean(Wind), &quot;U&quot;, &quot;D&quot;)] head(air) ## Ozone Solar.R Wind Temp Month Day Wind_class ## 1: 41 190 7.4 67 5 1 D ## 2: 36 118 8.0 72 5 2 D ## 3: 12 149 12.6 74 5 3 U ## 4: 18 313 11.5 62 5 4 U ## 5: NA NA 14.3 56 5 5 U ## 6: 28 NA 14.9 66 5 6 U 2.4.4.5 데이터 정렬 air[, season:=ifelse(Month %in% c(12,1,2), &quot;winter&quot;, ifelse(Month %in% c(3:5), &quot;spring&quot;, ifelse(Month %in% c(6:8), &quot;summer&quot;, &quot;fall&quot;)))] head(air) ## Ozone Solar.R Wind Temp Month Day Wind_class season ## 1: 41 190 7.4 67 5 1 D spring ## 2: 36 118 8.0 72 5 2 D spring ## 3: 12 149 12.6 74 5 3 U spring ## 4: 18 313 11.5 62 5 4 U spring ## 5: NA NA 14.3 56 5 5 U spring ## 6: 28 NA 14.9 66 5 6 U spring air[, .(Ozone_mean=mean(Ozone, na.rm=TRUE), Solar.R_mean=mean(Solar.R, na.rm=TRUE)), by=.(season)][order(Ozone_mean, decreasing=TRUE)] ## season Ozone_mean Solar.R_mean ## 1: summer 55.09836 193.5730 ## 2: fall 31.44828 167.4333 ## 3: spring 23.61538 181.2963 2.4.4.6 key를 활용하여 데이터 다루기 baseball&lt;-as.data.table(baseball) setkey(baseball, year) baseball[J(1960)] ## id year stint team lg g ab r h X2b X3b hr rbi sb cs bb so ibb ## 1: abernte02 1960 1 WS1 AL 2 1 1 1 0 0 0 0 0 0 0 0 0 ## 2: adairje01 1960 1 BAL AL 3 5 1 1 0 0 1 1 0 0 0 0 0 ## 3: aguirha01 1960 1 DET AL 37 28 0 1 0 0 0 0 0 0 0 19 0 ## 4: aparilu01 1960 1 CHA AL 153 600 86 166 20 7 2 61 51 8 43 39 3 ## 5: barbest01 1960 1 BAL AL 36 54 1 3 0 0 0 1 0 0 5 32 0 ## --- ## 195: torrejo01 1960 1 ML1 NL 2 2 0 1 0 0 0 0 0 0 0 1 0 ## 196: vernomi01 1960 1 PIT NL 9 8 0 1 0 0 0 1 0 0 1 0 1 ## 197: willibi01 1960 1 CHN NL 12 47 4 13 0 2 2 7 0 0 5 12 0 ## 198: willist02 1960 1 LAN NL 38 64 7 9 1 0 2 7 0 0 3 35 0 ## 199: willsma01 1960 1 LAN NL 148 516 75 152 15 2 0 27 50 12 35 47 8 ## hbp sh sf gidp ## 1: 0 0 0 0 ## 2: 0 0 0 0 ## 3: 0 2 0 0 ## 4: 1 20 6 12 ## 5: 0 4 0 0 ## --- ## 195: 0 0 0 0 ## 196: 0 0 0 0 ## 197: 0 0 0 1 ## 198: 0 7 0 1 ## 199: 3 3 2 11 2.4.4.7 key를 활용한 데이터 병합 2.5 결측치 2.5.1 결측치 인식 2.5.1.1 is.na(x) 2.5.1.2 complete.cases(x) Q1. airquality 데이터의 Ozone 변수에 대한 na값 존재 여부를 파악하고, 만약 na가 존재한다면 그 개수를 확인해 보자. is.na(airquality$Ozone) ## [1] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [25] TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE ## [37] TRUE FALSE TRUE FALSE FALSE TRUE TRUE FALSE TRUE TRUE FALSE FALSE ## [49] FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [61] TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [73] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE ## [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [97] FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE ## [109] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE ## [121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [145] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE sum(is.na(airquality$Ozone)) ## [1] 37 table(is.na(airquality$Ozone)) ## ## FALSE TRUE ## 116 37 Q2. apply 함수는 행 혹은 열별로 함수를 적용하여 한 번에 결과를 산출해 주는 함수이다. apply 함수와 사용자 정의 함수를 활용하여 airquality 데이터의 모든 변수에 대해 각각 결측치(na값)가 몇개씩 존재하는지 확인해 보자. apply(airquality, 2, function(x) sum(is.na(x))) ## Ozone Solar.R Wind Temp Month Day ## 37 7 0 0 0 0 Q3. complete.case 함수를 이용하여 airquality 데이터에서 na값이 하나라도 존재하는 행들을 air_na 변수에 저장하고, na값을 하나도 가지지 않는 행들을 air_com 변수에 저장하여라. # na값이 하나라도 존재하는 행들을 air_na 변수에 저장 # complete.cases 함수를 적용했을 때 FALSE를 반환하는 행들만 저장하면 됨. air_na &lt;- airquality[!complete.cases(airquality),] head(air_na) ## Ozone Solar.R Wind Temp Month Day ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 ## 10 NA 194 8.6 69 5 10 ## 11 7 NA 6.9 74 5 11 ## 25 NA 66 16.6 57 5 25 ## 26 NA 266 14.9 58 5 26 # na값이 하나도 없는 행들은 air_com 변수에 저장 # complete.cases 함수를 적용했을 때 TRUE를 반환하는 행들만 저장하면 됨. air_com &lt;- airquality[complete.cases(airquality),] head(air_com) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 7 23 299 8.6 65 5 7 ## 8 19 99 13.8 59 5 8 2.5.2 결측치 처리 2.5.2.1 단순 대치법 (Single Imputation) 분류 설명 completes analysis 결측값이 존재하는 행을 삭제 평균 대치법 관측 또는 실험을 통해 얻어진 데이터의 평균으로 결측치를 대치 (비조건부/조건부 평균대치법) 단순확률 대치법 평균대치법에서 추정량 표준 오차이ㅡ 과소 추정문제를 보완하고자 고안된 방법으로 Hot-deck 방법, nearest neightbor 방법 등이 있다. 2.5.2.1.1 결측치 제거 # 결측치가 존재하는 행 제거 데이터명[!is.na(데이터명)] 데이터명[complete.cases(데이터명),] 데이터명 %&gt;% filter(!is.na(데이터명)) # na.omit 함수 활용 na.omit(데이터명) 2.5.2.1.2 평균 대치법 Q. airquality의 Ozone 변수값이 존재하지 않는 경우, Ozone 변수값들의 평균으로 대치해 보자. airquality$Ozone&lt;-ifelse(is.na(airquality$Ozone), mean(airquality$Ozone, na.rm=TRUE), airquality$Ozone) table(is.na(airquality$Ozone)) ## ## FALSE ## 153 2.5.2.2 다중 대치법 (Multiple Imputation) 2.5.2.3 패키지 활용 R에서 결측치를 대치하기 위해 사용될 수 있는 함수에는 DMwR 패키지의 함수들이 있다. Q. DMwR의 함수들을 이용하여 NA값을 해당 변수의 중위수로 대치하는 전처리를 수행해 보자. NA값이 대치되는 ㅗ가정을 확인하기 위해 airquality 데이터에서 NA값을 하나라도 가지고 있는 행번호들을 미리 봅아놓고, 전처리 전의 원본 데이터와 전처리 후의 데이터를 비교하여 전처리가 잘 진행되었는지 확인해 보자. install.packages(setdiff(&quot;DMwR2&quot;, rownames(installed.packages()))) library(DMwR2) ## Registered S3 method overwritten by &#39;quantmod&#39;: ## method from ## as.zoo.data.frame zoo #airquality에서 na값을 가진 행들의 번호 추출 na_idx&lt;-which(!complete.cases(airquality)) air_before&lt;-airquality #na값을 제거한 데이터를 air_after에 저장(centralImputation 함수 활용) air_after&lt;-centralImputation(airquality) # 두 데이터에서 na_idx에 저장된 행번호에 해당하는 데이터들을 추출하여 na값이 잘 대체되었는지 비교 head(air_before[na_idx,]) ## Ozone Solar.R Wind Temp Month Day ## 5 42.12931 NA 14.3 56 5 5 ## 6 28.00000 NA 14.9 66 5 6 ## 11 7.00000 NA 6.9 74 5 11 ## 27 42.12931 NA 8.0 57 5 27 ## 96 78.00000 NA 6.9 86 8 4 ## 97 35.00000 NA 7.4 85 8 5 head(air_after[na_idx,]) ## Ozone Solar.R Wind Temp Month Day ## 5 42.12931 205 14.3 56 5 5 ## 6 28.00000 205 14.9 66 5 6 ## 11 7.00000 205 6.9 74 5 11 ## 27 42.12931 205 8.0 57 5 27 ## 96 78.00000 205 6.9 86 8 4 ## 97 35.00000 205 7.4 85 8 5 median(airquality$Ozone, na.rm=TRUE) ## [1] 42.12931 median(airquality$Solar.R, na.rm=TRUE) ## [1] 205 2.6 이상치 인식 2.6.1 이상치란? 2.6.2 사분위수 2.6.3 boxplot을 활용한 이상치 판별 Q1. 내장데이터 airquality의 Ozone 변수에 대한 boxplot을 그려보자. 또한 이를 OzoneBP 이라는 변수에 저장하여 lower whisker와 upper whisker 밖에 있는 이상치가 존재하는지를 확인해 보자. (OzoneBP&lt;-boxplot(airquality$Ozone)) ## $stats ## [,1] ## [1,] 1.00000 ## [2,] 21.00000 ## [3,] 42.12931 ## [4,] 46.00000 ## [5,] 82.00000 ## ## $n ## [1] 153 ## ## $conf ## [,1] ## [1,] 38.93592 ## [2,] 45.32270 ## ## $out ## [1] 115 135 97 97 85 108 122 89 110 168 118 84 85 96 91 ## ## $group ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## ## $names ## [1] &quot;1&quot; 데이터는 전체적으로 작은 값들에 많이 분포한 것을 확인할 수 있으며, upper whisker 위로 15개의 이상치 값 ($out)이 있음을 알 수 있다. Q2. lower whisker(Q1-1.5IQR) 보다 작거나 upper whisker(Q3+1.5IQR) 보다 큰값들을 이상치로 간주하고, 해당 값들이 저장된 행번호를 각각 upperOutlier, lowerOutlier 변수에 저장하자. 그리고 해당 행을 출력하여 데이터를 확인해 보자. (LowerQ&lt;-fivenum(airquality$Ozone)[2]) ## [1] 21 (UpperQ&lt;-fivenum(airquality$Ozone)[4]) ## [1] 46 (IQR&lt;-IQR(airquality$Ozone, na.rm=TRUE)) ## [1] 25 # 조건을 만족하는 행번호 추출 (upperOutlier&lt;-which(airquality$Ozone &gt; UpperQ+IQR*1.5)) ## [1] 30 62 69 70 71 86 99 100 101 117 121 122 123 124 127 # 조건을 만족하는 행번호 추출 (lowerOutlier&lt;-which(airquality$Ozone &lt; UpperQ-IQR*1.5)) ## [1] 9 11 18 21 23 76 147 airquality[upperOutlier, &quot;Ozone&quot;] ## [1] 115 135 97 97 85 108 122 89 110 168 118 84 85 96 91 airquality[lowerOutlier, &quot;Ozone&quot;] ## [1] 8 7 6 1 4 7 7 2.7 날짜 데이터 전처리 2.7.1 날짜 데이터 다루기 2.7.1.1 R의 날짜 데이터 형식 분류 설명 Date 날짜만을 나타내는 클래스 이며, 내부적으로 1970년 1월1일 이후 경과된 날 수를 저장함. POSIXct 날짜와 시간을 나타내며, 일초 간격의 정확도로 시간을 표현함. 내부적으로 1970년 1월1일에서 경과된 초 수와 시간대를 저장함. POSIXlt 날짜와 시간을 나타내며 1900년에서부터 경과된 연도, 월, 일, 시, 분, 초를 포함하는 9개의 정보를 리스트에 저장 (today&lt;-Sys.Date()) ## [1] &quot;2021-10-02&quot; class(today) ## [1] &quot;Date&quot; (time&lt;-Sys.time()) ## [1] &quot;2021-10-02 23:53:47 KST&quot; class(time) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; # today 내부의 값 확인: 1970년 1월 1일 이후로 경과한 일 수를 의미함. unclass(today) ## [1] 18902 # time 내부의 값 확인: 1970년 1월 1일 00:00:00 이후로 경과한 초 수를 의미함. unclass(time) ## [1] 1633186428 # unclass를 적용한 time값을 다시 원래 날짜 형식으로 변환하기 # unclass(time)은 1970년 1월 1일 이후로 경과한 초수를 의미하므로, # origin 인자값으로 &#39;1970-01-01&#39;을 지정 as.POSIXct(unclass(time), origin=&quot;1970-01-01&quot;) ## [1] &quot;2021-10-02 23:53:47 KST&quot; 2.7.1.2 날짜 표시형식 변경 Q1. Sys.time 함수를 이용해 현재 날짜와 시간을 now 변수에 저장한 후, 이를 “네자리 년도수-두자리 월-일 시:분:초”의 형태를 가진 문자형 데이터로 변환해 보자. now&lt;-Sys.time() class(now) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; format(now, &quot;%Y-%m-%d %H:%M:%S&quot;) ## [1] &quot;2021-10-02 23:53:47&quot; class(format(now, &quot;%y-%m-%d %H:%M:%S&quot;)) ## [1] &quot;character&quot; Q2. “20200101”이라는 문자열을 Date 형식으로 변환한 후, date 변수에 저장하여 class를 확인해 보자. (date&lt;-as.Date(&quot;20200101&quot;, format=&quot;%Y%m%d&quot;)) ## [1] &quot;2020-01-01&quot; class(date) ## [1] &quot;Date&quot; 2.7.1.3 날짜 데이터의 연산 Sys.Date() + 100 ## [1] &quot;2022-01-10&quot; as.Date(&quot;2021-01-01&quot;, format=&quot;%Y-%m-%d&quot;) + 365 ## [1] &quot;2022-01-01&quot; "],["통계분석.html", "3 통계분석 3.1 통계 자료의 획득방법 3.2 T-검정 (T-Test) 3.3 교차분석 3.4 분산분석 (ANOVA) 3.5 상관분석 3.6 회귀분석", " 3 통계분석 통계란 특정집단을 대상으로 수행한 조사나 실험을 통해 나온 결과에 대한 요약된 형태의 표현이다. 조사 또는 실험을 통해 데이터를 확보하며, 조사대상에 따라 총조사와 표본 조사로 구분한다. 3.1 통계 자료의 획득방법 3.1.1 총조사/전수 조사(census) 3.1.2 표본조사 3.1.2.1 표본추출 방법 (데이터 샘플링) 3.1.2.1.1 단순 임의 추출법 (simple random sampling) 전체 데이터에서 데이터를 선택할 확률을 모두 동일하게 하여 표본을 추출하는 방법이다. 복원추출은 한번 선택한 표본을 다시 추출할 수 있는 방법이며, 비복원추출은 한번 선택된 표본은 다시 추출할 수 없는 방법이다. 일반적으로 데이터를 training data와 test data로 분할할 때 가장 많이 사용하는 표본추출 방법이다. Q. iris 데이터로 분석을 진행하기 위해 전체 데이터를 7:3의 비율로 training data와 test data를 추출한 뒤 새로운 변수에 저장해 보자. (데이터 추출 방법은 단순 임의 추출을 이용한다.) # iris 데이터 행의 개수에서 70%에 해당하는 행번호를 랜덤으로 추출 # nrow(): 데이터의 행 개수를 산출해 주는 함수 idx &lt;- sample(1:nrow(iris), nrow(iris)*0.7, replace=FALSE) training&lt;-iris[idx,] head(training) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species SL_new ## 21 5.4 3.4 1.7 0.2 setosa 0.3055556 ## 55 6.5 2.8 4.6 1.5 versicolor 0.6111111 ## 126 7.2 3.2 6.0 1.8 virginica 0.8055556 ## 110 7.2 3.6 6.1 2.5 virginica 0.8055556 ## 11 5.4 3.7 1.5 0.2 setosa 0.3055556 ## 137 6.3 3.4 5.6 2.4 virginica 0.5555556 test&lt;-iris[-idx,] head(test) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species SL_new ## 6 5.4 3.9 1.7 0.4 setosa 0.3055556 ## 13 4.8 3.0 1.4 0.1 setosa 0.1388889 ## 18 5.1 3.5 1.4 0.3 setosa 0.2222222 ## 26 5.0 3.0 1.6 0.2 setosa 0.1944444 ## 28 5.2 3.5 1.5 0.2 setosa 0.2500000 ## 30 4.7 3.2 1.6 0.2 setosa 0.1111111 3.1.2.1.2 계통추출법 (systematic sampling) 3.1.2.1.3 집락추출법 (cluster random sampling) 3.1.2.1.4 층화추출법 (stratified random sampling) 특정 데이터가 여성 계층 70%, 남성 계층 30%로 구성되어 있다고 가정해 보자. 각 계층을 고루 대표할 수 있도록 표본을 추출하기 위해서는 여성과 남성 집단에 대해 0.7:0.3의 비율로 데이터를 뽑아야 한다. 이처럼 여성과 남성이라는 계층별로 표본을 추출하는 것을 층화 임의 추출이라고 한다. Q. iris 데이터에서 species가 setosa인 데이터를 20개, versicolor인 데이터를 15개, versinica인 데이터를 15개씩 층화 임의 추출을 사용해 추출해 보자. strata(data, stratanames=NULL, size, method=c(&quot;srswor&quot;,&quot;srswr&quot;,&quot;poisson&quot;,&quot;systematic&quot;), pik, descrption=FALSE) 인자 설명 data 표본을 추출할 데이터프레임 또는 행렬 stratanames 데이터에서 계층(집단)을 구분하는 변수들 (여러개일 경우 c()안에 나열) size 각 계층에서 추출할 데이터의 개수 method 데이터를 뽑는 방법 지정srswor: 비복원 단순 임의 추출srswr: 복원 단순 임의 추출poisson: 포아송 추출systematic: 계통 추출 install.packages(setdiff(&quot;sampling&quot;, rownames(installed.packages()))) library(sampling) sample&lt;-strata(data=iris, c(&quot;Species&quot;), size=c(20,15,15), method=&quot;srswor&quot;) head(sample) ## Species ID_unit Prob Stratum ## 1 setosa 1 0.4 1 ## 3 setosa 3 0.4 1 ## 4 setosa 4 0.4 1 ## 8 setosa 8 0.4 1 ## 10 setosa 10 0.4 1 ## 14 setosa 14 0.4 1 iris_sample&lt;-getdata(iris, sample) head(iris_sample) ## Sepal.Length Sepal.Width Petal.Length Petal.Width SL_new Species ID_unit ## 1 5.1 3.5 1.4 0.2 0.22222222 setosa 1 ## 3 4.7 3.2 1.3 0.2 0.11111111 setosa 3 ## 4 4.6 3.1 1.5 0.2 0.08333333 setosa 4 ## 8 5.0 3.4 1.5 0.2 0.19444444 setosa 8 ## 10 4.9 3.1 1.5 0.1 0.16666667 setosa 10 ## 14 4.3 3.0 1.1 0.1 0.00000000 setosa 14 ## Prob Stratum ## 1 0.4 1 ## 3 0.4 1 ## 4 0.4 1 ## 8 0.4 1 ## 10 0.4 1 ## 14 0.4 1 table(iris_sample$Species) ## ## setosa versicolor virginica ## 20 15 15 3.1.2.1.5 다단계 추출 (multi-stage sampling) 3.2 T-검정 (T-Test) T-검정은 두 집단의 평균을 통계적으로 비교하기 위해 사용하는 검정방법이다. 어떤 방식으로 집단의 평균을 비교하느냐에 따라 일표본 T-검정, 대응표본 T-검정, 독립표본 T-검정으로 나누어진다. 3.2.1 일표본 T-검정 (One Sample T-Test) 3.2.1.1 일표본 T-검정이란? 단일모집단에서 관심이 있는 연속형 변수의 평균값을 특정 기준값과 비교하고자 할 때 사용하는 검정방법이다. 예를 들어 A과수원에서 생산되는 사과의 평균 무게가 200g이라고 알려져 있을 때, 실제로 A과수원에서 생산되는 전체 사과의 평균 무게가 200g인지 알아보고 싶은 경우에 일표본 t-검정을 수행할 수 있다. 단일모집단에서 알고자 하는 값이 종속변수가 되며, 설정한 기준값과 종속변수의 평균값 사이의 차이가 통계적으로 유의하다면 귀무가설이 기각되고 대립가설이 채택됨으로써 두값이 다르다고 결론을 내릴 수 있다. 3.2.1.2 일표본 T-검정의 가정 모집단의 구성요소들이 정규분포를 이룬다는 가정 종속변수는 연속형 변수여야 하며, 검증하고자 하는 기준값이 있어야 한다. 3.2.1.3 일표본 T-검정의 단계 가설 설정 유의수준 설정 검정통계량의 값 및 유의확률 계산 귀무가설의 기각여부 판단 및 의사결정 일표본 T-검정을 수행하기 전 표본에 대한 정규성을 검정해야 할 경우 샤피로-윈크 검정, 콜모고로프 스미르노프 검정, Q-Q도를 통한 확인 등 다양한 방법을 활용할 수 있다. 그 중에서 샤피로-윌크 검정이 많이 사용된다. 샤피로-윌크 검정의 귀무가설은 “데이터가 정규분포를 따른다.”이고, 대립가설은 “데이터가 정규분포를 따르지 않는다.”이다. 따라서 검정 결과로 나오는 p-value값에 따라 데이터의 정규성을 확인할 수 있다. 샤피로-윌크 검정은 아래와 같이 shapiro.test함수를 통해 수행하며, data 자리에는 정규성 검정을 수행할 데이터 프레임을 지정한다. shapiro.test(data) 데이터가 정규분포를 따른다는 가정을 만족한 경우, t.test라는 함수를 이용하여 일표본-T검정을 수행한다. 반면 데이터가 정규성을 만족하지 않는 경우, wilcox.test 함수를 이용해 T-검정을 수행한다. t.test(x, alternative=c(&quot;two.sided&quot;,&quot;less&quot;,&quot;greater&quot;), mu=0) wilcox.test(x, alternative=c(&quot;two.sided&quot;,&quot;less&quot;,&quot;greater&quot;), mu=0) 인자 설명 x 표본으로부터 관측한 값(수치형 벡터) alternative 양측검정: “two.sided”입력단측검정: 표본평균이 특정값보다 작은지에 대해 검정을 수행할 시 “less”를 입력, 특정값보다 큰지에 대한 검정수행시 “greater”를 입력 mu 검정시 기준이 되는 값 Q. MASS 패키지의 cats데이터는 고양이들의 성별, 몸무게, 심장의 무게를 담고 있다. cats데이터에서 고양이들의 평균몸무게가 2.6kg인지 아닌지에 대한 통계적 검정을 수행하고, 결과를 해석해보자. (양측검정 수행, 유의수준=0.05) 검정을 수행하기에 앞서 설정할 수 있는 가설은 아래와 같다. 귀무가설(H0): 고양이들의 평균 몸무게는 2.6kg이다.대립가설(H1): 고양이들의 평균 몸무게는 2.6kg가 아니다. cats 데이터 확인 및 Bwt변수에 대한 정규성 검정 수행 library(MASS) head(cats) ## Sex Bwt Hwt ## 1 F 2.0 7.0 ## 2 F 2.0 7.4 ## 3 F 2.0 9.5 ## 4 F 2.1 7.2 ## 5 F 2.1 7.3 ## 6 F 2.1 7.6 str(cats) ## &#39;data.frame&#39;: 144 obs. of 3 variables: ## $ Sex: Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Bwt: num 2 2 2 2.1 2.1 2.1 2.1 2.1 2.1 2.1 ... ## $ Hwt: num 7 7.4 9.5 7.2 7.3 7.6 8.1 8.2 8.3 8.5 ... shapiro.test(cats$Bwt) ## ## Shapiro-Wilk normality test ## ## data: cats$Bwt ## W = 0.95188, p-value = 6.731e-05 정규성 검정의 결과 p-value가 6.731e-05로 유의수준 0.05보다 작기 때문에 ‘데이터가 정규분포를 따른다’(=정규성을 만족한다.) 라는 귀무가설을 기각한다. 즉, cats 데이터의 Bwt 변수는 정규분포를 따르지 않으므로 wilcox.test 함수를 이용해 T-검정을 수행한다. cats 데이터에 대한 일표본 T-검정 수행 wilcox.test(cats$Bwt, mu=2.6, alternative=&quot;two.sided&quot;) ## ## Wilcoxon signed rank test with continuity correction ## ## data: cats$Bwt ## V = 5607, p-value = 0.02532 ## alternative hypothesis: true location is not equal to 2.6 표본평균이 특정값과 같은지에 대해 알아보는 양측검정을 수행하는 것이기 때문에 wilcox.test 함수의 alternative(검정방향)에는 “two.sided”값을 지정하면 된다. wilcox 검정 결과 p-value는 0.02532로 유의수준 0.05 보다 작다. 따라서 귀무가설 ’고양이들의 평균 몸무게(Bwt 변수값의 평균)는 2.6(kg)이다.’을 기각하며, 고양이들의 평균 몸무게는 2.6kg이 아니라는 결론을 내릴 수 있다. 3.2.2 대응표본 T-검정 (Paired Sample T-Test) 3.2.2.1 대응표본 T-검정이란? 단일모집단에 대해 두 번의 처리를 가했을 때, 두 개의 처리에 따른 평균의 차이를 비교하고자 할 때 사용하는 검정방법이다. 예를 들어 어느 기업에서 판매사원들의 역량 향상을 위해 두 가지 방법으로 직업교육을 실시하고 나서, 두 가지 교육방법에 따른 판매실적 평균에 차이가 있는지를 검정하고자 할 때 대응표본 t-검정을 사용할 수 있다. 이때 ’직업교육 방법’이 독립변수, 그에 따른 ’판매실적’이 종속변수가 된다. 하나의 모집단에서 크기가 n개인 하나의 표본을 추출한 후, 표본 내의 개체들에 대해서 두 번의 측정을 실시한다. 따라서 관측값들은 서로 독립적이지 않고 쌍(pair)으로 이루어져 있어 대응표본 t-검정을 짝지어진 t-검정 (matched pair t-test)이라고도 한다. 모집단과 표본은 하나씩이지만, 각 개체들에 대해 두 개씩이ㅡ 관측값이 존재하므로 모수는 두 개다. 표본 내에 있는 각 개체별로 짝지어진 관측값 사이에 차이가 있는지를 검정하므로 자료의 형태는 아래의 표와 같다. 개체 관측값1(A) 관측값2(B) 차이(A-B=D) 1 A1 B1 A1-B1=D1 2 A2 B2 A2-B2=D2 … … … … n An Bn An-Bn=Dn 3.2.2.2 대응표본 T검정의 가정 대응표본 t-검정에서는 모집단의 관측값이 정규성(정규분포를 만족한다는 가정)을 만족해야 한다. 종속변수는 연속형 변수이어야 한다. 3.2.2.3 대응표본 T검정의 단계 가설 검정 귀무가설(H0): 2개의 모평균 간에는 차이가 없다. (\\(\\mu_{x}-\\mu_{y}-D=0\\)) 대립가설(H1): 2개의 모평균 간에는 차이가 있다. (\\(\\mu_{x}-\\mu_{y}-D\\ne0\\)) - 양측검정 2개의 모평균 간의 차이는 0보다 크다. (\\(\\mu_{x}-\\mu_{y}-D&gt;0\\)) - 우단측검정 2개의 모평균 간의 차이는 0보다 작다. (\\(\\mu_{x}-\\mu_{y}-D&lt;0\\)) - 좌단측검정 유의수준 설정 검정통계량의 값 및 유의확률 계산 귀무가설의 기각여부 판단 및 의사결정 t.test(x, y, alternative=c(&quot;two.sided&quot;,&quot;less&quot;,&quot;greater&quot;), paired=FALSE, m=0) Q. 10명의 환자를 대상으로 수면영양제를 복용하기 전과 후의 수면시간을 측정하여 영양제의 효과가 있는지를 판단하고자 한다. 영양제 복용 전과 후의 평균 수면시간에 차이가 있는지를 알아보는데, 단측검정을 수행하여 영양제 복용 후에 수면시간이 더 늘어났는지를 검정해보자. 수면영양제를 복용하기 전과 후의 수면시간은 아래에 제시된 바와 같다. (표본이 정규성을 만족한다는 가정하에 단측검정 수행, 유의수준=0.05) 검정을 수행하기에 앞서 설정할 수 있는 가설은 아래와 같다.귀무가설(H0): 수면영양제를 복용하기 전과 후의 평균 수면시간에는 차이가 없다. (\\(\\mu_{x}-\\mu_{y}-D=0\\))대립가설(H1): 수면영양제를 복용하기 전과 후의 평균 수면시간 차이는 0보다 작다. 즉, 수면영양제를 복용한 후 평균수면시간이 늘어났다. (\\(\\mu_{x}-\\mu_{y}-D&lt;0\\)) 수면 영양제 복용 전 10명의 환자들의 수면시간: 7, 3, 4, 5, 2, 1, 6, 6, 5, 4수면 영양제 복용 후 10명의 환자들의 수면시간: 8, 4, 5, 6, 2, 3, 6, 8, 6, 5 데이터 입력 (data&lt;-data.frame(before=c(7,3,4,5,2,1,6,6,5,4), after=c(8,4,5,6,2,3,6,8,6,5))) ## before after ## 1 7 8 ## 2 3 4 ## 3 4 5 ## 4 5 6 ## 5 2 2 ## 6 1 3 ## 7 6 6 ## 8 6 8 ## 9 5 6 ## 10 4 5 대응표본 T-검정수행 t.test(data$before, data$after, alternative=&quot;less&quot;, paired=TRUE) ## ## Paired t-test ## ## data: data$before and data$after ## t = -4.7434, df = 9, p-value = 0.0005269 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -0.6135459 ## sample estimates: ## mean of the differences ## -1 수면 영양제를 복용하기 전과 후의 평균 수면시간 차이가 비교하고자 하는 값(0)보다 작은지에 대하여 검정을 수행하기 때문에 alternative 인자에는 “less”를 입력한다. 대응표본 t-검정 수행 결과, 검정통계량(t값)은 -4.7434, df(자유도)는 9, 유의확률(p-value)은 0.0005269이다. p-value가 유의수준 0.05보다 작기 때문에 귀무가설을 기각하고, ’수면영양제를 복용하기 전과 후의 평균 수면시간의 차이는 통계적으로 유의하며, 영양제를 복용한 후 수면시간이 늘었다.’라는 결론을 내릴 수 있다. 3.2.3 독립표본 T-검정 (Independent Sample T-Test) 3.2.3.1 독립표본 T-검정이란? 두개의 독립된 모집단의 평균을 비교하고자 할 때 사용하는 검정방법이다. 예를 들어 성별에 따라 출근준비시간에 차이가 있는지를 통계적으로 검정하기 위해서 독립표본 t-검정을 사용할 수 있다. 이 때 그룹(집단)을 나누는 기준인 ’성별’이 독립변수이고, 그에 따른 관리값인 ’출근준비시간’이 종속변수이다. 두개의 모집단에서 크기가 n개인 표본을 각각 추출한 후 표본의 관측값들을 이용해 검정을 실시한다. 따라서 독립표본 t-검정에서는 모집단, 모수, 표본이 모두 두 개씩 존재한다. 3.2.3.2 독립표본 T-검정의 가정 두 모집단은 정규성을 만족해야 한다. 독립표본 t-검정에서 두 개의 모집단은 서로 독립적이어야 한다. 두 모집단의 분산이 서로 같음을 의미하는 등분산성 가정을 확인해야 한다. 등분산 가정은 비교하고자 하는 두 독립 집단의 모분산이 동일함을 의미하며, 등분산성 만족여부에 따라 다른 계산 방법이 사용된다. 따라서 이 가정을 확인하기 위해 독립표본 t-검정 수행 과정에서는 등분산 검정을 먼저 수행한 후 검정통계량을 계산한다. 독립변수는 범주형, 종속변수는 연속형이어야 한다. 3.2.3.3 독립표본 T-검정의 단계 가설검정 모수: 서로 독립된 두 모집단의 평균 (\\(mu_{1},mu_{2}\\)) 귀무가설(\\(H_{0}\\)) : 두 개의 모평균에는 차이가 없다. (\\(mu_{1}=mu_{2}\\)) 대립가설(\\(H_{1}\\)) : 두 개의 모평균에는 차이가 있다. (\\(mu_{1}\\ne mu_{2}\\)) - 양측검정 집단1의 모평균이 집단2의 모평균보다 크다. (\\(mu_{1}&gt;mu_{2}\\)) - 우단측검정 집단1의 모평균이 집단2의 모평균보다 작다. (\\(mu_{1}&lt;mu_{2}\\)) - 좌단측검정 유의수준 설정 등분산 검정 두 모집단이 등분산성을 만족하는지의 여부에 따라 유의확률과 검정통계량의 값이 다르게 계산된다. 따라서 독립표본 t-검정을 위해서는 반드시 등분산 검정이 선행되어야 한다. 귀무가설(\\(H_{0}\\)) : 두 집단의 분산이 동일하다. (\\(\\sigma_{1}^{2}=\\sigma_{2}^{2}\\)) 대립가설(\\(H_{1}\\)) : 두 집단의 분산이 동일하지 않다. (\\(\\sigma_{1}^{2}\\ne\\sigma_{2}^{2}\\)) 검정통계량의 값 및 유의확률 계산 귀무가설의 기각여부 판단 및 의사결정 유의확률(p-value) &lt; 유의수준(\\(\\alpha\\)) : 귀무가설을 기각하고, 대립가설을 채택한다. 유의확률(p-value) &gt; 유의수준(\\(\\alpha\\)) : 귀무가설을 기각하지 않는다. R에서 독립표본 t-검정을 수행하기에 앞서, 등분산 검정을 수행해야 하며 이를 위한 R의 다양한 함수들 중에서 var.test 함수의 문법을 알아보자. vat.test(x, y, alternative) var.test(formula, data, alternative) 등분산 검정의 과정을 거친 후 아래와 같이 t.test 함수를 이용해 독립표본 t-검정을 수행할 수 있다. t.test(x, y, alternative, var.equal=FALSE) t.test(formula, data, alternative, var.equal=FALSE) 인자 설명 x 모집단1로부터 측정한 관측값 (수치형 벡터) y 모집단2로부터 측정한 관측값 (수치형 벡터) formula data t-검정을 수행할 데이터명 alternative 양측검정시 “two.sided”, 단측검정시 “less”, “greater” 입력 equal 등분산성을 만족하는지의 여부 (TRUE 혹은 FALSE로 입력) Q. cats 데이터는 고양이들의 성별, 몸무게, 심장의 무게를 담고 있다. 고양이들의 성별에 따른 몸무게의 평균은 통계적으로 다르다고 할 수 있는지에 대한 검정을 수행하고, 결과를 해석해 보자. 검정을 수행하기에 앞서 설정할 수 있는 가설은 아래와 같다귀무가설: 고양이의 성별에 따른 평균 몸무게에는 통계적으로 유의한 차이가 없다.대립가설: 고양이의 성별에 따른 평균 몸무게에는 통계적으로 유의한 차이가 있다. 1) 독립 t검정을 수행하기에 앞서, 범주별 데이터값의 등분산성 검정 수행 library(MASS) data(&quot;cats&quot;) var.test(Bwt~Sex, data=cats) ## ## F test to compare two variances ## ## data: Bwt by Sex ## F = 0.3435, num df = 46, denom df = 96, p-value = 0.0001157 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.2126277 0.5803475 ## sample estimates: ## ratio of variances ## 0.3435015 등분산 검정의 결과 유의확률(p-value)이 0.0001157로 유의수준 0.05보다 매우 작기 때문에 귀무가설을 기각한다. 따라서 A, B 두 집단의 데이터는 등분산 가정을 만족한다고 할 수 없다. 2) 성별에 따른 몸무게가 등분산성을 만족하지 않는다는 조건 하에 독립 t검정을 수행 t.test(Bwt~Sex, data=cats, alternative=&quot;two.sided&quot;, var.equal=FALSE) ## ## Welch Two Sample t-test ## ## data: Bwt by Sex ## t = -8.7095, df = 136.84, p-value = 8.831e-15 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.6631268 -0.4177242 ## sample estimates: ## mean in group F mean in group M ## 2.359574 2.900000 독립표본 t-검정 수행결과, 검정통계량(t값)은 -8.7095, df(자유도)는 136.84, 유의확률(p-value)은 8.831e-15이다. p-value가 0에 가까운 매우 작은 숫자로 유의수준 0.05 보다 작기 때문에 귀무가설을 기각한다. 따라서 ‘고양이들의 성별에 따른 평균 몸무게에는 통계적으로 유의한 차이가 존재한다.’ 라는 결론을 내릴 수 있다. 3.3 교차분석 교차분석이란 명목척도 혹은 순서척도와 같은 범주형 자료들 간의 상호 연관성을 알아볼 때 사용하는 방법이다. 두 범주형 변수에 대한 관련성을 파악하고 이를 통계적으로 검정하여 복잡한 상황에 대한 통찰력을 얻을 수 있다. 3.3.1 교차분석 개념 3.3.1.1 교차분석의 개념 및 특징 범주형 자료(명목/서열 수준)인 두 변수 간이ㅡ 관계를 알아보기 위해 실시하는 분석 기법이다. 적합성 검정, 독립성 검정, 동질성 검정에 사용되며, 카이제곱 검정 통계량을 이용한다. 3.3.1.2 교차표 3.3.2 적합성 검정 3.3.2.1 적합성 검정이란? 실험에서 얻어진 관측값들이 예상한 이론과 일치하는지 아닌지를 검정하는 방법이다. 관측값들이 어떠한 이론적 분포를 따르고 있는지를 알아볼 수 있다. 즉, 모집단 분포에 대한 가정이 옳게 됐는지를 관측 자료와 비교하여 검정하는 것이다. 3.3.2.2 가설설정 n개의 표본 자료를 k개의 범주로 분류한 뒤, 각 범주의 관측도수(O)와 주어진 확률 분포에 대해 각 범주에 속하는 기대도수(E)들이 적합하는지의 여부를 검정하는 것이다. 귀무가설: 실제 분포와 이론적 분포 간에는 차이가 없다. (두 분포가 일치한다) 대립가설: 실제 분포와 이론적 분포 간에는 차이가 있다. (두 분포가 일치하지 않는다) 3.3.2.3 검정 통계량 3.3.2.4 자유도 3.3.2.5 R을 이용한 적합성 검정 R에서 적합성 검정은 chisq.test 함수로 수행하며, 사용 문법은 아래와 같다. chisq.test(x, y, p) Q. MASS 패키지의 survey 데이터에서 W.Hnd 변수는 설문 응답자가 왼손잡이 인지 오른손잡이 인지를 나타낸다. R을 이용하여 W.Hnd 변수에 대한 분할표를 생성하고, 아래와 같은 가설에 대한 적합도 검정을 수행해 보자. 귀무가설: 전체 응답자 중 왼손잡이의 비율이 20%, 오른손잡이의 비율이 80%이다. 대립가설: 전체 응답자 중 왼손잡이의 비율이 20%, 오른손잡이의 비율이 80%라고 할 수 없다. data(survey, package=&quot;MASS&quot;) table(survey$W.Hnd) ## ## Left Right ## 18 218 data&lt;-table(survey$W.Hnd) chisq.test(data, p=c(0.2,0.8)) ## ## Chi-squared test for given probabilities ## ## data: data ## X-squared = 22.581, df = 1, p-value = 2.015e-06 유의확률(p-value)이 2.015e-06로 0.05 보다 작으므로 ‘전체 응답자 중 왼손잡이는 20%, 오른손잡이는 80%이다’ 라는 귀무가설을 기각한다. 3.3.3 독립성 검정 3.3.3.1 독립성 검정이란? 모집단이 2개의 변수 A, B에 의해 범주화되었을 때, 이 두 변수들 사이의 관계가 독립인지 아닌지를 검정하는 것을 의미한다. 검정 통계량 값을 계산할 때는 교차표를 활용한다. 3.3.3.2 가설 검정 모집단을 범주화하는 기준이 되는 두 변수 A, B가 서로 독립적으로 관측값에 영향을 미치는지의 여부를 검정하는 것이다. 귀무가설(\\(H_{0}\\)): 두 변수 사이에는 연관이 없다. (독립이다) 대립가설(\\(H_{1}\\)): 두 변수 사이에는 연관이 있다. (종속이다) 3.3.3.3 검정 통계량 3.3.3.4 자유도 3.3.3.5 R을 이용한 독립성 검정 [함수 사용법] xtabs(formula, data) table(범주형변수) # 도수분포표 생성 table(범주형변수1, 범주형변수2) # 두 변수간 이원분할표 생성 Q. MASS 패키지의 survey 데이터에서 Exer 변수는 설문 응답자가 얼마나 자주 운동을 하는지에 대해 Freq, Some, None의 범주로 값을 저장하고 있다. W.Hnd 변수는 설문 응답자가 왼손잡이인지 오른손 잡이인지에 대해 Left, Right의 두가지 범주로 값을 가지고 있다. 주로 사용하는 손과 운동의 빈도가 서로 독립인지를 확인하기 위해 분할표를 생성하고, 아래의 가설에 대한 독립성 검정을 수행해 보자. 3.3.4 동질성 검정 3.4 분산분석 (ANOVA) 3.4.1 분산분석의 개념 분산분석은 두 개 이상의 집단에서 그룹 평균간 차이를 그룹내 변동에 비교하여 살펴보는 통계분석 방법이다. 즉, 두 개 이상 집단들의 평균 간 차이에 대한 통계적 유의성을 검정 (두 개 이상 집단들의 평균을 비교) 분산분석의 분류는 아래와 같다. 분석구분 분석명칭 독립변수 개수 종속변수 개수 단일변량 분산분석 일원배치 분산분석 1개 1개 단일변량 분산분석 이원배치 분산분석 2개 1개 단일변량 분산분석 다원배치 분산분석 3개 이상 1개 다변량 분산분석 MANOVA 1개 이상 2개 이상 3.4.2 일원배치 분산분석 (One-way ANOVA) 3.4.2.1 일원배치 분산분석의 개념 분산분석에서 반응값에 대한 하나의 범주형 변수의 영향을 알아보기 위해 사용되는 검증방법이다. 모집단의 수에는 제한이 없으며, 각 표본의 수는 갖지 않아도 된다. F 검정 통계량을 이용한다. 3.4.2.2 일원배치 분산분석의 가정 각 집단의 측정치는 서로 독립적이며, 정규분포를 따른다. 각 집단 측정치의 분산은 같다. (등분산 가정) 3.4.2.3 분산분석표 요인 제곱합(SS) 자유도(df) 평균제곱(MS) 분산비(F) 처리 SSA k-1 (k: 집단의 수) MSA F=MSA/MSE 오차 SSE N-k (N: 관측수) MSE 전체 SST N-1 3.4.2.4 가설 검정 귀무가설: k개의 집단간 모평균에는 차이가 없다. 대립가설: k개의 집단간 모평균이 모두 같다고 할 수 없다. 3.4.2.5 사후 검정 사후검정이란 분산분석의 결과 귀무가설이 기각되어 적어도 한 집단에서 평균의 차이가 있음이 통계적으로 증명되었을 경우, 어떤 집단들에 대해서 평균의 차이가 존재하는지를 알아보기 위해 실시하는 분석이다. 사후분석의 종류로는 던칸의 MRT방법, 피셔의 최소유의차 방법, 튜키의 HSD방법, Scheffe의 방법 등이 있다. 3.4.2.6 R을 활용한 일원배치 분산분석 R에서 분산분석을 수행하기 위해 사용하는 함수는 aov이며, 주의할 점은 그룹을 구분하는 기준이 되는 변수는 반드시 팩터형이어야 한다는 것이다. aov(formula, data) 등분산 검정의 결과로 귀무가설이 기각되었을 경우, 어떠한 집단들 사이에서 통계적으로 유의한 차이가 있는지를 알아보기 위해 수행하는 사후분석에는 다양한 방법이 있다. 그 중 Tukey의 HSD 검정법을 수행할 수 있는 R의 TukeyHSD 함수는 아래와 같다. TukeyHSD(x, conf.level=0.95, ...) Q. iris 데이터를 이용하여 종별로 꽃받침의 폭의 평균이 같은지 혹은 차이가 있는지를 확인하기 위해 일원배치 분산분석을 수행해 보자. 검정을 수행하기에 앞서 설정할 수 있는 가설은 아래와 같다. 귀무가설: 세가지 종에 대해 Sepal.Width의 평균은 모두 같다. 대립가설: 적어도 하나의 종에 대한 Sepal.Width의 평균값에는 차이가 있다. 1) 분산분석 result&lt;-aov(Sepal.Width~Species, data=iris) #분산분석표 확인 summary(result) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 11.35 5.672 49.16 &lt;2e-16 *** ## Residuals 147 16.96 0.115 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 분산분석표를 통해 확인한 결과, SSA의 자유도는 2(집단의 수-1=3-1), SSE의 자유도는 147 (관측값의 수 - 집단의 수 = 150 -3) 임을 확인할 수 있다. 분석결과, p-value값(&lt;2e-16)이 매우 작게 나와 유의수준 0.05 하에서 귀무가설을 기각한다. 따라서 세가지 종에 따른 꽃받침 폭의 평균이 모두 동일하지는 않다고 결론 내릴 수 있다. 즉, 적어도 어느 하나의 종의 꽃받침 폭 평균값은 나머지 종들과는 통계적으로 유의한 차이가 있다고 말할 수 있다. 그렇다면, 세가지 종들 중 특히 어떤 종들간에 꽃받침의 폭에 차이가 있는지를 파악하기 위해 사후검정을 수행해 보자. 2) 사후검정 TukeyHSD(aov(Sepal.Width~Species, data=iris)) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Sepal.Width ~ Species, data = iris) ## ## $Species ## diff lwr upr p adj ## versicolor-setosa -0.658 -0.81885528 -0.4971447 0.0000000 ## virginica-setosa -0.454 -0.61485528 -0.2931447 0.0000000 ## virginica-versicolor 0.204 0.04314472 0.3648553 0.0087802 사후분석에서는 귀무가설을 ’집단들 사이의 평균은 같다’로 두고, 대립가설을 ’집단들 사이의 평균은 같지 않다’로 둔다. 그리고 모든 집단 수준에 대해서 두 집단씩 짝을 지어 각각 다중비교를 수행한다. 예제의 사후분석 결과를 보면 versicolor-setosa, virginica-setosa, virginica-versicolor의 세가지 비교에 대해서 모두 수정된 p-value값 (p adj)이 0.05 보다 작으므로, 각각의 비교에 대한 귀무가설을 모두 기각한다. 즉 모든 종들에 대해서 꽃받침 폭의 평균값은 통계적으로 유의한 차이가 있다는 것을 알 수 있다. 또한 diff는 하이픈(-)의 왼쪽 집단과 오른쪽 짐단 간 반응값의 차이를 타나내는데, versicolor-setosa에 대한 diff값은 음수이므로, versicolor일 때보다 setosa일 때 곷받침의 폭이 통계적으로 유의하게 큰 값을 가진다고 해석할 수 있다. 3.4.3 이원배치 분산분석 (Two-way ANOVA) 3.4.3.1 이원배치 분산분석의 개념 분산분석에서 반응값에 대해 두 개의 범주형 변수 A, B의 영향을 알아보기 위해 사용되는 검정방법이다. 예를 들어 성별과 학년에 따른 시험점수의 차이에 대해 통계적으로 검정하기 위해 이원배치 분산분석을 사용할 수 있다. 두 독립변수 A, B 사이에 상관관계가 있는지를 살펴보는 교호작용(두 독립변수의 범주들의 조함으로 인해 반응변수에 미치는 특별한 영향)에 대한 검증이 반드시 진행되어야 한다. 3.4.3.2 이원배치 분산분석의 가정 각 집단 측정치의 분포는 정규분포이어야 한다. (정규성) 집단간 측정치의 분산은 같다. (등분산성) 3.4.3.3 주효과와 교호작용효과 이원배치 분산분석에서는 두 개의 독립변수값에 따르는 데이터의 주효과와 상호작용효과에 대한 검정을 수행한다. 주효과란 각각의 독립변수가 종속변수에 미치는 효과를 의미하며, 이를 검정하는 것을 주효과 검정이라 한다. 교효작용효과는 여러 독립변수들의 조합이 종속변수에 주는 영향을 의미한다. 즉 교호작용효과검정은 한 독립변수가 종속변수에 미치는 영향이 다른 독립변수의 수준에 따라서 달라지는지를 분석하는 것이다. 두 독립변수 A, B 사이에 상관관계가 존재할 경우, 교호작용이 있다는 의미이다. 교호작용이 없을 경우, 주효과 검정을 진행한다. 반면 교호작용이 있을 경우에는 검정이 무의미하다. 3.4.3.4 분산분석표 요인 자유도 제곱합 평균제곱합 F 요인A \\(I-1\\) (요인A의 수준수) SSA \\(MS_{A}=\\frac{SSA}{I-1}\\) \\(F_{A}=\\frac{MSA}{MSE}\\) 요인B \\(J-1\\) (요인B의 수준수) SSB \\(MS_{B}=\\frac{SSB}{J-1}\\) \\(F_{B}=\\frac{MSB}{MSE}\\) 상호작용 \\((I-1)(J-1)\\) SS~A*B~ \\(MS_{AB}=\\frac{SSAB}{(I-1)(J-1)}\\) \\(F_{AB}=\\frac{MSAB}{MSE}\\) 오차 \\(IJ(n-1)\\) SSE \\(MSE=\\frac{SSE}{IJ(n-1)}\\) 전체 SST \\(IJn-1\\) 3.4.3.5 가설 검정 귀무가설 \\(H_{0}\\): \\(\\alpha\\)변수에 따른 종속변수의 값(반응값)에는 차이가 없다. (\\(\\alpha_{1}=\\alpha_{2}=...=\\alpha_{a}=0\\)) \\(H_{0}\\): \\(\\beta\\)변수에 따른 종속변수의 값(반응값)에는 차이가 없다. (\\(\\beta_{1}=\\beta_{2}=...=\\beta_{b}=0\\)) \\(H_{0}\\): \\(\\alpha\\)과 \\(\\beta\\)변수의 상호작용 효과가 없다. (\\(\\alpha\\beta_{1}=\\alpha\\beta_{2}=...=\\alpha\\beta_{(a-1)(b-1)}=0\\)) 대립가설(H1) : Not H0 \\(H_{1}\\): \\(\\alpha\\)변수에 따른 종속변수의 값(반응값)에는 차이가 있다. \\(H_{1}\\): \\(\\beta\\)변수에 따른 종속변수의 값(반응값)에는 차이가 있다. \\(H_{1}\\): \\(\\alpha\\)과 \\(\\beta\\)변수의 상호작용 효과가 없다. 3.4.3.6 R을 활용한 이원배치 분산분석 [함수사용법] aov(formula, data) 두 개의 독립변수들 간의 상호작용효과를 시각화하기 위해서 interaction.plot 함수를 사용하여 상호작용효과 그래프를 그릴 수 있다. interaction.plot(x.factor, trace.factor, response) Q. mtcars 데이터는 32개의 차종에 대한 다양한 특성과 단위 연료당 주행거리를 담고 있다. am 변수는 변속기 종류이며, cyl변수는 실린더의 개수를 의미한다. 데이터를 분석에 적절한 형태로 전처리한 후, 변속기 종류와 실린더의 개수에 따라 주행거리 평균에 유의미한 차이가 존재하는지 이원 분산분석을 수행하고, 그 결과를 해석해 보자. 검정을 수행하기에 앞서 설정할 수 있는 가설은 아래와 같다. 주효과 검정에 대한 가설 귀무가설: 실린더 개수에 따른 주행거리의 차이는 존재하지 않는다. 대립가설: 실린더 개수에 따른 주행거리의 차이는 존재한다. 귀무가설: 변속기 종류에 따른 주행거리의 차이는 존재하지 않는다. 대립가설: 변속기 종류에 따른 주행거리의 차이는 존재한다. 상호작용효과 검정에 대한 가설 귀무가설: 변속기 종류와 실린더 개수 간에는 상호작용 효과가 없다. 대립가설: 변속기 종류와 실린더 개수 간에는 상호작용 효과가 있다. 1) 데이터 확인 및 전처리 data(&quot;mtcars&quot;) str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... # aov 함수를 사용하기 위해 독립변수인 cyl, am을 팩터형으로 변환 mtcars$cyl&lt;-as.factor(mtcars$cyl) mtcars$am&lt;-as.factor(mtcars$am) # cyl, am, mpg 변수들로만 구성된 분석용 데이터셋 생성 car&lt;-mtcars[,c(&quot;cyl&quot;, &quot;am&quot;, &quot;mpg&quot;)] str(car) ## &#39;data.frame&#39;: 32 obs. of 3 variables: ## $ cyl: Factor w/ 3 levels &quot;4&quot;,&quot;6&quot;,&quot;8&quot;: 2 2 1 2 3 2 3 1 1 2 ... ## $ am : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 1 1 1 1 1 1 1 ... ## $ mpg: num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... 2) 분산분석 수행 # 분산분석 수행 car_aov&lt;-aov(mpg~cyl*am, car) summary(car_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## cyl 2 824.8 412.4 44.852 3.73e-09 *** ## am 1 36.8 36.8 3.999 0.0561 . ## cyl:am 2 25.4 12.7 1.383 0.2686 ## Residuals 26 239.1 9.2 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 cyl 변수에 대한 p-value는 0.05보다 작으므로, 주효과 검정에서 ’실린더 개수에 따른 주행거리 평균간 차이는 존재하지 않는다’는 귀무가설을 기각한다. 따라서 실린더 개수에 따라 주행거리간 유의미한 차이는 존재한다고 해석할 수 있다. am 변수에 대한 p-value는 0.0561로 0.05보다 크므로, 주효과 검정에서 ’변속기 종류에 따른 주행거리 평균간 차이는 존재하지 않는다’는 귀무가설을 기각하지 않는다. cyl 변수와 am 변수간의 상호작용효과에 대한 검정결과, p-value는 0.2686으로 0.05보다 크므로 귀무가설을 기각하지 않는다. 따라서 실린더 개수와 변속기 종류 간에는 교호작용이 존재하지 않는다는 것을 알 수 있다. Q. 실린더 개수와 변속기 종류 사이에 상호작용 효과가 있는지 없는지를 시각화 해주는 상호작용 그래프를 그린 후 이를 해석해 보자. interaction.plot(car$cyl, car$am, car$mpg, col=c(&quot;red&quot;, &quot;blue&quot;)) 일반적으로 상호작용 그래프에서 두 선이 서로 교차하고 있을 시에는 x축에 있는 독립변수와 그래프에서 시각화된 독립변수 간에는 상호작용이 존재한다고 해설할 수 있다. 3.5 상관분석 3.5.1 상관분석 개념 3.5.1.1 상관분석 상관분석이란 두 변수 간 관계의 정도를 알아보기 위한 분석방법이다. 상관계수는 데이터간 상관관계가 얼마나 강한지를 수치화한 값으로 두 변수 간 관련성의 정도를 의미한다. 상관계수 값이 클수록 데이터 간의 관계가 존재한다는 의미를 가진다. 하지만 상관계수가 크다고 변수간 인과관계가 존재하는 것은 아니다. 상관계수를 계산하는 방법으로는 피어슨 상관계수, 스피어만 상관계수, 켄달의 순위 상관계수 등이 있으며, 일반적으로 피어슨 상관계수를 가장 많이 사용한다. 3.5.1.2 공분산과 상관계수 공분산은 두 확률변수가 함께 변화하는지의 정도를 측정하는 값으로, 양의 상관관계가 존재할 경우 양수값을 가지고, 반대로 음의 상관관계가 존재할 경우 음수값을 가진다. 공분산을 통해 상관성의 경향을 파악할 수는 있지만, 두 변수의 측정 단위 크기에 따라 값이 음의 무한대에서 양의 무한대 사이에 존재하게 되므로 절대적인 상관성의 정도를 파악하기에는 한계가 있다. 따라서 공분산을 두 변수의 표준편차 곱으로 나누어 표준화 시킨 상관계수를 이용해 두 변수간 상관성의 정도를 파악한다. 3.5.1.3 상관계수의 해석 두 변수가 서로 독립이라면 상관계수는 0이나, 상관계수가 0이라고 해서 반드시 두 변수가 독립인 것은 아니다. 3.5.2 상관분석의 유형 3.5.2.1 피어슨 상관계수 피어슨 상관계수는 두 연속형 자료가 모두 정규성을 따른다는 가정하에 선형적 상관관계를 측정하며, 상관계수는 -1부터 1 사이의 값을 가진다. 가장 많이 사용하며, 일반적으로 상관계수는 피어슨 상관계수를 의미한다. 3.5.2.2 스피어만 상관계수 두 변수가 정규성을 만족하지 않는 경우 혹은 변수가 순위 및 순서 형태로 주어지는 경우에 사용한다. 실제 값을 사용하는 대신 데이터에 순위를 매긴 후 그 순위에 대한 상관계수를 산출하는 비모수적 방법이다. -1부터 1사이의 값을 가지며, 피어슨 상관계수와 달리 비선형 관계의 연관성을 파악할 수 있다. 또한 연속형 자료가 아닌 이산형 혹은 순서형 자료에도 적용이 가능하다. 3.5.2.3 켄달의 상관계수 켄달의 순위상관계수는 데이터가 \\((X_{i}, Y_{i})\\)와 같이 순서쌍으로 주어져 있을 때, Xi가 커짐에 따라 Yi도 커질 경우를 부합, Xi가 커짐에 따라 Yi가 작아질 경우를 비부합이라고 본다. 전체 데이터에서 비부합쌍에 대한 부합쌍의 비율로 상관계수를 산출한다. -1에서 1 사이의 값을 가지며, 순위상관계수가 1일 경우 데이터에서 부합쌍의 비율이 100%임을 나타내고 순위상관계수가 -1일 경우 비부합쌍의 비율이 100%임을 나타낸다. 순위상관계수가 0일 경우에는 두 변수 X와 Y는 상관성이 없음을 의미한다. 3.5.3 상관계수 검정 3.5.3.1 상관계수에 대한 검정 귀무가설: 변수1과 변수2 간에는 상관관계가 없다. (상관계수=0) 대립가설: 변수1과 변수2 간에는 상관관계가 없다. (상관계수!=0) 상관계수에 대한 검정 결과로 얻은 p-value 값이 0.05 이하인 경우, 귀무가설을 기각하게 되므로 데이터에서 산출한 상관계수를 활용할 수 있다. 3.5.3.2 R을 이용한 상관분석 [함수 사용법] cor(x, y, method=c(&quot;pearson&quot;,&quot;kendal&quot;,&quot;spearman&quot;),use) 인자 인자값 x y method 상관계수를 계산할 유형 use na값 처리방법 산출한 상관계수에 대한 가설 검정을 수행할 때는 cor.test를 사용한다. cor.test(x, y, alternative=c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), method=c(&quot;pearson&quot;, &quot;kendall&quot;, &quot;spearman&quot;)) pairs(x, labels, ...) corrplot(corr, method, ...) Q1. airquality 데이터에서 Ozone, Slar.R, Wind, Temp 만으로 이루어진 데이터프레임 air를 생성하고, 네 가지 변수에 대한 상관계수를 산출해 보자. 단, 모든 변수값에 NA가 없는 데이터들만 이용하여 피어슨, 켄달, 스피어만 상관계수를 모두 산출하자. data(&quot;airquality&quot;) air&lt;-airquality[,c(1:4)] str(air) ## &#39;data.frame&#39;: 153 obs. of 4 variables: ## $ Ozone : int 41 36 12 18 NA 28 23 19 8 NA ... ## $ Solar.R: int 190 118 149 313 NA NA 299 99 19 194 ... ## $ Wind : num 7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ... ## $ Temp : int 67 72 74 62 56 66 65 59 61 69 ... #피어슨 상관계수 cor(air, use=&quot;pairwise.complete.obs&quot;, method=&quot;pearson&quot;) ## Ozone Solar.R Wind Temp ## Ozone 1.0000000 0.34834169 -0.60154653 0.6983603 ## Solar.R 0.3483417 1.00000000 -0.05679167 0.2758403 ## Wind -0.6015465 -0.05679167 1.00000000 -0.4579879 ## Temp 0.6983603 0.27584027 -0.45798788 1.0000000 #켄달 상관계수 cor(air, use=&quot;pairwise.complete.obs&quot;, method=&quot;kendall&quot;) ## Ozone Solar.R Wind Temp ## Ozone 1.0000000 0.2403194214 -0.4283602915 0.5862988 ## Solar.R 0.2403194 1.0000000000 0.0006785596 0.1442337 ## Wind -0.4283603 0.0006785596 1.0000000000 -0.3222418 ## Temp 0.5862988 0.1442336719 -0.3222417514 1.0000000 #스피어만 상관계수 cor(air, use=&quot;pairwise.complete.obs&quot;, method=&quot;spearman&quot;) ## Ozone Solar.R Wind Temp ## Ozone 1.0000000 0.3481864700 -0.5901551241 0.7740430 ## Solar.R 0.3481865 1.0000000000 -0.0009773325 0.2074275 ## Wind -0.5901551 -0.0009773325 1.0000000000 -0.4465408 ## Temp 0.7740430 0.2074275160 -0.4465407773 1.0000000 Temp와 Ozone 변수간 상관계수의 절대값이 가장 큰 것을 확인할 수 있다. Temp와 Ozone 간 피어슨 상관계수는 약 0.698로 두 변수는 양의 상관관계를 갖고 있음을 알 수 있다 . 반면, Solar.R과 Wind 변수간 피어슨 상관계수는 -0.056으로 절대값이 0에 가까워 상관성을 거의 가지고 있지 않음을 알 수 있다. Q2. air 데이터 내의 네 가지 변수 조합별 피어슨 상관계를 그래프로 시각화해 보자. air_cor&lt;-cor(air, use=&quot;pairwise.complete.obs&quot;) air_cor ## Ozone Solar.R Wind Temp ## Ozone 1.0000000 0.34834169 -0.60154653 0.6983603 ## Solar.R 0.3483417 1.00000000 -0.05679167 0.2758403 ## Wind -0.6015465 -0.05679167 1.00000000 -0.4579879 ## Temp 0.6983603 0.27584027 -0.45798788 1.0000000 pairs(air_cor) Q3. air 데이터의 Ozone과 Wind 변수에 대한 상관분석을 실시하고, 피어슨 상관계수에 대한 검정 결과를 해석해 보자. cor.test(air$Ozone, air$Wind, method=&quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: air$Ozone and air$Wind ## t = -8.0401, df = 114, p-value = 9.272e-13 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.7063918 -0.4708713 ## sample estimates: ## cor ## -0.6015465 검정결과 p-value가 9.272e-13 이므로 유의수준 0.05 하에서 귀무가설을 기각한다. Ozone과 Wind 변수간 상관계수는 0이 아니며, 계산된 -0.6015465를 상관계수로 활용할 수 있다. Ozone과 Wind간 상관계수는 음수이므로 둘 중 어느 한 변수값이 증가하면 다른 변수 값은 감소함을 알 수 있다. 절대값은 약 0.6이므로 두 변수는 약한 음의 상관관계를 가지고 있다고 해석할 수 있다. 3.6 회귀분석 3.6.1 회귀분석의 개념 하나 혹은 그 이상의 원인이 결과에 미치는 영향을 추정하여 식으로 표현할 수 있는 통계기법. 변수들 사이의 인과관계를 밝히고 모형을 적합하여 관심있는 변수를 예측하거나 추론하기 위해 사용하는 분석방법이다. 독립변수의 개수가 하나이면 단순선형회귀분석, 독립변수의 개수가 두 개 이상이면 다중선형회귀분석으로 분석할 수 있다. 3.6.1.1 선형회귀분석의 가정 독립변수와 종속변수 간의 선형성: 입력변수와 출력변수의 관계가 선형이어야 한다는 가정. 오차의 등분산성:오차(Error)란 종속변수의 예측값과 실제 관측값 간의 차이를 의미한다. 오차의 등분산성이란 오차의 분산은 독립변수 값과 무관하게 일정해야 한다는 가정이다. 잔차플롯(산점도)을 그렸을 때, 잔차와 독립변수간 아무런 관련성이 없게 점들이 무작위적으로 고르게 분포되어야 등분산성 가정을 만족하게 된다. 오차의 독립성: 오차들은 서로 독립적이라는 가정이다. 즉 예측값의 변화에 따라 오차항이 특정한 패턴을 가져서는 안된다. 오차의 정규성: 오차의 분포가 정규분포를 만족해야함을 의미한다. Q-Q plot, kolmogorov-Smirnov검정, Shapiro-Wilk 검정 등을 활용하여 정규성을 확인한다. 3.6.1.2 그래프를 활용한 선형회귀분석의 가정 검토 3.6.1.3 선형성 3.6.1.4 등분산성 3.6.1.5 정규성 3.6.2 단순선형회귀분석 3.6.2.1 단순선형회귀분석 (다변량 회귀분석) 하나의 독립변수가 종속변수에 미치는 영향을 추정할 수 있는 총계기법 다음과 같은 식으로 표현하며 \\(\\beta_{0}\\)는 절편, \\(\\beta_{1}\\)는 독립변수 \\(x_{1}\\)의 계수, \\(\\epsilon_{1}\\)는 오차를 나타낸다.\\(Y_{i}=\\beta_{0}+\\beta_{1}x_{1}+\\epsilon_{i}\\) 회귀분석은 회귀계수를 찾아 독립변수와 종속변수 사이의 구체적인 함수식을 생성하고, 이 회귀계수가 통계적으로 유의미한지를 파악한다. 또한 통계적으로 유의하다고 판단되는 회귀모형을 이용해 종속변수를 예측할 수 있다. 3.6.2.2 회귀분석시 검토사항 모형 내의 회귀계수가 유의한가? 회귀계수에 대한 t통계량의 p-value가 0.05보다 작으면 해당 회귀계수가 통계적으로 유의하다고 볼 수 있다. 회귀계수의 절대값이 클수록 종속변수에 더욱 큰 영향을 준다. 모형은 데이터를 얼마나 설명할 수 있는가? 결정계수(R2)를 확인한다. 결정계수는 0 ~ 1값을 가지며, 추정된 회귀식이 전체 데이터에서 설명할 수 있는 데이터의 비율을 의미한다. 따라서 높은 값을 가질수록 추정된 회귀식의 설명력이 높다고 할 수 있다. 다변량 회귀분석에서는 포함된 독립변수의 유의성과 가ㅗㄴ계없이 독립변수의 수가 많아지면 결정계수(R2)가 높아진다. 이러한 점을 보완하기 위해 수정된 결정계수(\\(R_{a}^{2}\\): adjusted R2)를 활용하여 모형의 설명력을 판단한다. 회귀모형은 통계적으로 유의한가? 회귀분석의 결과로 산출되는 F-통계량의 p-value가 0.05보다 작으면 해당 회귀식은 통계적으로 유의하다고 볼 수 있다. 모형이 데이터를 잘 적합하고 있는가? 모형의 잔차를 그래프로 그리고, 회귀진단을 수행하여 판단한다. 3.6.2.3 R을 이용한 단순선형회귀분석 [함수 사용법] lm(formula, data) Q. Cars93 데이터의 엔진크기를 독립변수, 가격을 종속변수로 설정하여 단순 선형 회귀분석을 실기한 후, 추정된 회귀모형에 대해 해석해 보자. library(MASS) data(&quot;Cars93&quot;) str(Cars93) ## &#39;data.frame&#39;: 93 obs. of 27 variables: ## $ Manufacturer : Factor w/ 32 levels &quot;Acura&quot;,&quot;Audi&quot;,..: 1 1 2 2 3 4 4 4 4 5 ... ## $ Model : Factor w/ 93 levels &quot;100&quot;,&quot;190E&quot;,&quot;240&quot;,..: 49 56 9 1 6 24 54 74 73 35 ... ## $ Type : Factor w/ 6 levels &quot;Compact&quot;,&quot;Large&quot;,..: 4 3 1 3 3 3 2 2 3 2 ... ## $ Min.Price : num 12.9 29.2 25.9 30.8 23.7 14.2 19.9 22.6 26.3 33 ... ## $ Price : num 15.9 33.9 29.1 37.7 30 15.7 20.8 23.7 26.3 34.7 ... ## $ Max.Price : num 18.8 38.7 32.3 44.6 36.2 17.3 21.7 24.9 26.3 36.3 ... ## $ MPG.city : int 25 18 20 19 22 22 19 16 19 16 ... ## $ MPG.highway : int 31 25 26 26 30 31 28 25 27 25 ... ## $ AirBags : Factor w/ 3 levels &quot;Driver &amp; Passenger&quot;,..: 3 1 2 1 2 2 2 2 2 2 ... ## $ DriveTrain : Factor w/ 3 levels &quot;4WD&quot;,&quot;Front&quot;,..: 2 2 2 2 3 2 2 3 2 2 ... ## $ Cylinders : Factor w/ 6 levels &quot;3&quot;,&quot;4&quot;,&quot;5&quot;,&quot;6&quot;,..: 2 4 4 4 2 2 4 4 4 5 ... ## $ EngineSize : num 1.8 3.2 2.8 2.8 3.5 2.2 3.8 5.7 3.8 4.9 ... ## $ Horsepower : int 140 200 172 172 208 110 170 180 170 200 ... ## $ RPM : int 6300 5500 5500 5500 5700 5200 4800 4000 4800 4100 ... ## $ Rev.per.mile : int 2890 2335 2280 2535 2545 2565 1570 1320 1690 1510 ... ## $ Man.trans.avail : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 1 1 1 1 1 ... ## $ Fuel.tank.capacity: num 13.2 18 16.9 21.1 21.1 16.4 18 23 18.8 18 ... ## $ Passengers : int 5 5 5 6 4 6 6 6 5 6 ... ## $ Length : int 177 195 180 193 186 189 200 216 198 206 ... ## $ Wheelbase : int 102 115 102 106 109 105 111 116 108 114 ... ## $ Width : int 68 71 67 70 69 69 74 78 73 73 ... ## $ Turn.circle : int 37 38 37 37 39 41 42 45 41 43 ... ## $ Rear.seat.room : num 26.5 30 28 31 27 28 30.5 30.5 26.5 35 ... ## $ Luggage.room : int 11 15 14 17 13 16 17 21 14 18 ... ## $ Weight : int 2705 3560 3375 3405 3640 2880 3470 4105 3495 3620 ... ## $ Origin : Factor w/ 2 levels &quot;USA&quot;,&quot;non-USA&quot;: 2 2 2 2 2 1 1 1 1 1 ... ## $ Make : Factor w/ 93 levels &quot;Acura Integra&quot;,..: 1 2 4 3 5 6 7 9 8 10 ... # 단순선형 회귀모형 생성 (Cars93_lm &lt;- lm(Price~EngineSize, Cars93)) ## ## Call: ## lm(formula = Price ~ EngineSize, data = Cars93) ## ## Coefficients: ## (Intercept) EngineSize ## 4.669 5.563 # 모형 살펴보기: summary() 이용 # summary(): 주어진 인자에 대한 요약 정보 산출 summary(Cars93_lm) ## ## Call: ## lm(formula = Price ~ EngineSize, data = Cars93) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.684 -4.627 -1.795 2.592 39.429 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.6692 2.2390 2.085 0.0398 * ## EngineSize 5.5629 0.7828 7.107 2.59e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.789 on 91 degrees of freedom ## Multiple R-squared: 0.3569, Adjusted R-squared: 0.3499 ## F-statistic: 50.51 on 1 and 91 DF, p-value: 2.588e-10 회귀계수 ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.6692 2.2390 2.085 0.0398 * ## EngineSize 5.5629 0.7828 7.107 2.59e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 회귀계수는 Coefficients 항목을 통해 확인할 수 있다. (Intercept)는 상수항을 나타내며, Estimate는 추정된 회귀계수, Std.Error는 표준오차, t value는 t통계량, Pr(&gt;|t|)는 독립변수의 Estimate에 대한 p-value를 의미한다. 회귀분석 결과를 기준으로 엔진크기(EngineSize)와 가격(Price) 사이에는 [Price = 4.669 + 5.5629*EngineSize] 라는 회귀식을 도출할 수 있다. 상수항과 독립변수의 EngineSize의 회귀계수에 대한 p-value가 모두 유의수준 0.05보다 작으므로 통계적으로 유의하다고 판단할 수 있다. 이 검정에 사용되는 귀무가설은 ‘계수(혹은 절편)은 0이다’이며, 대립가설은 ‘계수(혹은 절편)는 0이 아니다’ 이다. 모형의 설명력 ## Residual standard error: 7.789 on 91 degrees of freedom ## Multiple R-squared: 0.3569, Adjusted R-squared: 0.3499 결과의 아래쪽에 제시된 Adjusted R-squared(수정된 결정계수)값을 통해 모형의 설명력을 파악할 수 있다. 수정된 결정계수가 0.3499라는 것은 해당 회귀 모형이 현 데이터의 약 34.99%를 설명할 수 있다는 것을 의미한다. 모형의 통계적 유의성 ## F-statistic: 50.51 on 1 and 91 DF, p-value: 2.588e-10 summary를 수행한 결과의 맨 아래에 있는 F-통계량을 통해 모형의 통계적 유의성을 판단할 수 있다. F-통계량의 p-value 가 2.588e-10로 0.05보다 매우 작기 때문에 추정된 회귀 모형 ’Price = 4.6692 + 5.5629 * EngineSize’는 통계적으로 유의하다고 할 수 있다. 3.6.2.4 R을 이용한 선형회귀모형 진단 plot.lm 함수를 이용하면 생성된 선형회귀모형에 대한 다양한 그래프를 통해 잔차의 분포를 파악하고 모형을 평가할 수 있다. [함수 사용법] plot.lm(x, which) Q. 위 예제에서 생성한 선형회귀모델 Cars93_lm을 평가할 수 있는 다양한 그래프를 생성한 후 해석해 보자. par(mfrow=c(2,3)) plot(Cars93_lm, which=c(1:6)) Residuals vs Fitted : Residuals vs Fitted 그래프에서 x축은 회귀모형을 통해 예측된 y값이며, y축은 잔차를 나타낸다. 선형회귀모형은 오차가 정규분포를 따른다는 정규성을 가정하므로, 이 그래프에서 오차의 분포는 기울기가 0인 직선의 형태를 가지는 것이 이상적이다. Normal Q-Q : Normal Q-Q plot은 표준화된 잔차의 확률도이다. 정규성 가정을 만족한다면 그래프의 점들은 45도 각도의 직선을 이루는 형태를 띄어야 한다. Scale-Location : Scale-Location plot에서 x축은 회귀모형을 통해 예측된 y값이며, y축은 표준화 잔차를 나타낸다. 첫번째 그래프와 마찬가지로 기울기가 0인 직선의 형태가 관측되는 것이 이상적이다. 해당 직선에서 멀리 떨어진 점이 있다면, 그 지점에서 회귀모형이 y값을 잘 예측하지 못함을 나타낸다. 또한 이 점은 이상치일 가능성이 있다. Cook’s distance : Cook’s distance plot의 x축은 관측값을 순서대로 나열한 것이며, y축은 해당 지점의 쿡의 거리를 나타낸다. 쿡의 거리는 한 관측치가 회귀모형에 미치는 영향을 나타내는 측도이며, 일반적으로 1이상일 경우 매우 큰 영향을 주는 관측값으로 간주한다. Residuals vs Leverage : Residuals vs Leverage의 x축은 레버리지, y축은 표준화 잔차값을 나타낸다. 레버리지란 관측치가 다른 관측치 집단으로부터 떨어진 정도를 나타내며 설명변수가 얼마나 극단에 치우쳐 있는지를 보여준다. 쿡의 거리가 0.5 이상인 빨간 점선의 밖에 있는 점은 예측치를 크게 벗어난 관측치이다. Cook’s dist vs Leverage : 해당 그래프의 x축은 레버리지, y축은 쿡의 거리를 나타낸다. 레버리지와 쿡의 거리는 비례하는 관계에 있다. 3.6.2.5 선형회귀모형을 활용한 예측 생성한 회귀모형에 새로운 독립변수 값을 입력하여 종속변수 값을 예측할 수 있다. 이 때 예측의 방법에는 점추정과 구간추정의 두가지 방법을 사용한다. 점추정이란 특정한 값 하나로 종속변수 값을 예측하는 것이다. 따라서 예측값의 불확실성은 고려되지 않으며, 예측가능성이 가장 높은 단일값을 제시한다. 구간추정이란 불확실성을 고려하여 단일값이 아닌 범위값으로 종속변수를 예측하는 것이다. 구간추정에서는 회귀식의 계수에 대한 불확실성과 회귀식을 통해 도출된 값의 오차로 인한 불확실성을 고려하여 결과값을 예측한다. [함수사용법] predict.lm(object, newdata, interval=c(&quot;none&quot;,&quot;confidence&quot;,&quot;prediction&quot;), level) Q. Cars93 데이터의 엔진크기를 독립변수, 가격를 종속변수로 설정하여 회귀모형을 생성한 후, Cars93 데이터이ㅡ 5개 행을 랜덤으로 봅아 가격(Price)를 예측해보자. 예측시 predict 함수의 interval 인자값을 조정하여 그 결과를 비교해보자. # 회귀모형 생성 Cars93_lm &lt;- lm(Price~EngineSize, Cars93) # 실습을 위해 시드값 설정 set.seed(1234) # Cars93 데이터에서 랜덤으로 5개의 행번호를 추출하여 idx변수에 저장 (idx &lt;- sample(1:nrow(Cars93),5)) ## [1] 28 80 22 9 5 # 예측에 사용할 데이터셋 구성 test &lt;- Cars93[idx,] # 예측 수행1 (점추정) predict.lm(Cars93_lm, test, interval=&quot;none&quot;) ## 28 80 22 9 5 ## 21.35801 11.34472 23.02689 25.80836 24.13948 predict.lm 함수에서 interval 인자의 값을 “none”으로 지정하여 자동차의 가격을 예측하면 점추정을 수행해 단일값이 산출되는 것을 확인할 수 있다. # 예측 수행2 (회귀계수의 불확실성을 감안한 구간추정) predict.lm(Cars93_lm, test, interval=&quot;confidence&quot;) ## fit lwr upr ## 28 21.35801 19.672604 23.04341 ## 80 11.34472 8.555107 14.13433 ## 22 23.02689 21.145364 24.90842 ## 9 25.80836 23.426530 28.19019 ## 5 24.13948 22.078345 26.20061 interval의 인자값을 “confidence”로 지정하여 자동차의 가격을 예측한 결과를 살펴보자. fit은 점추정한 값이며 lwr은 구간의 최소값, upr은 구간의 최대값이다. 예를 들어 28번행의 자동차 가격 예측값은 21.35801 이며, 19.672604 ~ 23.04341 사이일 확률은 95%라고 해석할 수 있다. predict.lm(Cars93_lm, test, interval=&quot;prediction&quot;) ## fit lwr upr ## 28 21.35801 5.795423 36.92060 ## 80 11.34472 -4.375825 27.06526 ## 22 23.02689 7.441846 38.61194 ## 9 25.80836 10.155035 41.46169 ## 5 24.13948 8.531732 39.74723 마지막으로 interval 인자의 값을 “prediction”으로 설정하면 회귀계수의 불확실서오가 오차항을 함께 감안하여 자동차의 가격을 예측한다. 결과값에 대한 오차까지 감안했기 때문에 interval을 “confidence”로 설정했을 때보다 더 넓은 구간으로 자동차 가격을 예측한다. 3.6.3 다중선형회귀분석 (다변량 회귀분석) 다중선형회귀분석은 2개 이상의 독립변수가 종속변수에 미치는 영향을 추정하는 통계기법이다. 다중선형회귀분석은 중선형회귀분석 혹은 다변량 회귀분석이라고도 한다. 3.6.3.1 다중선형회귀분석 시 검토사항 데이터가 전제하는 가정을 만족시키는가? 회귀분석을 수행하고자 하는 데이터의 독립변수와 종속변수간 선형성, 오차의 독립성/등분산성/정규성 등을 만족하고 있는지 확인해야 한다. 모형 내의 회귀계수가 유의한가? 단변량 회귀분석에서 회귀계수의 유의성 검토와 마찬가지로 회귀계수에 대한 t통계량의 p-value값이 0.05보다 작으면 해당 회귀 계수가 통계적으로 유의하다고 볼 수 있다. 단, 다중회귀분석을 할 때는 모든 회귀계수가 유의한지를 검정한 후 해당 회귀식을 해석해야 한다. 회귀계수 절대값이 클수록 종속변수에 더 큰 영향을 주므로, 회귀분석 결과를 통해 여러 변수중 어떤 독립변수가 종속변수에 대한 영향력이 큰지를 파악할 수 있다. 모형은 데이터를 얼마나 설명할 수 있는가? 결정계수(R2) 혹은 수정된 결정계수(\\(R_{a}^{2}\\): adjusted R2)를 확인한다. 회귀모형은 통계적으로 유의한가? 회귀분석의 결과로 산출되는 F-통계량의 p-value 값이 0.05보다 작으면 해당 회귀식은 통계적으로 유의하다고 볼 수 있다. 5. 모형이 데이터를 잘 적합하고 있는가? 모형의 잔차와 종속변수에 대한 산점도를 그리고, 회귀진단을 수행하여 판단한다. 다중공선성 다중공선성은 회귀분석에서 독립변수들 간에 강한 상관관관계가 나타나는 문제이다. 이러한 다중공선성의 문제가 존재하면 정확한 회귀계수의 추정이 곤란하다. 따라서 독립변수들 간 상관관계가 있는지를 파악한 후, 다중공선성의 문제가 발생하면 문제가 있는 변수를 제거하거나 주성분회귀, 능형회귀모형 등을 적용하여 문제를 해결한다. 다중공선성을 검사하는 방법은 아래와 같다. 독립변수들 간의 상관계수를 구하여 상관성을 직접 파악 허용오차를 구했을 때 0.1이하이면 다중공선성 문제가 심각하다고 할 수 있다. 호용오차란 한 독립변수의 분산 중 다른 독립변수들에 의해서 설명되지 않는 부분을 의미하므로, 그 값이 작을수록 공선성은 높다고 볼 수 있다. 또한 허용오차는 0 ~ 1 사이의 값을 가진다. (허용오차=(1-\\(R_{i}^{2}\\)),\\(R_{i}^{2}\\)) 3.6.3.2 더미변수 3.6.3.2.1 범주형 변수 변환 3.6.3.2.2 lm함수의 범주형 변수 처리 iris_lm&lt;-lm(Petal.Length~Sepal.Length+Sepal.Width+Petal.Width+Species, iris) summary(iris_lm) ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length + Sepal.Width + Petal.Width + ## Species, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.78396 -0.15708 0.00193 0.14730 0.65418 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.11099 0.26987 -4.117 6.45e-05 *** ## Sepal.Length 0.60801 0.05024 12.101 &lt; 2e-16 *** ## Sepal.Width -0.18052 0.08036 -2.246 0.0262 * ## Petal.Width 0.60222 0.12144 4.959 1.97e-06 *** ## Speciesversicolor 1.46337 0.17345 8.437 3.14e-14 *** ## Speciesvirginica 1.97422 0.24480 8.065 2.60e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2627 on 144 degrees of freedom ## Multiple R-squared: 0.9786, Adjusted R-squared: 0.9778 ## F-statistic: 1317 on 5 and 144 DF, p-value: &lt; 2.2e-16 Species 변수는 setosa, versicolor, verginica의 세가지 범주로 이루어져 있는 범주형 변수이다. summary 결과 중 Coefficients를 살펴본 바, Species 변수의 setosa 범주값을 기준으로 더미변수화가 수행되어 Species 변수가 Speciesversicolor와 Speciesverginica로 변환된 것을 살펴볼 수 있다. 회귀분석 결과 도출된 회귀식은 아래와 같다. Petal.Length = -1.11099 + 0.60801*Sepal.Length - 0.18052*Sepal.Width + 0.60222*Petal.Width + 1.46337*Speciesversicolor + 1.97422*Speciesverginica 3.6.3.3 R을 이용한 다중선형회귀분석 [함수 사용법] lm(formula, data) Q. Cars93 데이터에서 엔진크기, RPM, 무게를 독립변수로 설정하고 자동차 가격을 종속변수로 설정하여 다변량 회귀분석을 수행한 뒤 그 결과를 해석해 보자. library(MASS) str(Cars93) ## &#39;data.frame&#39;: 93 obs. of 27 variables: ## $ Manufacturer : Factor w/ 32 levels &quot;Acura&quot;,&quot;Audi&quot;,..: 1 1 2 2 3 4 4 4 4 5 ... ## $ Model : Factor w/ 93 levels &quot;100&quot;,&quot;190E&quot;,&quot;240&quot;,..: 49 56 9 1 6 24 54 74 73 35 ... ## $ Type : Factor w/ 6 levels &quot;Compact&quot;,&quot;Large&quot;,..: 4 3 1 3 3 3 2 2 3 2 ... ## $ Min.Price : num 12.9 29.2 25.9 30.8 23.7 14.2 19.9 22.6 26.3 33 ... ## $ Price : num 15.9 33.9 29.1 37.7 30 15.7 20.8 23.7 26.3 34.7 ... ## $ Max.Price : num 18.8 38.7 32.3 44.6 36.2 17.3 21.7 24.9 26.3 36.3 ... ## $ MPG.city : int 25 18 20 19 22 22 19 16 19 16 ... ## $ MPG.highway : int 31 25 26 26 30 31 28 25 27 25 ... ## $ AirBags : Factor w/ 3 levels &quot;Driver &amp; Passenger&quot;,..: 3 1 2 1 2 2 2 2 2 2 ... ## $ DriveTrain : Factor w/ 3 levels &quot;4WD&quot;,&quot;Front&quot;,..: 2 2 2 2 3 2 2 3 2 2 ... ## $ Cylinders : Factor w/ 6 levels &quot;3&quot;,&quot;4&quot;,&quot;5&quot;,&quot;6&quot;,..: 2 4 4 4 2 2 4 4 4 5 ... ## $ EngineSize : num 1.8 3.2 2.8 2.8 3.5 2.2 3.8 5.7 3.8 4.9 ... ## $ Horsepower : int 140 200 172 172 208 110 170 180 170 200 ... ## $ RPM : int 6300 5500 5500 5500 5700 5200 4800 4000 4800 4100 ... ## $ Rev.per.mile : int 2890 2335 2280 2535 2545 2565 1570 1320 1690 1510 ... ## $ Man.trans.avail : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 1 1 1 1 1 ... ## $ Fuel.tank.capacity: num 13.2 18 16.9 21.1 21.1 16.4 18 23 18.8 18 ... ## $ Passengers : int 5 5 5 6 4 6 6 6 5 6 ... ## $ Length : int 177 195 180 193 186 189 200 216 198 206 ... ## $ Wheelbase : int 102 115 102 106 109 105 111 116 108 114 ... ## $ Width : int 68 71 67 70 69 69 74 78 73 73 ... ## $ Turn.circle : int 37 38 37 37 39 41 42 45 41 43 ... ## $ Rear.seat.room : num 26.5 30 28 31 27 28 30.5 30.5 26.5 35 ... ## $ Luggage.room : int 11 15 14 17 13 16 17 21 14 18 ... ## $ Weight : int 2705 3560 3375 3405 3640 2880 3470 4105 3495 3620 ... ## $ Origin : Factor w/ 2 levels &quot;USA&quot;,&quot;non-USA&quot;: 2 2 2 2 2 1 1 1 1 1 ... ## $ Make : Factor w/ 93 levels &quot;Acura Integra&quot;,..: 1 2 4 3 5 6 7 9 8 10 ... Price_lm&lt;-lm(Price~EngineSize+RPM+Weight, Cars93) summary(Price_lm) ## ## Call: ## lm(formula = Price ~ EngineSize + RPM + Weight, data = Cars93) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.511 -3.806 -0.300 1.447 35.255 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -51.793292 9.106309 -5.688 1.62e-07 *** ## EngineSize 4.305387 1.324961 3.249 0.00163 ** ## RPM 0.007096 0.001363 5.208 1.22e-06 *** ## Weight 0.007271 0.002157 3.372 0.00111 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.504 on 89 degrees of freedom ## Multiple R-squared: 0.5614, Adjusted R-squared: 0.5467 ## F-statistic: 37.98 on 3 and 89 DF, p-value: 6.746e-16 1) 회귀모형의 포뮬러 ## Call: ## lm(formula = Price ~ EngineSize + RPM + Weight, data = Cars93) formula 부분에서 ~의 왼쪽에 있는 ‘Price’ 변수가 종속변수이고, ~의 오른쪽에 있는 ‘EnginSize’, ‘RPM’, ‘Weight’ 변수들이 독립변수에 해당한다. 또한 분석용 데이터는 Cars93임을 알 수 있다. 2) 회귀계수 ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -51.793292 9.106309 -5.688 1.62e-07 *** ## EngineSize 4.305387 1.324961 3.249 0.00163 ** ## RPM 0.007096 0.001363 5.208 1.22e-06 *** ## Weight 0.007271 0.002157 3.372 0.00111 ** Coefficients 항목을 통해 회귀계수를 확인해 본 결과, 상수항을 포함한 모든 회귀계수의 p-value 값이 0.05 보다 작은 것을 확인할 수 있다. 따라서 상수항과 세 변수에 대한 회귀계수는 모두 통계적으로 유의하다. 회귀분석 결과로 추정된 회귀식은 [Price = -51.793292 + 4.305387 * EngineSize + 0.007096 * RPM + 0.007271 * Weight] 이다. 3) 모형의 설명력 ## Residual standard error: 6.504 on 89 degrees of freedom ## Multiple R-squared: 0.5614, Adjusted R-squared: 0.5467 수정된 결정계수는 0.5467이므로 회귀모형이 전체 데이터의 약 54.67%를 설명할 수 있다. 수정된 결정계수 값이 조금 낮게 나타났기 때문에 해당 회귀식이 데이터를 적절하게 설명하고 있다고는 할 수 없다. 4) 모형의 통계적 유의성 ## F-statistic: 37.98 on 3 and 89 DF, p-value: 6.746e-16 F-통계량은 37.98이며, 유의확률이 6.746e-16이므로 유의수준 0.05 하에서 추정된 회귀모형이 통계적으로 매우 유의함을 알 수 있다. 결정계수가 낮기 때문에 모형이 데이터에 대해 가지는 설명력은 낮지만 회귀분석 결과에서 회귀계수들이 통계적으로 유의하므로, 자동차의 가격을 엔진크기와 RPM 그리고 무게로 추정할 수 있다. 3.6.3.4 최적화회귀방정식의 선택 모형 내 설명변수의 수가 증가할수록 데이터 관리에는 많은 노력이 요구된다. 따라서 상황에 따라 종속변수에 영향을 미치는 유의미한 독립변수들을 선택하여 최적의 회귀방정식을 도출하는 과정이 필요하다. 변수를 선택할 때는 F-통계량이나 AIC와 같은 특정 기준을 근거로 변수를 제거하거나 선택한다. F-통계량의 유의확률이 유의수준보다 큰 변수는 통계적으로 유의하지 않으므로 제거해야 하고, AIC와 같은 벌점화 기준을 가장 낮게 만드는 변수 조합을 선택해야 한다. 단계적 변수선택 (Stepwise Variable Selection) 전진 선택법: 절편만 있는 상수모형에서 시작하여 중요하다고 생각되는 설명변수부터 차례로 모형에 추가한다. 후진 제거법: 모든 독립변수를 포함한 모옇에서 출발하여 종속변수에 가장 적은 영향을 주는 변수부터 하나씩 제거하면서 더 이상 제거할 변수가 없을 때의 모형을 선택한다. 단계적 방법: 전진선택법에 의해 변수를 추가하면서 벌점화된 선택기준 3.6.3.5 R을 이용한 변수선택법 1) 패키지 로드 및 다중회귀모형 생성 library(MASS) summary(lm_a &lt;- lm(Price ~ EngineSize + RPM + Width + Length, Cars93)) ## ## Call: ## lm(formula = Price ~ EngineSize + RPM + Width + Length, data = Cars93) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.756 -4.239 -0.497 2.534 35.598 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -20.151609 26.264641 -0.767 0.445 ## EngineSize 8.380637 1.445736 5.797 1.04e-07 *** ## RPM 0.007139 0.001445 4.942 3.65e-06 *** ## Width -0.654923 0.433125 -1.512 0.134 ## Length 0.136676 0.088223 1.549 0.125 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.823 on 88 degrees of freedom ## Multiple R-squared: 0.5227, Adjusted R-squared: 0.501 ## F-statistic: 24.1 on 4 and 88 DF, p-value: 1.762e-13 summary의 결과에서 모형의 유의성을 판단하기 위해 F-통계량을 확인한 결과, 유의확률이 1.762e-13이므로 생성된 회귀모형은 통계적으로 유의함을 확인할 수 있다. 하지만 입력변수들의 통계적 유의성을 검토해 본 결과, Width와 Length 변수의 회귀계수에 대한 유의확률이 0.05보다 큰 것을 확인할 수 있다. 적절한 모형을 선정하기 위해 유의확률이 가장 높은 Width 변수를 제외하고 다시 회귀모형을 생성해 lm_b에 저장해 보자. 2) 유의확률이 가장 높은 변수 Width를 제거하고 회귀모형(lm_b)을 다시 생성 summary(lm_b &lt;- lm(Price ~ EngineSize + RPM + Length, Cars93)) ## ## Call: ## lm(formula = Price ~ EngineSize + RPM + Length, data = Cars93) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.478 -4.269 -0.378 2.004 36.681 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -52.601153 15.251540 -3.449 0.000862 *** ## EngineSize 7.110311 1.185062 6.000 4.18e-08 *** ## RPM 0.007492 0.001436 5.218 1.17e-06 *** ## Length 0.074112 0.078480 0.944 0.347549 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.872 on 89 degrees of freedom ## Multiple R-squared: 0.5103, Adjusted R-squared: 0.4938 ## F-statistic: 30.92 on 3 and 89 DF, p-value: 8.699e-14 Width 변수가 제거된 모형의 유의성을 검토한 결과, F-통계량에 대한 유의확률은 8.699e-14으로 유의하게 나타났다. 모든 변수들의 t통계량에 대한 유의확률이 0.05보다 낮아야 하지만 Length 변수의 유의확률이 0.05보다 높게 나타나 유의하지 않은 결과를 보인다. 따라서 유의확률이 가장 높은 Length 변수를 제외한 회귀모형(lm_c)을 다시 생성해 보자. 3) 유의확률이 가장 높은 변수 Length를 제거하고 회귀모형(lm_c)을 다시 생성 summary(lm_c&lt;-lm(Price ~ EngineSize + RPM, Cars93)) ## ## Call: ## lm(formula = Price ~ EngineSize + RPM, data = Cars93) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.636 -4.085 -0.946 1.645 36.543 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -40.977171 9.000017 -4.553 1.65e-05 *** ## EngineSize 7.913115 0.825140 9.590 2.03e-15 *** ## RPM 0.007457 0.001434 5.198 1.25e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.868 on 90 degrees of freedom ## Multiple R-squared: 0.5054, Adjusted R-squared: 0.4944 ## F-statistic: 45.99 on 2 and 90 DF, p-value: 1.74e-14 F-통계량을 확인한 결과 유의수준 0.05 하에서 모형이 통계적으로 유의함을 확인할 수 있다. 또한 다변량회귀식에 최종 선정된 EngineSize, RPM 변수에 대한 각각의 유의확률 값이 모두 통계적으로 유의하게 나타났다 .수정된 결정계수는 0.4944로 적합된 회귀식이 전체 데이터를 잘 설명하고 있다고 말하기는 힘들다. 회귀계수에 대한 유의확률을 기반으로 후진제거법을 수행하여 얻게된 회귀식은 Price = -40.977171 + 7.913115 * EngineSize + 0.007457 * RPM 이다. [함수 사용법] step(object, scope, direction, k) 인자 설명 object 변수선택을 진행할 회귀모형 scope 변수선택 과정에서 사용되는 모형의 범위list 내부에 모형의 상한은 upper, 하한은 lower에 지정 direction 변수선택 방법을 지정forward:전진선택법,backward:후진제거법,stepwise:단계적선택법 k 모형선택기준으로 AIC, BIC 등을 사용할지를 지정k=2: 모형선택기준으로 AIC 사용k=log(자료의수):모형 선택 기준으로 BIC 사용 Q. Cars93 데이터에서 엔진크기, 마력, RPM, 너비, 길이, 무게를 독립변수로 가지고, 자동차의 가격을 종속변수로 가지는 선형회귀모형을 생성해보자. 그 후 step 함수를 사용해 ’후진제거법’으로 변수 선택을 수행한 후 결과를 해석해 보자. lm_result&lt;-lm(Price~EngineSize+Horsepower+RPM+Width+Length+Weight, Cars93) step(lm_result, direction=&quot;backward&quot;) ## Start: AIC=322.11 ## Price ~ EngineSize + Horsepower + RPM + Width + Length + Weight ## ## Df Sum of Sq RSS AIC ## - EngineSize 1 1.69 2556.1 320.17 ## - RPM 1 19.71 2574.1 320.82 ## &lt;none&gt; 2554.4 322.11 ## - Length 1 119.55 2674.0 324.36 ## - Weight 1 209.73 2764.2 327.45 ## - Width 1 585.01 3139.4 339.29 ## - Horsepower 1 720.84 3275.3 343.22 ## ## Step: AIC=320.17 ## Price ~ Horsepower + RPM + Width + Length + Weight ## ## Df Sum of Sq RSS AIC ## - RPM 1 49.36 2605.5 319.95 ## &lt;none&gt; 2556.1 320.17 ## - Length 1 140.92 2697.0 323.16 ## - Weight 1 208.09 2764.2 325.45 ## - Width 1 593.56 3149.7 337.59 ## - Horsepower 1 1476.65 4032.8 360.57 ## ## Step: AIC=319.95 ## Price ~ Horsepower + Width + Length + Weight ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 2605.5 319.95 ## - Length 1 132.02 2737.5 322.54 ## - Weight 1 279.31 2884.8 327.42 ## - Width 1 562.10 3167.6 336.12 ## - Horsepower 1 1898.74 4504.2 368.86 ## ## Call: ## lm(formula = Price ~ Horsepower + Width + Length + Weight, data = Cars93) ## ## Coefficients: ## (Intercept) Horsepower Width Length Weight ## 53.005861 0.129653 -1.480623 0.152968 0.007339 후진제거법은 모든 독립변수가 포함된 모형에서 시작하여 유의미하지 않은 변수들을 차례로 제거한다. Start 단계를 살펴보면 EngineSize 변수가 제거되었을 때 AIC 값이 가장 낮아짐을 확인할 수 있다. AIC값은 작을수록 더 좋은 모델임을 뜻하므로 첫번째 단계에서는 EngineSize 변수가 제거된 모형이 선택되었다. 다음 단계에서 RPM 변수가 제거되었을 경우 AIC값이 319.95로 가장 낮아지는 것을 볼 수 있다. 따라서 RPM 변수가 제거된 모형이 선택되었다. 이후에는 모형에 아무런 변화가 없을때(none) AIC 값이 가장 작다. 따라서 변수선택을 중단하고, 최종적으로 EngineSize와 RPM 변수가 제거된 Price ~ Horsepower + Width + Length + Weight 형태의 포뮬러가 회귀모형으로 선택되었다. 적합된 회귀식은 Price = 53.005861 + 0.129653 * Horsepower = 1.480623 * Width + 0.152968 * Length + 0.007339 * Weight 이다. "],["정형-데이터마이닝.html", "4 정형 데이터마이닝 4.1 데이터 분할과 성과분석 4.2 분류 분석 4.3 군집분석 4.4 연관분석", " 4 정형 데이터마이닝 4.1 데이터 분할과 성과분석 4.1.1 데이터 분할 4.1.1.1 sample [함수사용법] sample(x, size, replace=FALSE, prob...) Q. credit 데이터를 train, validation, test로 분할해보자. credit.df&lt;-read.csv(&quot;./data/german_credit_dataset.csv&quot;, header=TRUE, sep=&quot;,&quot;) str(credit.df) ## &#39;data.frame&#39;: 1000 obs. of 21 variables: ## $ credit.rating : int 1 1 1 1 1 1 1 1 1 1 ... ## $ account.balance : int 1 1 2 1 1 1 1 1 4 2 ... ## $ credit.duration.months : int 18 9 12 12 12 10 8 6 18 24 ... ## $ previous.credit.payment.status: int 4 4 2 4 4 4 4 4 4 2 ... ## $ credit.purpose : int 2 0 9 0 0 0 0 0 3 3 ... ## $ credit.amount : int 1049 2799 841 2122 2171 2241 3398 1361 1098 3758 ... ## $ savings : int 1 1 2 1 1 1 1 1 1 3 ... ## $ employment.duration : int 2 3 4 3 3 2 4 2 1 1 ... ## $ installment.rate : int 4 2 2 3 4 1 1 2 4 1 ... ## $ marital.status : int 2 3 2 3 3 3 3 3 2 2 ... ## $ guarantor : int 1 1 1 1 1 1 1 1 1 1 ... ## $ residence.duration : int 4 2 4 2 4 3 4 4 4 4 ... ## $ current.assets : int 2 1 1 1 2 1 1 1 3 4 ... ## $ age : int 21 36 23 39 38 48 39 40 65 23 ... ## $ other.credits : int 3 3 3 3 1 3 3 3 3 3 ... ## $ apartment.type : int 1 1 1 1 2 1 2 2 2 1 ... ## $ bank.credits : int 1 2 1 2 2 2 2 1 2 1 ... ## $ occupation : int 3 3 2 2 2 2 2 2 1 1 ... ## $ dependents : int 1 2 1 2 1 2 1 2 1 1 ... ## $ telephone : int 1 1 1 1 1 1 1 1 1 1 ... ## $ foreign.worker : int 1 1 1 2 2 2 2 2 1 1 ... set.seed(1111) idx&lt;-sample(3, nrow(credit.df), replace=TRUE, prob=c(0.5,0.3,0.2)) train&lt;-credit.df[idx==1,] validation&lt;-credit.df[idx==2,] test&lt;-credit.df[idx==3,] nrow(train) ## [1] 483 nrow(validation) ## [1] 293 nrow(test) ## [1] 224 4.1.1.2 createDataPartition caret 패키지에서 목적변수를 고려한 데이터 분리를 지원하며, 함수를 사용해 분리한 데이터는 변수값의 비율이 원본 데이터와 같게 유지된다. [함수사용법] createDataPartition(y, times, p, list=TRUE, ...) Q. credit 데이터를 train, test로 분할해 보자. install.packages(setdiff(&quot;caret&quot;, rownames(installed.packages()))) library(caret) ## Loading required package: lattice ## Loading required package: ggplot2 ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:sampling&#39;: ## ## cluster # 목적변수로 credit.rating을 지정, 생성할 데이터 분할은 1개로 지정, 훈련데이터는 70%로 설정 part&lt;-createDataPartition(credit.df$credit.rating, times=1, p=0.7) parts&lt;-as.vector(part$Resample1) train&lt;-credit.df[parts,] test&lt;-credit.df[-parts,] 4.1.2 성과분석 4.1.2.1 오분류표 (Confusion Matrix) 4.1.2.1.1 개념 목표 변수의 실제 범주와 모형에 의해 예측된 분류 범주 사이의 관계를 나타내는 표 TP (True Positive) TN (True Negative) FP (False Positive) FN (False Negative) 4.1.2.1.2 분석 지표 정분류율 : 전체 관측치 중 실제값과 예측치가 일치한 정도 \\(Accuracy=\\frac{TN+TP}{TN+TP+FN+FP}\\) 오분류율 : 전체 관측치 중 실제값과 예측치가 다른 정도 \\(1 - Accuracy\\) 민감도 (Sensitivity (TPR: True Positive Rate)) : 실제값이 True인 관측치 중 예측치가 적중한 정도 \\(Sensitivity=\\frac{TP}{TP+FN}\\) 특이도 (Specificity (TNR: True Negative Rate)) : 실제값이 False인 관측치 중 예측치가 적중한 정도 \\(Specificity=\\frac{TN}{TN+FP}\\) 정확도 (Precision) : True로 예측된 것 중 실제로 True인 것들의 비율 \\(Precision=\\frac{TP}{TP+FP}\\) 재현율 (Recall) : 실제 True인 값 중 True를 얼마나 찾았는지에 대한 비율 \\(Recall=\\frac{TP}{TP+FN} (=Sensitivity)\\) F1-score : 정확도와 재현율을 보정하여 하나의 지표로 나타낸 값 \\(F_{1}=2\\times \\frac{Precision \\times Recall}{Precision+Recall}\\) [함수사용법] confusionMatrix(data, reference) Q. 임의의 값을 활용하여 Confusion Matrix를 그려보자. install.packages(setdiff(&quot;caret&quot;, rownames(installed.packages()))) library(caret) predicted&lt;-factor(c(1,0,0,1,1,1,0,0,0,1,1,1)) actual&lt;-factor(c(1,0,0,1,1,0,1,1,0,1,1,1)) xtabs(~predicted + actual) ## actual ## predicted 0 1 ## 0 3 2 ## 1 1 6 sum(predicted==actual)/NROW(actual) # 정분류율을 직접 식으로 계산 ## [1] 0.75 confusionMatrix(predicted, actual) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 3 2 ## 1 1 6 ## ## Accuracy : 0.75 ## 95% CI : (0.4281, 0.9451) ## No Information Rate : 0.6667 ## P-Value [Acc &gt; NIR] : 0.3931 ## ## Kappa : 0.4706 ## ## Mcnemar&#39;s Test P-Value : 1.0000 ## ## Sensitivity : 0.7500 ## Specificity : 0.7500 ## Pos Pred Value : 0.6000 ## Neg Pred Value : 0.8571 ## Prevalence : 0.3333 ## Detection Rate : 0.2500 ## Detection Prevalence : 0.4167 ## Balanced Accuracy : 0.7500 ## ## &#39;Positive&#39; Class : 0 ## 4.1.2.2 ROC 그래프 ROC 그래프의 x축에는 FP Ratio(1-특이도)를 나타내며, y축에는 민감도를 나타내 이 두 평가값의 관계로 모형을 평가한다. 모형의 성과를 평가하는 기준은 ROC 그래프의 밑부분 면적이며, 면적이 넓을수록 좋은 모형으로 평가한다. [함수사용법] prediction(predictions, labels) performance(prediction.object, acc(accuracy), fpr(FP Rate), tpr(TP Rate), ...) Q. 임의의 값으로 ROC Curve를 그려보자. library(ROCR) set.seed(12345) probability&lt;-runif(100) (labels&lt;-ifelse(probability&gt;0.5&amp;runif(100)&lt;0.4, 1, 2)) ## [1] 1 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 1 2 2 1 2 2 2 2 1 2 2 1 2 2 2 2 2 2 ## [38] 1 1 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 1 2 2 2 2 1 2 1 2 2 ## [75] 2 2 2 2 2 2 1 2 2 2 2 2 1 2 1 2 2 2 2 1 2 1 1 2 2 2 pred&lt;-prediction(probability, labels) plot(performance(pred, &quot;tpr&quot;, &quot;fpr&quot;)) performance(pred, &quot;auc&quot;)@y.values # AUROC ## [[1]] ## [1] 0.1735986 4.2 분류 분석 분류 분석은 반응변수의 속성값에 대해 다양한 변수를 이용하여 모형을 구축하고 이를 사용해 새로운 자료에 대한 예측 및 분류를 수행하는 분석이다. 반응변수가 범주형인 경우의 예측 모형은 새로 입력되는 자료에 대한 분류가 주목적이며, 반응변수가 연속형인 경우에는 그 값을 예측하는 것이 주목적이다. 예측 민 분류 기법은 목표 마케팅, 성과예측, 의학진단, 사기검출, 제조 등 다양한 분야에 이용되고 있다. 4.2.1 로지스틱 회귀분석 로지스틱 회귀모형은 반응변수가 범주형인 경우에 적용되는 회귀분석 모형이다. 이 방법은 새로운 설명변수의 값이 주어질 때 반응변수의 각 범주에 속할 확률이 얼마인지를 추정하여, 추정 확률을 기준치에 따라분류하는 목적으로 활용된다. 이 때, 모형의 적합을 통해 추정된 확률을 사후확률 (Posterior Probability)라고 한다. 반응변수 y에 대한 다중 로지스틱 회귀모형은 다음과 같다. 로지스틱 회귀모형은 오즈(odds)의 관점에서 해석이 가능하다. exp(\\(\\beta_{1}\\))의 의미는 나머지 변수(x1, …,xk)가 주어질 때, 한단위 증가할 때마다 성공(y=1)의 오즈가 몇 배 증가하는지를 나타내는 값이다. 오즈비(odds ratio) : 오즈는 성공할 확률이 실패할 확률의 몇배인지를 나타내는 확률이며, 오즈비는 오즈의 비율이다. 4.2.1.1 R을 이용한 이항 로지스틱 회귀분석 [함수사용법] glm(formula, data, family=&quot;binomial&quot;...) 인자 설명 formula 수식(종속변수~독립변수) data 분석하고자 하는 데이터 family 분석에 따른 link function 선택, binomial(이항), gaussian(가우시안), Gamma(감마), poisson(포아송) 등이 있음. predict(model, newdata, type, ...) 인자 설명 model 개발한 모형 newdata 예측을 수행할 test 데이터 type 예측 결과의 유형 지정, link(log-odds값), class(범주형(factor)값), response(0~1 확률값) Q. credit 데이터를 분할하고, train 데이터로 로지스틱 회귀모델을 만들어 보자. credit&lt;-read.csv(&quot;./data/credit_final.csv&quot;) class(credit$credit.rating) # 종속변수 factor 변환 ## [1] &quot;integer&quot; credit$credit.rating&lt;-factor(credit$credit.rating) str(credit) ## &#39;data.frame&#39;: 1000 obs. of 21 variables: ## $ credit.rating : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ account.balance : int 1 1 2 1 1 1 1 1 3 2 ... ## $ credit.duration.months : int 18 9 12 12 12 10 8 6 18 24 ... ## $ previous.credit.payment.status: int 3 3 2 3 3 3 3 3 3 2 ... ## $ credit.purpose : int 2 4 4 4 4 4 4 4 3 3 ... ## $ credit.amount : int 1049 2799 841 2122 2171 2241 3398 1361 1098 3758 ... ## $ savings : int 1 1 2 1 1 1 1 1 1 3 ... ## $ employment.duration : int 1 2 3 2 2 1 3 1 1 1 ... ## $ installment.rate : int 4 2 2 3 4 1 1 2 4 1 ... ## $ marital.status : int 1 3 1 3 3 3 3 3 1 1 ... ## $ guarantor : int 1 1 1 1 1 1 1 1 1 1 ... ## $ residence.duration : int 4 2 4 2 4 3 4 4 4 4 ... ## $ current.assets : int 2 1 1 1 2 1 1 1 3 4 ... ## $ age : int 21 36 23 39 38 48 39 40 65 23 ... ## $ other.credits : int 2 2 2 2 1 2 2 2 2 2 ... ## $ apartment.type : int 1 1 1 1 2 1 2 2 2 1 ... ## $ bank.credits : int 1 2 1 2 2 2 2 1 2 1 ... ## $ occupation : int 3 3 2 2 2 2 2 2 1 1 ... ## $ dependents : int 1 2 1 2 1 2 1 2 1 1 ... ## $ telephone : int 1 1 1 1 1 1 1 1 1 1 ... ## $ foreign.worker : int 1 1 1 2 2 2 2 2 1 1 ... set.seed(123) idx&lt;-sample(1:nrow(credit), nrow(credit)*0.7, replace=FALSE) train&lt;-credit[idx,] test&lt;-credit[-idx,] logistic&lt;-glm(credit.rating~.,data=train,family=&quot;binomial&quot;) summary(logistic) ## ## Call: ## glm(formula = credit.rating ~ ., family = &quot;binomial&quot;, data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4763 -0.7811 0.4133 0.7147 2.0078 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.249e+00 1.419e+00 -2.994 0.002754 ** ## account.balance 8.687e-01 1.224e-01 7.096 1.28e-12 *** ## credit.duration.months -2.145e-02 1.072e-02 -2.000 0.045501 * ## previous.credit.payment.status 5.635e-01 1.897e-01 2.971 0.002973 ** ## credit.purpose -4.133e-01 1.111e-01 -3.721 0.000198 *** ## credit.amount -7.722e-05 5.011e-05 -1.541 0.123341 ## savings 3.531e-01 9.689e-02 3.645 0.000268 *** ## employment.duration 1.311e-01 1.003e-01 1.307 0.191067 ## installment.rate -1.986e-01 1.002e-01 -1.983 0.047357 * ## marital.status 1.724e-01 9.722e-02 1.774 0.076139 . ## guarantor 6.995e-01 3.548e-01 1.971 0.048679 * ## residence.duration -2.940e-02 9.385e-02 -0.313 0.754063 ## current.assets -2.963e-01 1.075e-01 -2.757 0.005828 ** ## age 1.587e-02 1.009e-02 1.573 0.115623 ## other.credits 4.845e-01 2.480e-01 1.953 0.050801 . ## apartment.type 4.437e-01 2.061e-01 2.152 0.031369 * ## bank.credits -2.773e-01 2.391e-01 -1.160 0.246186 ## occupation -1.608e-01 1.672e-01 -0.962 0.335964 ## dependents -1.087e-01 2.831e-01 -0.384 0.700955 ## telephone 4.068e-01 2.257e-01 1.803 0.071425 . ## foreign.worker 1.433e+00 8.141e-01 1.760 0.078390 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 846.57 on 699 degrees of freedom ## Residual deviance: 651.47 on 679 degrees of freedom ## AIC: 693.47 ## ## Number of Fisher Scoring iterations: 5 회귀계수의 p-value가 유의수준 0.05보다 높게 나타나는 변수가 많으므로, step 함수에서 단계적 선택법을 이용하여 로지스틱 회귀분석을 다시 실시한다. step.logistic&lt;-step(glm(credit.rating~1, data=train, family=&quot;binomial&quot;), scope=list(lower~1, upper=~account.balance+credit.duration.months+previous.credit.payment.status+credit.purpose+credit.amount+savings+employment.duration+installment.rate+marital.status+guarantor+residence.duration+current.assets+age+other.credits+apartment.type+bank.credits+occupation+dependents+telephone+foreign.worker), direction=&quot;both&quot;) ## Start: AIC=848.57 ## credit.rating ~ 1 ## ## Df Deviance AIC ## + account.balance 1 761.97 765.97 ## + savings 1 817.12 821.12 ## + credit.duration.months 1 818.40 822.40 ## + previous.credit.payment.status 1 821.29 825.29 ## + current.assets 1 832.86 836.86 ## + credit.amount 1 835.30 839.30 ## + age 1 835.37 839.37 ## + credit.purpose 1 837.72 841.72 ## + employment.duration 1 837.80 841.80 ## + other.credits 1 839.28 843.28 ## + foreign.worker 1 839.45 843.45 ## + installment.rate 1 842.94 846.94 ## + marital.status 1 843.24 847.24 ## + telephone 1 843.75 847.75 ## + apartment.type 1 844.24 848.24 ## &lt;none&gt; 846.57 848.57 ## + occupation 1 845.33 849.33 ## + guarantor 1 845.60 849.60 ## + bank.credits 1 845.79 849.79 ## + dependents 1 846.34 850.34 ## + residence.duration 1 846.39 850.39 ## ## Step: AIC=765.97 ## credit.rating ~ account.balance ## ## Df Deviance AIC ## + credit.duration.months 1 738.84 744.84 ## + previous.credit.payment.status 1 747.38 753.38 ## + current.assets 1 748.44 754.44 ## + savings 1 749.48 755.48 ## + foreign.worker 1 751.18 757.18 ## + credit.purpose 1 751.40 757.40 ## + age 1 752.09 758.09 ## + credit.amount 1 752.24 758.24 ## + other.credits 1 754.34 760.34 ## + employment.duration 1 756.99 762.99 ## + guarantor 1 757.16 763.16 ## + marital.status 1 759.13 765.13 ## + installment.rate 1 759.48 765.48 ## &lt;none&gt; 761.97 765.97 ## + occupation 1 760.38 766.38 ## + apartment.type 1 760.65 766.65 ## + telephone 1 760.86 766.86 ## + residence.duration 1 760.91 766.91 ## + dependents 1 761.72 767.72 ## + bank.credits 1 761.95 767.95 ## - account.balance 1 846.57 848.57 ## ## Step: AIC=744.84 ## credit.rating ~ account.balance + credit.duration.months ## ## Df Deviance AIC ## + previous.credit.payment.status 1 724.73 732.73 ## + savings 1 725.43 733.43 ## + credit.purpose 1 727.04 735.04 ## + age 1 730.32 738.32 ## + foreign.worker 1 731.34 739.34 ## + employment.duration 1 732.40 740.40 ## + current.assets 1 733.16 741.16 ## + other.credits 1 733.23 741.23 ## + guarantor 1 733.69 741.69 ## + marital.status 1 734.34 742.34 ## + apartment.type 1 735.60 743.60 ## + telephone 1 735.74 743.74 ## + installment.rate 1 736.81 744.81 ## &lt;none&gt; 738.84 744.84 ## + residence.duration 1 737.75 745.75 ## + occupation 1 738.60 746.60 ## + dependents 1 738.69 746.69 ## + bank.credits 1 738.81 746.81 ## + credit.amount 1 738.84 746.84 ## - credit.duration.months 1 761.97 765.97 ## - account.balance 1 818.40 822.40 ## ## Step: AIC=732.73 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status ## ## Df Deviance AIC ## + savings 1 709.53 719.53 ## + credit.purpose 1 713.08 723.08 ## + foreign.worker 1 717.65 727.65 ## + guarantor 1 718.72 728.72 ## + age 1 719.12 729.12 ## + current.assets 1 719.90 729.90 ## + employment.duration 1 720.59 730.59 ## + other.credits 1 720.85 730.85 ## + bank.credits 1 721.21 731.21 ## + marital.status 1 721.60 731.60 ## + apartment.type 1 722.13 732.13 ## + telephone 1 722.61 732.61 ## + installment.rate 1 722.70 732.70 ## &lt;none&gt; 724.73 732.73 ## + residence.duration 1 724.10 734.10 ## + occupation 1 724.21 734.21 ## + dependents 1 724.63 734.63 ## + credit.amount 1 724.70 734.70 ## - previous.credit.payment.status 1 738.84 744.84 ## - credit.duration.months 1 747.38 753.38 ## - account.balance 1 793.25 799.25 ## ## Step: AIC=719.53 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings ## ## Df Deviance AIC ## + credit.purpose 1 697.86 709.86 ## + guarantor 1 701.87 713.87 ## + foreign.worker 1 703.31 715.31 ## + current.assets 1 704.12 716.12 ## + age 1 704.83 716.83 ## + other.credits 1 705.56 717.56 ## + marital.status 1 705.89 717.89 ## + employment.duration 1 706.35 718.35 ## + apartment.type 1 706.49 718.49 ## + bank.credits 1 706.94 718.94 ## + installment.rate 1 707.42 719.42 ## &lt;none&gt; 709.53 719.53 ## + telephone 1 708.50 720.50 ## + occupation 1 709.13 721.13 ## + residence.duration 1 709.29 721.29 ## + dependents 1 709.40 721.40 ## + credit.amount 1 709.43 721.43 ## - savings 1 724.73 732.73 ## - previous.credit.payment.status 1 725.43 733.43 ## - credit.duration.months 1 733.39 741.39 ## - account.balance 1 760.85 768.85 ## ## Step: AIC=709.86 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings + credit.purpose ## ## Df Deviance AIC ## + current.assets 1 690.41 704.41 ## + foreign.worker 1 690.93 704.93 ## + guarantor 1 690.96 704.96 ## + age 1 692.75 706.75 ## + marital.status 1 692.89 706.89 ## + employment.duration 1 694.44 708.44 ## + apartment.type 1 694.56 708.56 ## + other.credits 1 694.89 708.89 ## &lt;none&gt; 697.86 709.86 ## + bank.credits 1 696.00 710.00 ## + installment.rate 1 696.12 710.12 ## + occupation 1 696.52 710.52 ## + telephone 1 696.92 710.92 ## + credit.amount 1 697.40 711.40 ## + dependents 1 697.63 711.63 ## + residence.duration 1 697.77 711.77 ## - credit.purpose 1 709.53 719.53 ## - savings 1 713.08 723.08 ## - previous.credit.payment.status 1 713.72 723.72 ## - credit.duration.months 1 722.75 732.75 ## - account.balance 1 751.20 761.20 ## ## Step: AIC=704.41 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings + credit.purpose + current.assets ## ## Df Deviance AIC ## + apartment.type 1 682.70 698.70 ## + age 1 683.31 699.31 ## + foreign.worker 1 684.57 700.57 ## + guarantor 1 684.94 700.94 ## + marital.status 1 685.00 701.00 ## + employment.duration 1 686.34 702.34 ## + other.credits 1 688.09 704.09 ## + telephone 1 688.35 704.35 ## &lt;none&gt; 690.41 704.41 ## + bank.credits 1 688.64 704.64 ## + installment.rate 1 689.04 705.04 ## + occupation 1 690.02 706.02 ## + residence.duration 1 690.11 706.11 ## + dependents 1 690.13 706.13 ## + credit.amount 1 690.34 706.34 ## - current.assets 1 697.86 709.86 ## - credit.purpose 1 704.12 716.12 ## - previous.credit.payment.status 1 705.30 717.30 ## - savings 1 706.36 718.36 ## - credit.duration.months 1 706.49 718.49 ## - account.balance 1 744.08 756.08 ## ## Step: AIC=698.7 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings + credit.purpose + current.assets + apartment.type ## ## Df Deviance AIC ## + foreign.worker 1 676.21 694.21 ## + guarantor 1 677.12 695.12 ## + age 1 678.76 696.76 ## + marital.status 1 679.37 697.37 ## + employment.duration 1 679.61 697.61 ## + other.credits 1 679.81 697.81 ## + installment.rate 1 680.47 698.47 ## + telephone 1 680.58 698.58 ## &lt;none&gt; 682.70 698.70 ## + bank.credits 1 681.07 699.07 ## + occupation 1 682.35 700.35 ## + residence.duration 1 682.39 700.39 ## + dependents 1 682.63 700.63 ## + credit.amount 1 682.63 700.63 ## - apartment.type 1 690.41 704.41 ## - current.assets 1 694.56 708.56 ## - previous.credit.payment.status 1 696.22 710.22 ## - credit.purpose 1 697.55 711.55 ## - savings 1 699.58 713.58 ## - credit.duration.months 1 700.14 714.14 ## - account.balance 1 735.18 749.18 ## ## Step: AIC=694.21 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings + credit.purpose + current.assets + apartment.type + ## foreign.worker ## ## Df Deviance AIC ## + guarantor 1 672.46 692.46 ## + age 1 672.50 692.50 ## + other.credits 1 673.01 693.01 ## + employment.duration 1 673.02 693.02 ## + marital.status 1 673.43 693.43 ## + telephone 1 673.89 693.89 ## &lt;none&gt; 676.21 694.21 ## + installment.rate 1 674.68 694.68 ## + bank.credits 1 674.79 694.79 ## + residence.duration 1 675.92 695.92 ## + occupation 1 675.94 695.94 ## + credit.amount 1 675.98 695.98 ## + dependents 1 676.17 696.17 ## - foreign.worker 1 682.70 698.70 ## - apartment.type 1 684.57 700.57 ## - current.assets 1 687.01 703.01 ## - previous.credit.payment.status 1 689.51 705.51 ## - credit.duration.months 1 691.63 707.63 ## - credit.purpose 1 691.85 707.85 ## - savings 1 692.44 708.44 ## - account.balance 1 730.93 746.93 ## ## Step: AIC=692.46 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings + credit.purpose + current.assets + apartment.type + ## foreign.worker + guarantor ## ## Df Deviance AIC ## + age 1 668.86 690.86 ## + other.credits 1 669.00 691.00 ## + employment.duration 1 669.15 691.15 ## + marital.status 1 669.93 691.93 ## + telephone 1 670.01 692.01 ## &lt;none&gt; 672.46 692.46 ## + bank.credits 1 670.79 692.79 ## + installment.rate 1 671.11 693.11 ## + credit.amount 1 672.07 694.07 ## + occupation 1 672.19 694.19 ## + residence.duration 1 672.20 694.20 ## - guarantor 1 676.21 694.21 ## + dependents 1 672.43 694.43 ## - foreign.worker 1 677.12 695.12 ## - apartment.type 1 680.81 698.81 ## - current.assets 1 682.08 700.08 ## - previous.credit.payment.status 1 686.53 704.53 ## - credit.purpose 1 687.00 705.00 ## - credit.duration.months 1 688.83 706.83 ## - savings 1 689.97 707.97 ## - account.balance 1 729.18 747.18 ## ## Step: AIC=690.86 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings + credit.purpose + current.assets + apartment.type + ## foreign.worker + guarantor + age ## ## Df Deviance AIC ## + other.credits 1 665.40 689.40 ## + marital.status 1 666.21 690.21 ## + bank.credits 1 666.82 690.82 ## &lt;none&gt; 668.86 690.86 ## + telephone 1 666.95 690.95 ## + employment.duration 1 667.09 691.09 ## + installment.rate 1 667.18 691.18 ## - age 1 672.46 692.46 ## + credit.amount 1 668.46 692.46 ## - guarantor 1 672.50 692.50 ## + occupation 1 668.56 692.56 ## + dependents 1 668.86 692.86 ## + residence.duration 1 668.86 692.86 ## - foreign.worker 1 673.34 693.34 ## - apartment.type 1 673.96 693.96 ## - current.assets 1 678.94 698.94 ## - previous.credit.payment.status 1 680.46 700.46 ## - credit.duration.months 1 683.44 703.44 ## - credit.purpose 1 683.80 703.80 ## - savings 1 685.13 705.13 ## - account.balance 1 726.24 746.24 ## ## Step: AIC=689.4 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings + credit.purpose + current.assets + apartment.type + ## foreign.worker + guarantor + age + other.credits ## ## Df Deviance AIC ## + marital.status 1 662.52 688.52 ## &lt;none&gt; 665.40 689.40 ## + telephone 1 663.54 689.54 ## + employment.duration 1 663.56 689.56 ## + installment.rate 1 663.60 689.60 ## + bank.credits 1 663.93 689.93 ## + credit.amount 1 664.86 690.86 ## - other.credits 1 668.86 690.86 ## + occupation 1 664.96 690.96 ## - age 1 669.00 691.00 ## - guarantor 1 669.27 691.27 ## + dependents 1 665.39 691.39 ## + residence.duration 1 665.39 691.39 ## - foreign.worker 1 670.15 692.15 ## - apartment.type 1 671.10 693.10 ## - current.assets 1 674.92 696.92 ## - previous.credit.payment.status 1 675.51 697.51 ## - credit.purpose 1 678.94 700.94 ## - credit.duration.months 1 679.13 701.13 ## - savings 1 681.77 703.77 ## - account.balance 1 723.42 745.42 ## ## Step: AIC=688.52 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings + credit.purpose + current.assets + apartment.type + ## foreign.worker + guarantor + age + other.credits + marital.status ## ## Df Deviance AIC ## + installment.rate 1 659.86 687.86 ## &lt;none&gt; 662.52 688.52 ## + telephone 1 660.70 688.70 ## + bank.credits 1 661.03 689.03 ## + employment.duration 1 661.22 689.22 ## - marital.status 1 665.40 689.40 ## + credit.amount 1 662.07 690.07 ## - guarantor 1 666.10 690.10 ## + occupation 1 662.11 690.11 ## - other.credits 1 666.21 690.21 ## - age 1 666.25 690.25 ## + dependents 1 662.49 690.49 ## - apartment.type 1 666.51 690.51 ## + residence.duration 1 662.52 690.52 ## - foreign.worker 1 666.90 690.90 ## - current.assets 1 671.86 695.86 ## - previous.credit.payment.status 1 671.87 695.87 ## - credit.purpose 1 676.96 700.96 ## - credit.duration.months 1 677.03 701.03 ## - savings 1 679.14 703.14 ## - account.balance 1 720.47 744.47 ## ## Step: AIC=687.86 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings + credit.purpose + current.assets + apartment.type + ## foreign.worker + guarantor + age + other.credits + marital.status + ## installment.rate ## ## Df Deviance AIC ## &lt;none&gt; 659.86 687.86 ## + credit.amount 1 657.88 687.88 ## + telephone 1 658.20 688.20 ## + bank.credits 1 658.44 688.44 ## + employment.duration 1 658.44 688.44 ## - installment.rate 1 662.52 688.52 ## - guarantor 1 663.09 689.09 ## - foreign.worker 1 663.52 689.52 ## + occupation 1 659.54 689.54 ## - marital.status 1 663.60 689.60 ## - other.credits 1 663.74 689.74 ## + dependents 1 659.77 689.77 ## + residence.duration 1 659.85 689.85 ## - age 1 664.07 690.07 ## - apartment.type 1 664.22 690.22 ## - previous.credit.payment.status 1 668.62 694.62 ## - current.assets 1 669.29 695.29 ## - credit.purpose 1 673.93 699.93 ## - credit.duration.months 1 674.24 700.24 ## - savings 1 676.80 702.80 ## - account.balance 1 716.56 742.56 summary(step.logistic) ## ## Call: ## glm(formula = credit.rating ~ account.balance + credit.duration.months + ## previous.credit.payment.status + savings + credit.purpose + ## current.assets + apartment.type + foreign.worker + guarantor + ## age + other.credits + marital.status + installment.rate, ## family = &quot;binomial&quot;, data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4734 -0.8128 0.4436 0.7404 1.8425 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.453085 1.295853 -3.436 0.000589 *** ## account.balance 0.877274 0.120850 7.259 3.89e-13 *** ## credit.duration.months -0.030855 0.008188 -3.768 0.000164 *** ## previous.credit.payment.status 0.484631 0.165356 2.931 0.003381 ** ## savings 0.377087 0.095484 3.949 7.84e-05 *** ## credit.purpose -0.395226 0.108290 -3.650 0.000263 *** ## current.assets -0.314207 0.103329 -3.041 0.002359 ** ## apartment.type 0.423587 0.202868 2.088 0.036798 * ## foreign.worker 1.371175 0.809628 1.694 0.090344 . ## guarantor 0.608202 0.347239 1.752 0.079853 . ## age 0.019056 0.009441 2.018 0.043540 * ## other.credits 0.483427 0.243663 1.984 0.047256 * ## marital.status 0.181255 0.093959 1.929 0.053720 . ## installment.rate -0.146927 0.090550 -1.623 0.104674 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 846.57 on 699 degrees of freedom ## Residual deviance: 659.86 on 686 degrees of freedom ## AIC: 687.86 ## ## Number of Fisher Scoring iterations: 5 총 20개의 독립변수 중 13개의 독립변수가 선택되었으며, *과 .은 각 유의확률에서 채택이 되는지를 알 수 있다. 로지스틱 회귀식은 아래와 같이 나타난다. \\(P(credit.rating)=\\frac{1}{1+exp[-(-1.45+0.88account.balance+...-0.15installment.rate)]}\\) estimate가 양수이면 독립변수가 1단위 증가할 때 확률이 1에 가까워지고, estimate가 음수이면 독립변수가 1단위 증가할 때 확률이 0에 가까워진다. library(caret) (pred&lt;-predict(step.logistic, test[,-1], type=&quot;response&quot;)) # 예측값을 &quot;response&quot;로 지정하여 확률값을 출력 ## 1 3 4 7 9 12 15 17 ## 0.4120408 0.6118378 0.8287752 0.9181769 0.8756254 0.6843918 0.6123956 0.7257466 ## 18 21 22 25 27 28 32 35 ## 0.8545862 0.6562587 0.8187122 0.9117270 0.3548313 0.8912199 0.6963481 0.5354274 ## 42 43 44 47 50 58 60 62 ## 0.5585623 0.8364706 0.7470300 0.7531220 0.5600975 0.8353797 0.8895306 0.7449967 ## 63 66 70 73 75 77 82 86 ## 0.9429548 0.9387643 0.8516873 0.8224228 0.9481515 0.9478558 0.9323424 0.6739541 ## 92 93 97 99 101 102 103 107 ## 0.7073080 0.2550983 0.8217615 0.6815670 0.9686422 0.2506009 0.7734371 0.9082236 ## 109 112 114 123 126 133 140 142 ## 0.5705113 0.5738746 0.8735354 0.6018558 0.9023466 0.9552089 0.9404364 0.9864618 ## 144 145 146 147 149 150 154 156 ## 0.9329562 0.7825584 0.9806404 0.7563203 0.7958647 0.7759120 0.5112201 0.8989627 ## 157 174 176 182 183 192 194 198 ## 0.2000154 0.6635239 0.8362179 0.7691743 0.2697873 0.7822958 0.9230847 0.7904401 ## 202 208 213 214 215 216 227 233 ## 0.9506486 0.8442133 0.4700817 0.9926162 0.8831600 0.9248918 0.4090421 0.8773947 ## 245 247 248 249 253 254 257 269 ## 0.9013957 0.8224225 0.9496794 0.7450681 0.8175941 0.6385456 0.6716667 0.8695574 ## 272 283 285 288 293 296 300 307 ## 0.9139702 0.9389755 0.3110148 0.9274089 0.9865352 0.7869555 0.9441139 0.7054924 ## 312 313 314 321 325 329 335 345 ## 0.9424433 0.9347673 0.4857308 0.9393010 0.9600646 0.9739688 0.9446268 0.8643433 ## 350 353 354 356 359 360 361 363 ## 0.8665074 0.7143366 0.6647453 0.9745540 0.6030884 0.9571675 0.9343947 0.9881441 ## 366 367 368 369 370 375 380 383 ## 0.8746333 0.8839657 0.6299201 0.1938767 0.4109438 0.9539802 0.9581369 0.8369876 ## 385 387 400 405 408 410 411 416 ## 0.9426122 0.7129784 0.6004036 0.7937358 0.6749529 0.8698886 0.9479245 0.7891121 ## 423 425 432 436 439 444 449 453 ## 0.8673777 0.8854057 0.9597774 0.9703585 0.2658115 0.9508754 0.8628265 0.6940124 ## 454 460 462 467 469 472 474 482 ## 0.6250801 0.9624819 0.7661106 0.9162244 0.3083106 0.8821974 0.9332950 0.7324789 ## 484 485 486 487 488 489 491 493 ## 0.9949158 0.9599063 0.8572638 0.8723732 0.9409401 0.8786549 0.8639081 0.1861704 ## 495 496 497 502 506 511 513 514 ## 0.8758798 0.8549234 0.8996813 0.8678649 0.9368716 0.9470541 0.5835893 0.7205346 ## 515 517 518 520 521 525 529 531 ## 0.7173306 0.5639953 0.5906169 0.7641548 0.6460617 0.8607718 0.7875209 0.9619511 ## 536 540 542 543 546 550 551 556 ## 0.3886664 0.4620445 0.6683025 0.7880538 0.7462993 0.9224374 0.5281521 0.6557149 ## 563 565 568 569 572 576 579 580 ## 0.2330092 0.8570038 0.8865552 0.9570444 0.9035184 0.4413908 0.6281038 0.3626785 ## 582 583 584 586 587 592 594 599 ## 0.6094103 0.8412318 0.4426206 0.6207627 0.5037450 0.4947543 0.1509256 0.6393267 ## 607 611 616 622 628 631 635 641 ## 0.9444137 0.4469861 0.4350562 0.6942851 0.9373949 0.4534486 0.1898027 0.6636863 ## 642 643 652 653 654 656 664 669 ## 0.4613032 0.6911431 0.9049479 0.8419885 0.9659676 0.8858289 0.6496323 0.8331154 ## 674 675 683 684 689 690 693 695 ## 0.8500149 0.7906134 0.9087419 0.9736985 0.8522894 0.3264172 0.7586834 0.8787157 ## 699 701 708 713 715 728 730 731 ## 0.7674790 0.7948799 0.8674318 0.9289444 0.8646000 0.7547228 0.7213844 0.5971793 ## 735 736 737 740 743 748 749 756 ## 0.5683151 0.9734926 0.6673813 0.6835835 0.7376732 0.8792291 0.7720190 0.1485896 ## 758 759 763 772 773 776 786 787 ## 0.1935449 0.2411321 0.7705442 0.3751020 0.6321914 0.1383704 0.8161353 0.5249610 ## 790 791 793 795 796 797 799 801 ## 0.5824022 0.3912481 0.3479640 0.8999023 0.3893990 0.9279937 0.2878022 0.5014722 ## 806 808 825 826 827 828 829 830 ## 0.1469686 0.2080182 0.2501811 0.7994888 0.2773739 0.6401503 0.6154802 0.4419988 ## 833 839 848 849 850 855 856 866 ## 0.7936517 0.4876081 0.1777989 0.6991339 0.7273259 0.9482981 0.1826371 0.9643884 ## 868 874 875 879 884 887 892 896 ## 0.6638910 0.6762806 0.2602226 0.3118001 0.5363162 0.2179213 0.5033415 0.7543552 ## 897 898 901 907 909 912 914 919 ## 0.9062067 0.5762438 0.5157371 0.2894535 0.6484000 0.5229902 0.1543145 0.2554326 ## 921 924 929 936 939 945 946 948 ## 0.2209400 0.3185682 0.3902414 0.7428232 0.4296006 0.6761346 0.5554851 0.9205723 ## 950 952 956 963 964 967 970 971 ## 0.9513133 0.9508658 0.6010311 0.1464600 0.3378311 0.6942551 0.3984775 0.4066565 ## 972 973 977 978 983 984 985 989 ## 0.2523158 0.9263933 0.4497004 0.4379014 0.5426059 0.1238137 0.6028227 0.2606977 ## 993 995 997 998 ## 0.9482636 0.2613449 0.5866442 0.9168300 pred1&lt;-as.data.frame(pred) pred1$grade&lt;-ifelse(pred1$pred&lt;0.5, pred1$grade&lt;-0, pred1$grade&lt;-1) confusionMatrix(data=as.factor(pred1$grade), reference=test[,1], positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 43 23 ## 1 52 182 ## ## Accuracy : 0.75 ## 95% CI : (0.697, 0.798) ## No Information Rate : 0.6833 ## P-Value [Acc &gt; NIR] : 0.006892 ## ## Kappa : 0.3708 ## ## Mcnemar&#39;s Test P-Value : 0.001224 ## ## Sensitivity : 0.8878 ## Specificity : 0.4526 ## Pos Pred Value : 0.7778 ## Neg Pred Value : 0.6515 ## Prevalence : 0.6833 ## Detection Rate : 0.6067 ## Detection Prevalence : 0.7800 ## Balanced Accuracy : 0.6702 ## ## &#39;Positive&#39; Class : 1 ## 구축된 로지스틱 회귀모형으로 test 데이터의 기존 credit.rating 열을 제외한 데이터로 예측을 한다. 정분류율을 확인하기 전에 예측값이 확률로 나타나기 때문에 기준이 되는 확률보다 크면 1, 작으면 0으로 범주를 추가한다. 정분류율(Accuracy)은 0.75이며, 민감도는 0.8878로 높게 나타났다. 또 특이도는 0.4526이다. 정확도가 높다고 해서 무조건 좋은 모형은 아니며, 분석 분야에 따라 다양한 지표들을 활용하여 분석 모형을 선택할 수 있다. install.packages(setdiff(&quot;ROCR&quot;, rownames(installed.packages()))) library(ROCR) pred.logistic.roc&lt;-prediction(as.numeric(pred1$grade), as.numeric(test[,1])) plot(performance(pred.logistic.roc, &quot;tpr&quot;, &quot;fpr&quot;)) abline(a=0, b=1, lty=2, col=&quot;black&quot;) performance(pred.logistic.roc,&quot;auc&quot;)@y.values ## [[1]] ## [1] 0.6702182 prediction 함수와 performance 함수로 값을 구하여 plot 함수로 ROC 커브를 그렸으며, AUC값은 @y.values값으로 확인할 결과 0.67로 나타났다. 4.2.1.2 R을 이용한 다항 로지스틱 회귀분석 예측하고자 하는 분류가 3개 이상이 된다면 다항 로지스틱 회귀분석을 사용한다. R에서는 nnet 패키지의 multinom 등의 함수로 분석을 한다. multinom(formula, data) Q. iris 데이터의 Species를 분류하는 다항 로지스틱 회귀분석을 실시하고 오분류표를 만들어 보자. idx&lt;-sample(1:nrow(iris), nrow(iris)*0.7, replace=FALSE) train.iris&lt;-iris[idx,] test.iris&lt;-iris[-idx,] library(nnet) mul.iris&lt;-multinom(Species~., train.iris) ## # weights: 21 (12 variable) ## initial value 115.354290 ## iter 10 value 11.770297 ## iter 20 value 6.059059 ## iter 30 value 5.722903 ## iter 40 value 5.717299 ## iter 50 value 5.710495 ## iter 60 value 5.709390 ## iter 70 value 5.708363 ## iter 80 value 5.706817 ## iter 90 value 5.706767 ## iter 100 value 5.706668 ## final value 5.706668 ## stopped after 100 iterations # 예측을 통한 정분류율 확인 pred.mul&lt;-predict(mul.iris, test.iris[,-5]) confusionMatrix(pred.mul, test.iris[,5]) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 10 0 ## virginica 0 0 20 ## ## Overall Statistics ## ## Accuracy : 1 ## 95% CI : (0.9213, 1) ## No Information Rate : 0.4444 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 1 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 1.0000 1.0000 ## Specificity 1.0000 1.0000 1.0000 ## Pos Pred Value 1.0000 1.0000 1.0000 ## Neg Pred Value 1.0000 1.0000 1.0000 ## Prevalence 0.3333 0.2222 0.4444 ## Detection Rate 0.3333 0.2222 0.4444 ## Detection Prevalence 0.3333 0.2222 0.4444 ## Balanced Accuracy 1.0000 1.0000 1.0000 4.2.2 의사결정나무 의사결정나무는 분류함수를 의사결정 규칙으로 이뤄진 나무 모양으로 그리는 방법이다. 계산 결과가 의사결정나무에 직접 나타나기 때문에 해석이 간편하다. 의사결정나무는 주어진 입력값에 대하여 출력값을 예측하는 모형으로 분류나무와 회귀나무 모형이 있다. 4.2.2.1 의사결정나무의 분석 과정 의사결정나무의 형성과정은 크게 성장, 가지치기, 타당성 평가, 해석 및 예측으로 이루어진다. 4.2.2.1.1 성장단계 각 마디에서 적절한 최적의 분류규칙을 찾아서 나무를 성장시키는 과정으로 적절한 정지규칙을 만족하면 중단한다. 분리 규칙을 설정하는 분리 기준은 이산형 목표변수, 연속형 목표변수에 따라 나뉘며 아래와 같은 기준값을 사용한다. 이산형 목표변수 기준값 분리기준 카이제곱 통계량 p값 p값이 가장 작은 예측변수와 그때의 최적분리에 의해서 자식마디를 형성 지니 지수 지니 지수를 감소시켜주는 예측변수와 그 때의 최적 분리에 의해서 자식 마디를 형성 엔트로피 지수 엔트로피 지수가 가장 작은 예측 변수와 이 때의 최적분리에 의해 자식 마디를 형성 연속형 목표변수 기준값 분리기준 분산분석에서 F통계량 p값이 가장 작은 예측변수와 그때의 최적분리에 의해서 자식마디를 형성 분산의 감소량 분산의 감소량을 최대화 하는 기준의 최적분리에 의해서 자식마디를 형성 정지규칙은 더 이상 분리가 일어나지 않고, 현재의 마디가 끝마디가 되도록 하는 규칙이며, 의사결정나무의 깊이를 지정하거나 끝마디의 레코드 수의 최소 개수를 지정한다. 4.2.2.1.2 가지치기 단계 오차를 크게 할 위험이 높거나 부적절한 추론 규칙을 가지고 있는 가지 또는 불필요한 가지를 제거하는 단계이다. 나무의 크기를 모형의 복잡도로 볼 수 있으며, 최적의 나무 크기는 자료로부터 추정하게 된다. 일반적으로 사용되는 방법은 마디에 속하는 자료가 일정수 이하일 때 분할을 정지하고 비용-복잡도 가지치기를 이용하여 성장시킨 나무를 가지치기하게 된다. 4.2.2.1.3 타당성 평가 단계 이익도표, 위험도표 혹은 시험자료를 이용하여 의사결정나무를 평가하는 단계이다. 4.2.2.1.4 해설 및 예측 단계 구축된 나무모형을 해석하고 예측모형을 설정한 후 예측에 적용하는 단계이다. 4.2.2.2 의사결정나무 알고리즘 4.2.2.2.1 CART (Classification and Regression Tree) 4.2.2.2.2 C4.5와 C5.0 4.2.2.2.3 CHAID (SHi-squared Automatic Interaction Detection) 4.2.2.3 R을 이용한 의사결정나무 분석 [함수사용법] rpart(formula, data, method, control=rpart.control(), ...) Q. 앞서 분할한 credit 데이터의 train 데이터로 의사결정나무 모델을 만들어 보자. library(rpart) library(rpart.plot) dt.model&lt;-rpart(credit.rating~., method=&quot;class&quot;, data=train, control=rpart.control(maxdepth=5, minsplit=15)) prp(dt.model, type=4, extra=2) 총 700개의 관측치 중 495개의 관측치를 1로 분류했으며, account.balance &gt;= 3인 325개의 노드 중 288이 1로 분류되었음을 의미한다. prp 함수는 rpart.plot 패키지에 속한 함수이며, type, extra 등의 인자를 사용하여 그래프의 모양을 바꿀 수 있다. # rpart 함수를 활용하여 의사결정나무분석 실시 (최적 나무 선정) dt.model$cptable ## CP nsplit rel error xerror xstd ## 1 0.05365854 0 1.0000000 1.0000000 0.05873225 ## 2 0.04390244 3 0.8341463 0.9853659 0.05847732 ## 3 0.03414634 4 0.7902439 0.9804878 0.05839093 ## 4 0.01000000 5 0.7560976 0.9756098 0.05830383 (opt&lt;-which.min(dt.model$cptable[,&quot;xerror&quot;])) ## 4 ## 4 (cp&lt;-dt.model$cptable[opt, &quot;CP&quot;]) ## [1] 0.01 (prune.c&lt;-prune(dt.model, cp=cp)) ## n= 700 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 700 205 1 (0.2928571 0.7071429) ## 2) account.balance&lt; 2.5 375 168 1 (0.4480000 0.5520000) ## 4) credit.duration.months&gt;=22.5 160 69 0 (0.5687500 0.4312500) ## 8) savings&lt; 2.5 128 47 0 (0.6328125 0.3671875) ## 16) credit.purpose&gt;=1.5 111 35 0 (0.6846847 0.3153153) * ## 17) credit.purpose&lt; 1.5 17 5 1 (0.2941176 0.7058824) * ## 9) savings&gt;=2.5 32 10 1 (0.3125000 0.6875000) * ## 5) credit.duration.months&lt; 22.5 215 77 1 (0.3581395 0.6418605) ## 10) previous.credit.payment.status&lt; 1.5 15 3 0 (0.8000000 0.2000000) * ## 11) previous.credit.payment.status&gt;=1.5 200 65 1 (0.3250000 0.6750000) * ## 3) account.balance&gt;=2.5 325 37 1 (0.1138462 0.8861538) * cptable 인자를 통해서 교차타당성 오차를 제공하여 의사결정나무 모델의 가지치기, 트리의 최대 크기조절에 사용한다. nsplit은 분할횟수, xerror는 해당 CP에서 cross validation 했을 때 오류율, xstd는 해당 CP에서 cross validation 했을 때 편차를 나타낸다. cptable에서 xerror가 가장 낮은 split 개수를 선택한다. 위 결과를 확인했을 때, xerror가 가장 낮을 때 nsplit은 5이며, 앞선 모형의 그래프를 봤을 때 의사 결정나무 모델이 분할을 5번까지 한다고 할 수 있다. plotcp(dt.model) plotcp의 결과에서도 xerror가 가장 낮을 때 결과에 따라 교차타당성오차를 최소로 하는 트리를 형성한다. 결과적으로 나무의 크기가 6일 때 최적의 나무라고 할 수 있다. install.packages(setdiff(&quot;caret&quot;, rownames(installed.packages()))) library(caret) pred.dt&lt;-predict(dt.model, test[,-1], type=&quot;class&quot;) confusionMatrix(data=pred.dt, reference=test[,1], positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 42 21 ## 1 53 184 ## ## Accuracy : 0.7533 ## 95% CI : (0.7005, 0.8011) ## No Information Rate : 0.6833 ## P-Value [Acc &gt; NIR] : 0.0047614 ## ## Kappa : 0.3734 ## ## Mcnemar&#39;s Test P-Value : 0.0003137 ## ## Sensitivity : 0.8976 ## Specificity : 0.4421 ## Pos Pred Value : 0.7764 ## Neg Pred Value : 0.6667 ## Prevalence : 0.6833 ## Detection Rate : 0.6133 ## Detection Prevalence : 0.7900 ## Balanced Accuracy : 0.6698 ## ## &#39;Positive&#39; Class : 1 ## 정분류율(Accuracy)은 0.7533며, 민감도는 0.8976로 높게 나타났다. 또, 특이도는 0.4421이다. 정확도가 높다고 해서 무조건 좋은 모형은 아니며, 분석 분야에 따라 다양한 지표들을 활용하여 분석모형을 선택할 수 있다. # ROC 커브 그리기 및 AUC 산출 install.packages(setdiff(&quot;ROCR&quot;, rownames(installed.packages()))) library(ROCR) pred.dt.roc&lt;-prediction(as.numeric(pred.dt), as.numeric(test[,1])) plot(performance(pred.dt.roc,&quot;tpr&quot;, &quot;fpr&quot;)) abline(a=0,b=1,lty=2,col=&quot;black&quot;) performance(pred.dt.roc,&quot;auc&quot;)@y.values ## [[1]] ## [1] 0.6698331 prediction 함수와 performance 함수로 값을 구하여 plot 함수로 ROC 커브를 그렸으며, AUC값은 @y.values값으로 확인한 결과 0.6698로 나타났다. Q. 앞서 분리한 iris 데이터의 Species를 분류하는 의사결정나무분석을 실시하고 오분류표를 만들어 보자. install.packages(setdiff(&quot;rpart&quot;, rownames(installed.packages()))) library(rpart) library(rpart.plot) dt.model2&lt;-rpart(Species~., data=train.iris) prp(dt.model2, type=4, extra=2) pred.dt2&lt;-predict(dt.model2, test.iris[,-5], type=&quot;class&quot;) confusionMatrix(data=pred.dt2, reference=test.iris[,5]) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 10 1 ## virginica 0 0 19 ## ## Overall Statistics ## ## Accuracy : 0.9778 ## 95% CI : (0.8823, 0.9994) ## No Information Rate : 0.4444 ## P-Value [Acc &gt; NIR] : 8.12e-15 ## ## Kappa : 0.9656 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 1.0000 0.9500 ## Specificity 1.0000 0.9714 1.0000 ## Pos Pred Value 1.0000 0.9091 1.0000 ## Neg Pred Value 1.0000 1.0000 0.9615 ## Prevalence 0.3333 0.2222 0.4444 ## Detection Rate 0.3333 0.2222 0.4222 ## Detection Prevalence 0.3333 0.2444 0.4222 ## Balanced Accuracy 1.0000 0.9857 0.9750 4.2.3 앙상블 기법 앙상블 기법은 주어진 자료로부터 여러개의 예측모형들을 만든 후 예측모형들을 조합하여 하나의 최종 예측모형을 만드는 방법이다. 학습방법이 가장 불안전한 의사결정나무에 주로 사용한다. 4.2.3.1 배깅 (Bagging) 4.2.3.1.1 개념 주어진 자료에서 여러개의 부트스트랩 자료를 생성하고 각 부트스트랩 자료에 예측모형을 만든후 결합하여 최종 예측모형을 만드는 방법이다. 보팅은 여러개의 모형으로부터 산출된 결과 중 다수결에 의해서 최종 결과를 선정하는 과정이다. 최적의 의사결정나무를 구축할 때 가장 어려운 부분이 가지치기이지만 배깅에서는 가지치기를 하지 않고 최대로 성정한 의사결정나무들을 활용한다. 훈련자료의 모집단의 분포를 모르기 때문에 실제 문제에서는 평균예측모형을 구할 수 없다. 배깅은 이러한 문제를 해결하기 위해 훈련자료를 모집단으로 생각하고 평균예측모형을 구하여 분산을 줄이고 예측력을 향상시킬 수 있다. 4.2.3.1.2 R을 이용한 Bagging 분석 bagging(formula, data, mfinal, control=, ...) 인자 설명 formula 수식 data 분석하고자하는 데이터 mfinal 반복수 또는 사용할 트리의 수 (default=100) control 의사결정나무를 만들 때 사용할 option을 설정 Q. 앞서 분할한 credit 데이터의 train 데이터로 Bagging 모델을 만들어 보자. install.packages(setdiff(&quot;adabag&quot;, rownames(installed.packages()))) library(adabag) ## Loading required package: foreach ## Loading required package: doParallel ## Loading required package: iterators ## Loading required package: parallel bag&lt;-bagging(credit.rating~., data=train, mfinal=15) names(bag) ## [1] &quot;formula&quot; &quot;trees&quot; &quot;votes&quot; &quot;prob&quot; &quot;class&quot; ## [6] &quot;samples&quot; &quot;importance&quot; &quot;terms&quot; &quot;call&quot; names 함수를 통해 bagging 함수로 생성된 결과들에 어떤 것들이 있는지 확인이 가능하다. 주로 사용하는 인자들에 대한 설명은 아래와 같다. trees: bagging을 통해 생성된 의사결정나무들을 확인할 수 있다. votes: 각 의사결정나무들이 1행 데이터에 대해 1 또는 2열의 분류를 가진다는 것에 대한 투표를 진행한 것이다. prob: 각 행에 대해 1 또는 2열의 특징으로 분류되는 확률을 나타내는 것이다. class: bagging 기법을 활용해 각 행의 분류를 예측한 것이다. samples: 각 의사결정나무에 사용된 부트스트랩 데이터의 레코드 번호를 나타낸다. importance: 변수의 상대적인 중요도를 나타내며, 지니지수의 gain을 고려한 측도이다. bag$importance ## account.balance age ## 32.1612585 9.6931394 ## apartment.type bank.credits ## 0.5240433 0.7862148 ## credit.amount credit.duration.months ## 9.5477783 9.9671607 ## credit.purpose current.assets ## 4.7106811 4.0205891 ## dependents employment.duration ## 0.0000000 2.6329518 ## foreign.worker guarantor ## 0.3666136 3.3424176 ## installment.rate marital.status ## 1.8668982 1.7141508 ## occupation other.credits ## 1.5483235 1.0551176 ## previous.credit.payment.status residence.duration ## 6.2876622 3.1676583 ## savings telephone ## 5.1833824 1.4239586 importance 인자에서 변수의 상대적 중요도를 봤을 때, account.balance, credit.duration.months, age 순서로 변수 중요도가 크다는 것을 파악할 수 있다. library(caret) pred.bg&lt;-predict(bag, test, type=&quot;class&quot;) confusionMatrix(data=as.factor(pred.bg$class), reference=test$credit.rating, positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 50 31 ## 1 45 174 ## ## Accuracy : 0.7467 ## 95% CI : (0.6935, 0.7949) ## No Information Rate : 0.6833 ## P-Value [Acc &gt; NIR] : 0.009816 ## ## Kappa : 0.3905 ## ## Mcnemar&#39;s Test P-Value : 0.135908 ## ## Sensitivity : 0.8488 ## Specificity : 0.5263 ## Pos Pred Value : 0.7945 ## Neg Pred Value : 0.6173 ## Prevalence : 0.6833 ## Detection Rate : 0.5800 ## Detection Prevalence : 0.7300 ## Balanced Accuracy : 0.6875 ## ## &#39;Positive&#39; Class : 1 ## 로지스틱 회귀모형, 의사결정나무 모형과 동일한 형태로 정분류율을 확인할 수 있으며, 분석 결과에서 예측한 값의 class가 numeric형이므로 as.factor 함수를 이용하여 factor로 변형을 해야 한다. 정분류율은 0.7467이며, 민감도는 0.8488로 높게 나타났다. 또, 특이도는 0.5263이다. 정확도가 높다고 해서 무조건 좋은 모형은 아니며, 분석 분야에 따라 다양한 지표들을 활용하여 분석모형을 선택할 수 있다. library(ROCR) pred.bg.roc&lt;-prediction(as.numeric(pred.bg$class), as.numeric(test[,1])) plot(performance(pred.bg.roc, &quot;tpr&quot;, &quot;fpr&quot;)) abline(a=0, b=1, lty=2, col=&quot;black&quot;) performance(pred.bg.roc, &quot;auc&quot;)@y.values ## [[1]] ## [1] 0.6875481 prediction 함수와 performance 함수로 값을 구하여 plot 함수로 ROC 커브를 그렸으며, AUC값은 @y.values값으로 확인한 결과 0.6875로 나타났다. 4.2.3.2 부스팅 (Boosting) 4.2.3.2.1 개념 예측력이 약한 모형들을 결합하여 강한 예측모형을 만드는 방법으로 Adaboost는 이진분류 문제에서 랜덤 분류기보다 조금 더 좋은 분류기 n개에 각각 가중치를 설정하고 n개의 분류기를 결합하여 최종 분류기를 만드는 방법을 제안하였다. 훈련오차를 빨리, 쉽게 줄일 수 있고 배깅에 비해 많은 경우 예측오차가 향상되어 Adaboost의 성능이 배깅보다 뛰어난 경우가 많다. 4.2.3.2.2 R을 이용한 Boosting 분석 [함수사용법] boosting(formula, data, boos=TRUE/FALSE, control=, ...) Q. 앞서 분할한 credit 데이터의 train 데이터로 Boosting 모델을 만들어 보자. library(adabag) boost&lt;-boosting(credit.rating ~ ., data=train, boos=TRUE, mfinal=80) names(boost) ## [1] &quot;formula&quot; &quot;trees&quot; &quot;weights&quot; &quot;votes&quot; &quot;prob&quot; ## [6] &quot;class&quot; &quot;importance&quot; &quot;terms&quot; &quot;call&quot; names 함수를 통해 boosting 함수로 생성된 결과들에 어떤 것들이 있는지 확인이 가능하다. 주로 사용하는 인자들에 대한 설명은 아래와 같다. trees: boosting을 통해 생성된 의사결정나무들을 확인할 수 있다. (80개) weitgts: 각 의사결정나무에 부여된 가중치값을 확인할 수 있다. votes: 각 의사결정나무들이 1행 데이터에 대해 1 또는 2열의 분류를 가진다는 것에 대한 투표를 진행한 것이다. prob: 각 행에 대해 1 또는 2열의 특징으로 분류되는 확률을 나타내는 것이다. class: boosting 기법을 활용해 각 행의 분류를 예측한 것이다. importance: 변수의 상대적인 중요도를 나타내며, 지니지수의 gain을 고려한 측도이다. boost$importance ## account.balance age ## 5.0653036 14.7869602 ## apartment.type bank.credits ## 1.8398062 1.5915217 ## credit.amount credit.duration.months ## 23.0980549 10.0939561 ## credit.purpose current.assets ## 4.3067863 4.6719210 ## dependents employment.duration ## 1.3666935 5.2701945 ## foreign.worker guarantor ## 0.3727978 1.4688110 ## installment.rate marital.status ## 3.6999734 2.2674559 ## occupation other.credits ## 4.7645403 1.6476463 ## previous.credit.payment.status residence.duration ## 3.0414926 4.7966697 ## savings telephone ## 4.2742639 1.5751511 importance 인자에서 변수의 상대적 중요도를 봤을 때, credit.amount, age, credit.duration.months 순서로 변수 중요도가 크다는 것을 파악할 수 있다. library(caret) pred.boos&lt;-predict(boost, test, type=&quot;class&quot;) confusionMatrix(data=as.factor(pred.boos$class), reference=test$credit.rating, positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 48 39 ## 1 47 166 ## ## Accuracy : 0.7133 ## 95% CI : (0.6586, 0.7638) ## No Information Rate : 0.6833 ## P-Value [Acc &gt; NIR] : 0.1455 ## ## Kappa : 0.3223 ## ## Mcnemar&#39;s Test P-Value : 0.4504 ## ## Sensitivity : 0.8098 ## Specificity : 0.5053 ## Pos Pred Value : 0.7793 ## Neg Pred Value : 0.5517 ## Prevalence : 0.6833 ## Detection Rate : 0.5533 ## Detection Prevalence : 0.7100 ## Balanced Accuracy : 0.6575 ## ## &#39;Positive&#39; Class : 1 ## 로지스틱 회귀모형, 의사결정나무 모형과 동일한 형태로 정분류율을 확인할 수 있으며, 분석 결과에서 예측한 값의 class가 numeric형이므로 as.factor 함수를 이용하여 factor로 변형을 해야 한다. 정분류율은 0.7133이며, 민감도는 0.8098로 높게 나타났다. 또, 특이도는 0.5053이다. 정확도가 높다고 해서 무조건 좋은 모형은 아니며, 분석분야에 따라 다양한 지표들을 활용하여 분석모형을 선택할 수 있다. library(ROCR) pred.boos.roc&lt;-prediction(as.numeric(pred.boos$class), as.numeric(test[,1])) plot(performance(pred.boos.roc, &quot;tpr&quot;, &quot;fpr&quot;)) abline(a=0, b=1, lty=2, col=&quot;black&quot;) performance(pred.boos.roc,&quot;auc&quot;)@y.values ## [[1]] ## [1] 0.6575096 prediction 함수와 performance 함수로 값을 구하여 plot 함수로 ROC 커브를 그렸으며, AUC값은 @y.values값으로 확인한 결과 0.6575로 나타났다. 4.2.3.3 랜덤포레스트 (Random Forest) 4.2.3.3.1 개념 의사결정나무의 특징인 분산이 크다는 점을 고려하여 배깅과 부스팅보다 더 많은 무작위성을 주어 약한 학습기들을 생성한 후 이를 선형 결합하여 최종 학습기를 만드는 방법이다. R프로그램에서는 randomForest 패키지로 구현이 가능하다. randomForest 함수를 사용하고 random input에 따른 forest of tree를 생성하여 이를 이용한 분류를 한다. 수천개의 변수를 통해 변수 제거없이 실행되므로 정확도 측면에서 좋은 성과를 보인다. 이론적 설명이나 최종 결과에 대한 해석이 어렵다는 단점이 있지만 예측력이 매우 높은 것으로 알려져 있다. 특히 입력변수가 많은 경우, 배깅/부스팅과 비슷하거나 좋은 예측력을 보인다. 4.2.3.3.2 R을 이용한 RandomForest 분석 R에서 RandomForest 분석을 수행할 수 있는 함수는 randomForest 패키지의 randomForest 함수이며, 이를 이용하여 분류분석을 실시한다. [함수사용법] randomForest(formula, data, ntree, mtry, ...) 인자 설명 formula 수식(종속변수 ~ 독립변수) data 분석하고자 하는 데이터 ntree 사용할 트리의 수, 너무 작은 숫자를 입력하면 예측 불가 mtry 각 분할에서 랜덤으로 뽑힌 변수의 개수보통 classification은 sqrt(변수 개수), regression은 (변수 개수/3) Q. 앞서 분할한 credit 데이터의 train 데이터로 randomforest 모델을 만들어 보자. library(randomForest) ## randomForest 4.6-14 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attaching package: &#39;randomForest&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## margin ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine (rf.model&lt;-randomForest(credit.rating ~ ., data=train, ntree=50, # 나무 50개 사용 mtry=sqrt(20), # 사용할 변수의 개수 (classification이므로 sqrt(20)개) importance=TRUE) # 변수중요도를 결과를 확인 ) ## ## Call: ## randomForest(formula = credit.rating ~ ., data = train, ntree = 50, mtry = sqrt(20), importance = TRUE) ## Type of random forest: classification ## Number of trees: 50 ## No. of variables tried at each split: 4 ## ## OOB estimate of error rate: 25.43% ## Confusion matrix: ## 0 1 class.error ## 0 88 117 0.5707317 ## 1 61 434 0.1232323 랜덤포레스트 분석 결과에서 “OOB estimate of error rate”의 값은 에러 추정치로서 값이 낮을수록 분류모델의 성능이 좋다고 판단할 수 있다. Confusion matrix의 결과에서 class.error값으로 분류 에러를 통해 모델 성능을 확인할 수 있다. names(rf.model) ## [1] &quot;call&quot; &quot;type&quot; &quot;predicted&quot; &quot;err.rate&quot; ## [5] &quot;confusion&quot; &quot;votes&quot; &quot;oob.times&quot; &quot;classes&quot; ## [9] &quot;importance&quot; &quot;importanceSD&quot; &quot;localImportance&quot; &quot;proximity&quot; ## [13] &quot;ntree&quot; &quot;mtry&quot; &quot;forest&quot; &quot;y&quot; ## [17] &quot;test&quot; &quot;inbag&quot; &quot;terms&quot; names 함수를 통해 randomForest 함수로 생성된 결과들에 어떤 것들이 있는지 확인이 가능하다. 주로 사용하는 인자들에 대한 설명은 아래와 같다. predicted: Out-of-bag samples에 기초한 예측값을 확인할 수 있다. err.rate: 입력데이터 각각에 대한 예측 오류율을 확인할 수 있다. importance: 변수 중요도를 나타내며 Gini값을 기준으로 한다. MeanDecreaseAccuracy와 MeanDecreaseGini 모두 값이 클수록 중요도가 높다고 해석할 수 있다. varImpPlot(rf.model) varImpPlot 함수로 importance 인자 결과를 시각화할 수 있다. 변수의 상대적 중요도를 Mean DecreaseGini를 기준으로 봤을 때, credit.amout, age, account.balance 순서로 변수 중요도가 크다는 것을 파악할 수 있다. library(caret) pred.rf&lt;-predict(rf.model, test[,-1], type=&quot;class&quot;) confusionMatrix(data=pred.rf, reference=test[,1], positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 44 21 ## 1 51 184 ## ## Accuracy : 0.76 ## 95% CI : (0.7076, 0.8072) ## No Information Rate : 0.6833 ## P-Value [Acc &gt; NIR] : 0.0021620 ## ## Kappa : 0.3941 ## ## Mcnemar&#39;s Test P-Value : 0.0006316 ## ## Sensitivity : 0.8976 ## Specificity : 0.4632 ## Pos Pred Value : 0.7830 ## Neg Pred Value : 0.6769 ## Prevalence : 0.6833 ## Detection Rate : 0.6133 ## Detection Prevalence : 0.7833 ## Balanced Accuracy : 0.6804 ## ## &#39;Positive&#39; Class : 1 ## 정분류율은 0.76이며, 민감도는 0.8976으로 높게 나타났다. 또 특이도는 0.4632이다. 정확도가 높다고 해서 무조건 좋은 모형은 아니며, 분석분야에 따라 다양한 지표들을 활용하여 분석 모형을 선택할 수 있다. library(ROCR) pred.rf.roc&lt;-prediction(as.numeric(pred.rf), as.numeric(test[,1])) plot(performance(pred.rf.roc,&quot;tpr&quot;,&quot;fpr&quot;)) abline(a=0,b=1,lty=2,col=&quot;black&quot;) performance(pred.rf.roc, &quot;auc&quot;)@y.values[[1]] ## [1] 0.6803594 prediction 함수와 performance 함수로 값을 구하여 plot 함수로 ROC 커브를 그렸으며, AUC값은 @y.values값으로 확인한 결과 0.6804로 나타났다. Q. 앞서 분리한 iris 데이터의 Species를 분류하는 랜덤포레스트분석을 실시하고 오분류표를 만들어 보자. library(randomForest) (rf.model2&lt;-randomForest(Species ~ ., data=train.iris, ntree=50, mtry=sqrt(4), importance=TRUE)) ## ## Call: ## randomForest(formula = Species ~ ., data = train.iris, ntree = 50, mtry = sqrt(4), importance = TRUE) ## Type of random forest: classification ## Number of trees: 50 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 7.62% ## Confusion matrix: ## setosa versicolor virginica class.error ## setosa 35 0 0 0.0000000 ## versicolor 0 36 4 0.1000000 ## virginica 0 4 26 0.1333333 pred.rf2&lt;-predict(rf.model2, test.iris[,-5], type=&quot;class&quot;) confusionMatrix(data=pred.rf2, reference=test.iris[,5], positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 10 1 ## virginica 0 0 19 ## ## Overall Statistics ## ## Accuracy : 0.9778 ## 95% CI : (0.8823, 0.9994) ## No Information Rate : 0.4444 ## P-Value [Acc &gt; NIR] : 8.12e-15 ## ## Kappa : 0.9656 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 1.0000 0.9500 ## Specificity 1.0000 0.9714 1.0000 ## Pos Pred Value 1.0000 0.9091 1.0000 ## Neg Pred Value 1.0000 1.0000 0.9615 ## Prevalence 0.3333 0.2222 0.4444 ## Detection Rate 0.3333 0.2222 0.4222 ## Detection Prevalence 0.3333 0.2444 0.4222 ## Balanced Accuracy 1.0000 0.9857 0.9750 4.2.4 SVM (Support Vector Machine) 서포트 벡터 머신은 기계학습 분야 중 하나로 패턴인식, 자료 분석 등을 위한 지도학습 모델이며 주로 회귀와 분류 문제 해결에 사용된다. 서포트 벡터 머신 알고리즘은 주어진 데이터 집합을 바탕으로 하여 새로운 데이터가 어떤 범주에 속할 것인지를 판단하는 비확률적 이진 선형 분류 모델을 생성한다. 4.2.4.1 작동 원리 데이터의 각 그룹을 구분하는 분류자를 결정 초평면, 각 그룹에 속한 데이터들 중에서도 초평면에 가장 가까이에 붙어 있는 최정방 데이터들을 서포트 벡터, 서포트 벡터와 초평면 사이의 수직거리를 마진이라고 한다. SVM은 고차원 혹은 무한 차원의 공간에서 마진을 최대화하는 초평면 (MMH, Maximum Margin Hyperplane: 최대마진 초평면) 을 찾아 분류와 회귀를 수행한다. SVM 모형은 선형 분류뿐만 아니라 비선형 분류에서도 사용되는데, 비선형 분류에서는 입력자료를 다차원 공간상으로 매핑할 때 커널 트릭을 사용하기도 한다. 4.2.4.2 R을 이용한 SVM 분석 [함수사용법] svm(formula, data, kernel, gamma, cost, ...) 인자 설명 formula 수식(종속변수 ~ 독립변수) data 분석하고자 하는 데이터 kernel 훈련과 예측에 사용되는 커널“radial”,“linear”,“polynomial”,“sigmoid”가 있음.실제 문제에서 커널의 선택이 결과의 정확도에 큰 영향을 주지 않음. gamma 초평면의 기울기, default=1/(데이터차원) cost 과적합을 막는 정도, default=1 tune.svm(formula, data, kernel, gamma, cost, ...) 인자 설명 formula 수식(종속변수 ~ 독립변수) data 분석하고자 하는 데이터 gamma 초평면의 기울기 cost 과적합을 막는 정도 Q. 앞서 분할한 credit 데이터의 train 데이터를 이용하여 tune.svm 함수로 최적의 파라미터를 찾고 SVM 모델을 만들어 보자. install.packages(setdiff(&quot;e1071&quot;, rownames(installed.packages()))) library(e1071) tune.svm(credit.rating ~ ., data=credit, gamma = 10^(-6:-1), cost = 10^(1:2)) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## gamma cost ## 0.01 10 ## ## - best performance: 0.235 tune.svm 함수에서 gamma와 cost의 주어진 범위 내에서 최적값을 찾아준다. 여기서는 gamma 6개, cost 2개, 즉 6 * 12개의 조합에서 모수조율이 이루어진다. 분석결과에서 best parameters를 통해 gamma는 0.01, cost는 10이 최적의 파라미터임을 확인할 수 있다. svm.model&lt;-svm(credit.rating~., data=train, kernel=&quot;radial&quot;, gamma=0.01, cost=10) summary(svm.model) ## ## Call: ## svm(formula = credit.rating ~ ., data = train, kernel = &quot;radial&quot;, ## gamma = 0.01, cost = 10) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: radial ## cost: 10 ## ## Number of Support Vectors: 389 ## ## ( 212 177 ) ## ## ## Number of Classes: 2 ## ## Levels: ## 0 1 svm 함수에서 gamma와 cost를 설정하고, kernel을 “radial”으로 지정한다. kernel은 radial (가우시안 RBF)이 default로 되어 있다. summary 함수로 svm 모델의 cost값과 Support Vectors의 수(train 데이터 수)를 확인할 수 있다. # 예측을 통한 정분류를 확인 install.packages(setdiff(&quot;caret&quot;, rownames(installed.packages()))) library(caret) pred.svm&lt;-predict(svm.model, test, type=&quot;class&quot;) confusionMatrix(data=pred.svm, reference=test[,1], positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 52 31 ## 1 43 174 ## ## Accuracy : 0.7533 ## 95% CI : (0.7005, 0.8011) ## No Information Rate : 0.6833 ## P-Value [Acc &gt; NIR] : 0.004761 ## ## Kappa : 0.41 ## ## Mcnemar&#39;s Test P-Value : 0.200994 ## ## Sensitivity : 0.8488 ## Specificity : 0.5474 ## Pos Pred Value : 0.8018 ## Neg Pred Value : 0.6265 ## Prevalence : 0.6833 ## Detection Rate : 0.5800 ## Detection Prevalence : 0.7233 ## Balanced Accuracy : 0.6981 ## ## &#39;Positive&#39; Class : 1 ## 정분류율을 0.7533이며, 민감도는 0.8488로 높게 나타났다. 또, 특이도는 0.5474이다. 정확도가 높다고 해서 무조건 좋은 모형은 아니며, 분석 분야에 따라 다양한 지표들을 활용하여 분석모형을 선택할 수 있다. install.packages(setdiff(&quot;ROCR&quot;, rownames(installed.packages()))) library(ROCR) pred.svm.roc&lt;-prediction(as.numeric(pred.svm), as.numeric(test[,1])) plot(performance(pred.svm.roc, &quot;tpr&quot;, &quot;fpr&quot;)) abline(a=0, b=1, lty=2, col=&quot;black&quot;) performance(pred.svm.roc, &quot;auc&quot;)@y.values ## [[1]] ## [1] 0.6980745 prediction 함수와 performance 함수로 값을 구하여 plot함수로 ROC 커브를 그렸으며, AUC값은 @y.values 값으로 확인한 결과 0.6981로 나타났다. Q. 앞서 분리한 iris 데이터의 Species를 분류하는 SVM 분석을 실시하고 오분류표를 만들어 보자. install.packages(setdiff(&quot;e1071&quot;, rownames(installed.packages()))) library(e1071) tune.svm(Species ~ ., data=iris, gamma=2^(-1:1), cost=2^(2:4)) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## gamma cost ## 0.5 4 ## ## - best performance: 0.03333333 svm.model2&lt;-svm(Species~., data=train.iris, kernel=&quot;radial&quot;, gamma=0.5, cost=16) pred.svm2&lt;-predict(svm.model2, test.iris, type=&quot;class&quot;) confusionMatrix(data=pred.svm2, reference=test.iris[,5], positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 10 1 ## virginica 0 0 19 ## ## Overall Statistics ## ## Accuracy : 0.9778 ## 95% CI : (0.8823, 0.9994) ## No Information Rate : 0.4444 ## P-Value [Acc &gt; NIR] : 8.12e-15 ## ## Kappa : 0.9656 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 1.0000 0.9500 ## Specificity 1.0000 0.9714 1.0000 ## Pos Pred Value 1.0000 0.9091 1.0000 ## Neg Pred Value 1.0000 1.0000 0.9615 ## Prevalence 0.3333 0.2222 0.4444 ## Detection Rate 0.3333 0.2222 0.4222 ## Detection Prevalence 0.3333 0.2444 0.4222 ## Balanced Accuracy 1.0000 0.9857 0.9750 4.2.5 나이브 베이즈 분류 나이브 베이즈 분류는 데이터에서 변수들에 대한 조건부 독립을 가정하는 알고리즘으로 클래스에 대한 사전 정보와 데이터로부터 추출된 정보를 결합하고, 베이즈 정리를 이용하여 특정 데이터가 어떤 클래스에 속하는지를 분류하는 알고리즘이다. 텍스트 분류에서 문서를 여러 범주중 하나로 판단하는 문제에 대한 솔루션으로 사용될 수 있다. 4.2.5.1 Bayes theorem 나이브 베이즈 알고리즘의 기본이 되는 개념으로, 두 확률 변수의 사전 확률과 사후 확률 사이의 관계를 나타내는 정리이다. 사건 A와 B가 있을 때, 사건 B가 일어난 것을 전제로 한 사건 A의 조건부 확률을 구하고자 한다. 하지만 현재 가지고 있는 정보는 사건 A가 일어난 것을 전제로 한 사건 B의 조건부 확률, A의 확률, B의 확률뿐이다. 이때, 원래 구하고자 했던 ’사건 B가 일어난 것을 전제로 한 사건 A의 조건부 확률’을 다음과 같이 구할 수 있다는 것이 베이즈 정리이다. \\(P(A|B) = \\frac{P(B \\cap A)}{P(B)} = \\frac{P(A)P(B|A)}{P(B)}=\\frac{P(A)P(B|A)}{P(A)P(B|A)+P(A^{C}P(B|A^{C}))}\\) \\(P(A|B)\\) : 사건 B가 발생했을 때 사건 A가 발생할 확률 -&gt; 사후확률 (posterior) \\(P(B|A)\\) : 사건 A가 발생했을 때 사건 B가 발생할 확률 -&gt; 우도 (likelihood) \\(P(A \\cap B)\\) : 사건 A와 B가 동시에 발생할 확률 \\(P(A)\\) : 사건 A가 발생할 확률 -&gt; 사전확률 (prior) \\(P(B)\\) : 사건 B가 발생할 화률 -&gt; 관찰값 (evidence) 위 식을 다음과 같은 식으로도 표현이 가능하다. \\(posterior = \\frac{prior\\times likelihood}{evidence}\\) 4.2.5.2 나이브 베이즈 분류 나이브 베이즈 분류는 하나의 속성값을 기준으로 다른 속성이 독립적이라 전제했을 때 해당 속성 값이 클래스 분류에 미치는 영향을 측정한다. 속성값에 대해 다른 속성이 독립적이라는 가정은클래스 조건 독립성이라 한다. 4.2.5.3 R을 이용한 나이브 베이즈 분류 분석 함수사용법 naiveBayes(formula, data, laplace=0, ...) Q. 앞서 분할한 credit 데이터의 train 데이터로 나이브 베이즈 분류 모델을 만들어 보자. library(e1071) nb.model&lt;-naiveBayes(credit.rating~., data=train, laplace=0) nb.model ## ## Naive Bayes Classifier for Discrete Predictors ## ## Call: ## naiveBayes.default(x = X, y = Y, laplace = laplace) ## ## A-priori probabilities: ## Y ## 0 1 ## 0.2928571 0.7071429 ## ## Conditional probabilities: ## account.balance ## Y [,1] [,2] ## 0 1.746341 0.7436323 ## 1 2.381818 0.7983236 ## ## credit.duration.months ## Y [,1] [,2] ## 0 24.37073 13.31920 ## 1 18.99192 11.00404 ## ## previous.credit.payment.status ## Y [,1] [,2] ## 0 2.126829 0.6211250 ## 1 2.375758 0.5728491 ## ## credit.purpose ## Y [,1] [,2] ## 0 3.102439 0.8768504 ## 1 2.868687 0.9841504 ## ## credit.amount ## Y [,1] [,2] ## 0 3765.078 3338.421 ## 1 2974.749 2453.003 ## ## savings ## Y [,1] [,2] ## 0 1.497561 0.9631667 ## 1 2.008081 1.2288434 ## ## employment.duration ## Y [,1] [,2] ## 0 2.263415 1.088657 ## 1 2.529293 1.079178 ## ## installment.rate ## Y [,1] [,2] ## 0 3.112195 1.067385 ## 1 2.937374 1.130242 ## ## marital.status ## Y [,1] [,2] ## 0 2.224390 1.097408 ## 1 2.385859 1.046781 ## ## guarantor ## Y [,1] [,2] ## 0 1.082927 0.2764467 ## 1 1.107071 0.3095159 ## ## residence.duration ## Y [,1] [,2] ## 0 2.814634 1.077752 ## 1 2.852525 1.105960 ## ## current.assets ## Y [,1] [,2] ## 0 2.580488 1.043002 ## 1 2.258586 1.040575 ## ## age ## Y [,1] [,2] ## 0 33.16098 11.01730 ## 1 36.25051 11.45939 ## ## other.credits ## Y [,1] [,2] ## 0 1.760976 0.4275317 ## 1 1.848485 0.3589130 ## ## apartment.type ## Y [,1] [,2] ## 0 1.863415 0.5948126 ## 1 1.929293 0.4856759 ## ## bank.credits ## Y [,1] [,2] ## 0 1.326829 0.4702025 ## 1 1.361616 0.4809545 ## ## occupation ## Y [,1] [,2] ## 0 2.931707 0.6456639 ## 1 2.872727 0.6378446 ## ## dependents ## Y [,1] [,2] ## 0 1.141463 0.3493521 ## 1 1.155556 0.3628001 ## ## telephone ## Y [,1] [,2] ## 0 1.346341 0.4769683 ## 1 1.414141 0.4930714 ## ## foreign.worker ## Y [,1] [,2] ## 0 1.009756 0.09853057 ## 1 1.046465 0.21070209 분석 결과에서 A-priori probabilities는 사전확률을 나타내고 있으며, Conditional probabilities로 각 변수에 대해 조건부 확률을 표로 제공하고 있다. 수치형 변수의 경우 평균, 표준편차를 제공한다. # 예측을 통한 정분류율 확인 library(caret) pred.nb&lt;-predict(nb.model, test[, -1], type=&quot;class&quot;) confusionMatrix(data=pred.nb, reference=test[,1], positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 65 53 ## 1 30 152 ## ## Accuracy : 0.7233 ## 95% CI : (0.669, 0.7732) ## No Information Rate : 0.6833 ## P-Value [Acc &gt; NIR] : 0.07551 ## ## Kappa : 0.3997 ## ## Mcnemar&#39;s Test P-Value : 0.01574 ## ## Sensitivity : 0.7415 ## Specificity : 0.6842 ## Pos Pred Value : 0.8352 ## Neg Pred Value : 0.5508 ## Prevalence : 0.6833 ## Detection Rate : 0.5067 ## Detection Prevalence : 0.6067 ## Balanced Accuracy : 0.7128 ## ## &#39;Positive&#39; Class : 1 ## 정분류율(Accuracy)은 0.7233이며, 민감도(Sensitivity)는 0.7415로 높게 나타났다. 또, 특이도 (Specificity)는 0.6842이다. 정확도가 높다고 해서 무조건 좋은 모형은 아니며, 분석 분야에 따라 다양한 지표들을 활용하여 분석 모형을 선택할 수 있다. install.packages(setdiff(&quot;ROCR&quot;, rownames(installed.packages()))) library(ROCR) pred.nb.roc&lt;-prediction(as.numeric(pred.nb), as.numeric(test[,1])) plot(performance(pred.nb.roc, &quot;tpr&quot;, &quot;fpr&quot;)) abline(a=0, b=1, lty=2, col=&quot;black&quot;) performance(pred.nb.roc, &quot;auc&quot;)@y.values ## [[1]] ## [1] 0.712837 prediction 함수와 performance 함수로 값을 구하여 plot 함수로 ROC 커브를 그렸으며, AUC값은 @y.values값으로 확인한 결과 0.7128로 나타났다. 4.2.6 K-NN (K-Nearest Neighbor) K-NN은 어떤 범주로 나누어져 있는 데이터셋이 있을 때, 새로운 데이터가 추가된다면 이를 어떤 범주로 분류할 것인지를 결정할 때 사용할 수 있는 분류 알고리즘으로 지도학습 (Supervised Learning)의 한 종류이다. 4.2.6.1 K-NN 알고리즘의 원리 K-NN 알고리즘에서는 새로운 데이터의 클래스를 해당 데이터와 가장 가까운 K개 데이터들의 클래스(범주)로 결정한다. K-NN 알고리즘에서는 최근접 이웃 간의 거리를 계산할 때 유클리디안 거리, 맨하탄 거리, 민코우스키 거리 등을 사용할 수 있으며, 대표적으로 유클리디안 거리를 사용한다. 4.2.6.2 K의 선택 K의 선택은 학습의 난이도와 데이터의 개수에 따라 결정될 수 있으며, 일반적으로는 훈련 데이터 개수의 제곱근으로 설정한다. 그리고 k를 짝수로 했을 때, 인접객체의 범주가 동률일 경우가 나오므로 반드시 홀수의 값으로 k를 선택하는 것이 중요하다. K를 너무 크게 설정할 경우 주변에 있는 데이터와 근접성이 떨어져 클러스터링이 잘 이루어지지 않고, 너무 작게 설정할 경우 이상치 혹은 잡음 데이터와 이웃이 될 가능성이 있으므로 적절한 k를 선택하는 것이 중요하다. 4.2.6.3 R을 이용한 K-NN 분석 knn 분석 이전에 훈련, 데스트 데이터의 종속변수를 제외한 뒤에 분석을 실시한다. 그리고 거리를 이용한 분석이므로 데이터의 형태가 범주형 변수가 아닌 수치형으로 변환되어야 한다. 함수사용법 knn(train, test, cl, k, ...) Q. 앞서 분할한 credit 데이터의 train 데이터로 K-NN 모델을 만들어 보자. library(class) train.data&lt;-train[,-1] head(train.data) ## account.balance credit.duration.months previous.credit.payment.status ## 415 3 12 3 ## 463 3 15 2 ## 179 3 18 2 ## 526 3 36 3 ## 195 3 12 2 ## 938 2 18 3 ## credit.purpose credit.amount savings employment.duration installment.rate ## 415 3 522 3 4 4 ## 463 1 3812 2 1 1 ## 179 4 1950 1 3 4 ## 526 3 9566 1 2 2 ## 195 3 1262 1 2 3 ## 938 4 884 1 4 4 ## marital.status guarantor residence.duration current.assets age ## 415 3 1 4 2 42 ## 463 1 1 4 3 23 ## 179 3 1 1 3 34 ## 526 1 1 2 3 31 ## 195 3 1 2 3 25 ## 938 3 1 4 3 36 ## other.credits apartment.type bank.credits occupation dependents telephone ## 415 2 2 2 3 2 2 ## 463 2 2 1 3 1 2 ## 179 1 2 2 3 1 2 ## 526 1 2 2 3 1 1 ## 195 2 2 1 3 1 1 ## 938 2 2 1 3 2 2 ## foreign.worker ## 415 1 ## 463 1 ## 179 1 ## 526 1 ## 195 1 ## 938 1 test.data&lt;-test[,-1] head(test.data) ## account.balance credit.duration.months previous.credit.payment.status ## 1 1 18 3 ## 3 2 12 2 ## 4 1 12 3 ## 7 1 8 3 ## 9 3 18 3 ## 12 1 30 3 ## credit.purpose credit.amount savings employment.duration installment.rate ## 1 2 1049 1 1 4 ## 3 4 841 2 3 2 ## 4 4 2122 1 2 3 ## 7 4 3398 1 3 1 ## 9 3 1098 1 1 4 ## 12 1 6187 2 3 1 ## marital.status guarantor residence.duration current.assets age other.credits ## 1 1 1 4 2 21 2 ## 3 1 1 4 1 23 2 ## 4 3 1 2 1 39 2 ## 7 3 1 4 1 39 2 ## 9 1 1 4 3 65 2 ## 12 4 1 4 3 24 2 ## apartment.type bank.credits occupation dependents telephone foreign.worker ## 1 1 1 3 1 1 1 ## 3 1 1 2 1 1 1 ## 4 1 2 2 2 1 2 ## 7 2 2 2 1 1 2 ## 9 2 2 1 1 1 1 ## 12 1 2 3 1 1 1 class&lt;-train[,1] head(class) ## [1] 1 1 1 1 1 0 ## Levels: 0 1 knn.3&lt;-knn(train.data, test.data, class, k=3) knn.7&lt;-knn(train.data, test.data, class, k=7) knn.10&lt;-knn(train.data, test.data, class, k=10) # 각각의 k에 대해 분류 table 작성과 분류 정확도 확인 (t.1&lt;-table(knn.3, test$credit.rating)) ## ## knn.3 0 1 ## 0 27 51 ## 1 68 154 (t.1[1,1]+t.1[2,2])/sum(t.1) ## [1] 0.6033333 (t.2&lt;-table(knn.7, test$credit.rating)) ## ## knn.7 0 1 ## 0 18 26 ## 1 77 179 (t.2[1,1]+t.2[2,2])/sum(t.2) ## [1] 0.6566667 (t.3&lt;-table(knn.10, test$credit.rating)) ## ## knn.10 0 1 ## 0 12 15 ## 1 83 190 (t.3[1,1]+t.3[2,2])/sum(t.3) ## [1] 0.6733333 분석 이전에 종속변수(credit.rating)을 제외한 데이터를 train.data와 test.data에 저장하고 class에 훈련 데이터의 종속변수를 저장한다 .그리고 k가 3, 7, 10일 때 각각 모델을 knn 함수를 사용하여 만든다. 분석 결과를 확인하기 위해서 각각의 k에 대해 분류 table과 정분류율을 계산하여 가장 정분류율이 높은 모델을 찾는다. 위의 결과에서 k를 10으로 했을 때 정분류율이 67%로 가장 높게 나타났다. result&lt;-numeric() k=3:22 for (i in k) { pred&lt;-knn(train.data, test.data, class, k=i-2) t&lt;-table(pred, test$credit.rating) result[i-2]&lt;-(t[1,1] + t[2,2])/sum(t) } result ## [1] 0.5966667 0.6166667 0.6033333 0.5800000 0.6333333 0.6633333 0.6566667 ## [8] 0.6666667 0.6766667 0.6633333 0.6700000 0.6666667 0.6800000 0.6766667 ## [15] 0.6733333 0.6800000 0.6900000 0.6800000 0.6900000 0.6833333 sort(result, decreasing=TRUE) ## [1] 0.6900000 0.6900000 0.6833333 0.6800000 0.6800000 0.6800000 0.6766667 ## [8] 0.6766667 0.6733333 0.6700000 0.6666667 0.6666667 0.6633333 0.6633333 ## [15] 0.6566667 0.6333333 0.6166667 0.6033333 0.5966667 0.5800000 which(result==max(result)) ## [1] 17 19 K-NN에서 최적의 K를 선정하는 것이 중요하다. 그렇기 때문에 최적의 k의 값을 선정해야 하며, 여기에서는 정분류율이 가장 높은 k가 최적의 k값이라고 선정하여 함수를 구현했다. 위의 결과에서 k가 17, 19일 때 분류 정확도가 가장 좋다고 나타나며, 정분류율은 69%이다. 4.2.7 인공신경망 모형 (Artificial Neural Network) 인공신경망 모형은 동물의 뇌신경계를 모방하여 분류 또는 예측하기 위해 만들어진 모형이다. 신경망에서는 입력은 인간의 뇌의 시냅스에 해당하며 개별 신호의 강도에 따라 가중되며, 활성 함수는 인공신경망의 출력을 계산한다. 인공신경망은 가중치를 반복적으로 조정하여 학습하며 뉴런들은 링크로 연결되어 있고, 각 링크에는 수치적인 가중치가 있다. 인공신경망은 신경망의 가중치를 초기화하고 훈련 데이터를 통해 가중치를 갱신하여 신경망의 구조를 선택하고, 활용할 학습 알고리즘을 결정한 후 신경망을 훈련 시킨다. 4.2.7.1 특징 4.2.7.1.1 구조 입력 링크에서 여러 신호를 받아서 새로운 활성화 수준을 계산하고, 출력 링크로 출력 신호를 보낸다. 입력신호는 미가공 데이터 또는 다른 뉴런으로부터의 출력이며, 출력신호는 문제의 최종해 (Solution)이 되거나 다른 뉴런의 입력이 될 수 있다. 4.2.7.1.2 뉴런의 계산 뉴런은 전이함수, 즉 활성화 함수 (activation function)를 사용하며, 활성화 함수를 이용해 출력을 결정하며 입력신호의 가중치 합을 계산하여 임계값과 비교한다. 가중치 합이 임계값보다 작으면 뉴런의 출력은 -1 혹은 0, 같거나 크면 +1 혹은 x의 값을 출력한다. 4.2.7.1.3 뉴런의 활성화 함수 시그모이드 함수 softmax 함수 Relu 함수 4.2.7.1.4 단일 뉴런의 학습 (단층 퍼셉트론) 퍼셉트론은 선형 결합기와 하드 리미터로 구성되며, 초평면은 n차원 공간을 두 개의 영역으로 나눈다. 초평면을 선형 분리함수로 정의한다. \\[\\sum_{i=1}^n x_iw_i-\\theta=0\\] 4.2.7.2 R을 이용한 인공신경망 분석 R에서 인공신경망 분석을 수행할 수 있는 패키지는 nnet와 neuralnet이 있으며 각각 nnet 함수와 neuralnet 함수를 제공한다. 4.2.7.2.1 nnet nnet 패키지는 전통적인 역전파를 가지고 feed-forward 신경망을 훈련하는 알고리즘을 제공한다. 그리고 신경망의 매개변수는 엔트로피와 SSE로 최적화되며, 출력결과를 softmax 함수를 사용해 확률 형태로 변환이 가능하고 과적합을 막기 위해 가중치 감소를 제공한다. nnet 함수는 size, maxit, decay 인자 외에도 가중치를 설정하는 weights, 초기 가중치 값을 설정하는 wts 등의 인자가 있다. nnet 함수로 생성된 모델의 변수 중요도를 파악하기 위해서는 NeuralNetTools 패키지의 garson 함수를 사용하여 확인한다. 함수사용법 nnet(formula, data, size, maxit, decay=5e-04 ...) 인자 설명 formula 수식(종속변수 ~ 독립변수) data 분석하고자 하는 데이터 size hidden node의 개수 maxit 학습 반복횟수, 반복 중 가장 좋은 모델을 선정함. decay 가중치 감소의 모수, 보통 5e-04 채택 함수사용법 garson(mod_in) 인자 설명 mod_in 생성된 인공신경망 모델 Q. 앞서 분할한 credit 데이터의 train 데이터로 nnet 함수를 활용한 인공신경망 모델을 만들어 보자. library(nnet) set.seed(1231) nn.model&lt;-nnet(credit.rating ~ ., data=train, size=2, maxit=200, decay=5e-04) ## # weights: 45 ## initial value 431.435559 ## iter 10 value 423.284097 ## iter 20 value 423.069450 ## iter 30 value 412.275788 ## iter 40 value 379.171871 ## iter 50 value 343.935787 ## iter 60 value 338.439394 ## iter 70 value 334.864491 ## iter 80 value 331.713287 ## iter 90 value 331.330445 ## iter 100 value 330.202215 ## iter 110 value 328.168809 ## iter 120 value 320.901862 ## iter 130 value 316.684662 ## iter 140 value 313.512983 ## iter 150 value 312.780486 ## iter 160 value 312.686319 ## iter 170 value 312.614992 ## iter 180 value 312.507704 ## iter 190 value 312.286285 ## iter 200 value 312.204529 ## final value 312.204529 ## stopped after 200 iterations summary(nn.model) ## a 20-2-1 network with 45 weights ## options were - entropy fitting decay=5e-04 ## b-&gt;h1 i1-&gt;h1 i2-&gt;h1 i3-&gt;h1 i4-&gt;h1 i5-&gt;h1 i6-&gt;h1 i7-&gt;h1 i8-&gt;h1 i9-&gt;h1 ## -11.03 35.33 -1.77 14.85 -6.62 0.00 26.54 -2.95 -1.53 4.76 ## i10-&gt;h1 i11-&gt;h1 i12-&gt;h1 i13-&gt;h1 i14-&gt;h1 i15-&gt;h1 i16-&gt;h1 i17-&gt;h1 i18-&gt;h1 i19-&gt;h1 ## -6.35 0.83 -23.34 1.32 3.49 8.71 -3.97 -24.77 -9.60 6.63 ## i20-&gt;h1 ## -5.47 ## b-&gt;h2 i1-&gt;h2 i2-&gt;h2 i3-&gt;h2 i4-&gt;h2 i5-&gt;h2 i6-&gt;h2 i7-&gt;h2 i8-&gt;h2 i9-&gt;h2 ## -3.17 10.72 -10.34 7.07 0.76 -0.02 1.81 7.05 -2.46 13.33 ## i10-&gt;h2 i11-&gt;h2 i12-&gt;h2 i13-&gt;h2 i14-&gt;h2 i15-&gt;h2 i16-&gt;h2 i17-&gt;h2 i18-&gt;h2 i19-&gt;h2 ## 1.54 -0.92 -7.43 1.62 1.62 1.12 -6.78 1.57 -11.20 11.62 ## i20-&gt;h2 ## -1.12 ## b-&gt;o h1-&gt;o h2-&gt;o ## -0.71 2.34 3.51 분석 결과를 확인하면 총 45개의 가중치가 주어졌음을 #weights: 45에서 확인할 수 있으며, iteration이 반복될수록 error이 줄어들고 있음을 확인할 수 있다. 그리고 200번째 반복 후에 학습을 멈췄으며, 최종 error값이 312.204529임을 final value를 보고 확인할 수 있다. summary 함수로 분석결과를 확인하면 “a 20-2-1 network with 45 weights”는 입력노드 20개, 은닉노드 2개, 출력노드 1개를 의미하고 가중치는 총 45개임을 알 수 있다. install.packages(setdiff(&quot;devtools&quot;, rownames(installed.packages()))) library(devtools) source_url(&#39;https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r&#39;) # X11() plot.nnet(nn.model) - summary의 결과에서 나타난 것처럼 20개의 입력노드, 2개의 은닉노드, 1개의 출력노드, 2개의 상수항을 확인할 수 있다. 그림에서 선의 굵기는 연결선의 가중치에 비례한다. install.packages(setdiff(&quot;NeuralNetTools&quot;, rownames(installed.packages()))) library(NeuralNetTools) # X11() garson(nn.model) 변수 중요도 그래프를 통해 모델의 분류에서 중요한 변수를 확인할 수 있다. 변수 중요도를 파악한 결과 account.balance, current.assets, dependents 순으로 변수 중요도가 크다는 것을 파악할 수 있다. install.packages(setdiff(&quot;caret&quot;, rownames(installed.packages()))) library(caret) pred.nn&lt;-predict(nn.model, test[,-1], type=&quot;class&quot;) confusionMatrix(data=as.factor(pred.nn), reference=test[,1], positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 51 41 ## 1 44 164 ## ## Accuracy : 0.7167 ## 95% CI : (0.662, 0.767) ## No Information Rate : 0.6833 ## P-Value [Acc &gt; NIR] : 0.1185 ## ## Kappa : 0.3397 ## ## Mcnemar&#39;s Test P-Value : 0.8283 ## ## Sensitivity : 0.8000 ## Specificity : 0.5368 ## Pos Pred Value : 0.7885 ## Neg Pred Value : 0.5543 ## Prevalence : 0.6833 ## Detection Rate : 0.5467 ## Detection Prevalence : 0.6933 ## Balanced Accuracy : 0.6684 ## ## &#39;Positive&#39; Class : 1 ## 정분류율을 0.7167이며, 민감도는 0.8000으로 높게 나타났다. 또, 특이도는 0.5368이다. 정확도가 높다고 해서 무조건 좋은 모형은 아니며, 분석 분야에 따라 다양한 지표들을 활용하여 분석 모형을 선택할 수 있다. install.packages(setdiff(&quot;ROCR&quot;, rownames(installed.packages()))) library(ROCR) pred.nn.roc&lt;-prediction(as.numeric(pred.nn), as.numeric(test[,1])) plot(performance(pred.nn.roc, &quot;tpr&quot;, &quot;fpr&quot;)) abline(a=0, b=1, lty=2, col=&quot;black&quot;) performance(pred.nn.roc, &quot;auc&quot;)@y.values ## [[1]] ## [1] 0.6684211 prediction 함수와 performance 함수로 값을 구하여 plot 함수로 ROC 커브를 그렸으며, AUC값은 @y.values값으로 확인한 결과 0.6684로 나타났다. 4.2.7.2.2 neuralnet neuralnet 패키지는 회귀분석의 맥락에서 신경망을 훈련하기 위해 만들어져서 탄력적 역전파가 사용되었고 인공싱경망 중 빠른 알고리즘의 하나이다. neuralnet함수는 다양한 역전파 알고리즘을 통해 모형을 적합하며, 수행결과는 plot 함수로 편리하게 시각화가 가능하다. 아래의 설명인자 이외에도 err.fct(오차 총합 지정, sse와 ce), act.fct(활서오하 함수 지정, logistic과 tanh) 등으로 추가 모형의 조정이 가능하다. 함수사용법 neuralnet(formula, data, algorithm, threshold, hidden, stepmax ...) 인자 설명 formula 수식(종속변수 ~ 독립변수) data 분석하고자 하는 데이터 algorithm 사용할 알고리즘을 지정, “backprop”(역전파), “rprop+”(Default), “rprop-” 등이 있음 threshold 훈련중단 기준으로 default는 0.01 hidden 은닉 노드의 개수, c(n,m)으로 입력하면 첫번째 hidden layer에 n개의 hidden node를 가지고 두번째 hidden layer에 m개의 hidden node를 가짐 stepmax 인공 신경망 훈련 수행 최대횟수 Q. infert 데이터는 자연유산과 인공유산 후의 불임에 대한 사례-대조 연구자료로 8개의 변수와 248개의 관측치를 가지고 있다. 반응변수 case 변수는 (1:사례, 0:대조)로 나타낸다. infert 데이터를 train, test로 분할하고 neuralnet 함수를 활용하여 인공신경망 모델을 만들어 보자. install.packages(setdiff(&quot;neuralnet&quot;, rownames(installed.packages()))) library(neuralnet) data(infert) in.part&lt;-createDataPartition(infert$case, times=1, p=0.7) table(infert[in.part$Resample1, &quot;case&quot;]) ## ## 0 1 ## 118 56 parts&lt;-as.vector(in.part$Resample1) train.infert&lt;-infert[parts,] test.infert&lt;-infert[-parts,] nn.model2&lt;-neuralnet(case~age+parity+induced+spontaneous, data=train.infert, hidden=c(2,2), algorithm=&quot;rprop+&quot;, threshold=0.01, stepmax=1e+5) # X11() plot(nn.model2) plot으로 나타냈을 때, hidden layer 2개에 hidden node도 2개가 나타남을 확인할 수 있으며, nnet의 그래프와 다르게 가중치가 선의 굵기로 나타나지 않고 수치로 나타남을 확인할 수 있다. names(nn.model2) ## [1] &quot;call&quot; &quot;response&quot; &quot;covariate&quot; ## [4] &quot;model.list&quot; &quot;err.fct&quot; &quot;act.fct&quot; ## [7] &quot;linear.output&quot; &quot;data&quot; &quot;exclude&quot; ## [10] &quot;net.result&quot; &quot;weights&quot; &quot;generalized.weights&quot; ## [13] &quot;startweights&quot; &quot;result.matrix&quot; neuralnet() 함수의 수행 결과의 추가적인 정보는 names 함수를 통해 확인할 수 있다. 분석에 사용한 전체 자료는 $data에 저장되어 있으며, 모형 적합에 사용된 자료는 $covariate와 $response를 통해 확인이 가능하다. 그리고 적합값은 $net.result에 제공되고 가중치의 초기값과 적합값은 $startweights와 $weights에서 제공한다. library(neuralnet) set.seed(1231) test.infert$nn.model2_pred.prob &lt;- compute(nn.model2, covariate=test.infert[,c(2:4,6)])$net.result compute 함수는 각 뉴런의 출력값을 계산해주며, 기존의 분류모형에서 사용된 predict 함수의 역할을 하여 예측값을 구해준다. 분석에 사용한 예측변수를 covariate 인자에 추가하여 예측값을 $net.result를 통해 확인할 수 있다. test.infert$nn.model2_pred &lt;- ifelse(test.infert$nn.model2_pred.prob &gt; 0.5, 1, 0) 로지스틱 회귀분석과 동일하게 neuralnet의 예측값은 범주로 나타나는 것이 아닌 확률값으로 나타나기 때문에 기준이 되는 확률보다 크면 1, 작으면 0으로 범주를 추가한다. confusionMatrix(as.factor(test.infert$nn.model2_pred), as.factor(test.infert[,5])) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 41 16 ## 1 6 11 ## ## Accuracy : 0.7027 ## 95% CI : (0.5852, 0.8034) ## No Information Rate : 0.6351 ## P-Value [Acc &gt; NIR] : 0.13806 ## ## Kappa : 0.3037 ## ## Mcnemar&#39;s Test P-Value : 0.05501 ## ## Sensitivity : 0.8723 ## Specificity : 0.4074 ## Pos Pred Value : 0.7193 ## Neg Pred Value : 0.6471 ## Prevalence : 0.6351 ## Detection Rate : 0.5541 ## Detection Prevalence : 0.7703 ## Balanced Accuracy : 0.6399 ## ## &#39;Positive&#39; Class : 0 ## 정분류율은 0.7이며, 민감도는 0.8163으로 높게 나타났다. 또 특이도는 0.4800이다. 정확도가 높다고 해서 무조건 좋은 모형은 아니며, 분석분야에 따라 다양한 지표들을 활용하여 분석 모형을 선택할 수 있다. 4.3 군집분석 군집분석은 각 개체의 유사성을 측정하여 유사성이 높은 대상 집단을 분류하고, 군집에 속한 객체들의 유사성과 서로 다른 군집에 속한 개체간의 상이성을 규명하는 다변량 분석기법이다. 군집 분석에서 이용되는 다변량 자료는 별도의 반응변수가 요구되지 않으며, 오로지 개체들간의 유사성에만 기초하여 군집을 형성한다. 군집 분석은 이상값 탐지에도 사용되며, 심리학, 사회학, 경영학, 생물학 등 다양한 분야에 이용되고 있다. 4.3.1 군집분석 4.3.1.1 개요 각 객체의 유사성을 측정하여 유사성이 높은 대상 집단을 분류하고, 군집에 속한 객체들의 유사성과 서로 다른 군집에 속한 객체 간의 상이성을 규명하는 분석 방법이다. 군집 분석은 특성에 따라 고객을 여러개의 배타적인 집단으로 나누는 것이며, 결과는 구체적인 군집 분석 방법에 따라 차이가 나타날 수 있다. 군집의 개수나 구조에 대한 가정 없이 데이터들 사이의 거리를 기준으로 군집화를 유도하며, 마케팅 조사에서 소비자들의 상품구매행동이나 life style에 따른 소비자 군을 분류하여 시장 전략 수립 등에 활용한다. 4.3.1.2 특징 4.3.1.2.1 요인분석과의 차이점 요인분석은 유사한 변수를 함께 묶어주는 것이 목적이다. 4.3.1.2.2 판별분석과의 차이점 판별분석은 사저넹 집단이 나누어져 있는 자료를 통해 새로운 데이터를 기존의 집단에 할당하는 것이 목적이다. 4.3.1.3 거리 4.3.1.3.1 연속형 변수의 경우 유클리디안 거리 표준화 거리 마할라노비스 거리 체비셰프 거리 맨하탄 거리 캔버라 거리 민코우스키 거리 4.3.1.3.2 범주형 변수의 경우 자카드 거리 자카드 계수 코사인 유사도 4.3.2 계층적 군집분석 계층적 군집분석은 n개의 군집으로 시작해 점차 군집의 개수를 줄여 나가는 방법이다. 계층적 군집을 형성하는 방법에는 합병형 방법과 분리형 방법이 있다. 4.3.2.1 계층적 군집분석 종류 4.3.2.1.1 최단연결법 4.3.2.1.2 최장연결법 4.3.2.1.3 평균연결법 4.3.2.1.4 와드연결법 4.3.2.1.5 군집화 4.3.2.1.6 R을 활용한 계층적 군집분석 함수사용법 dist(data, method) 인자 설명 data 분석하고자 하는 데이터 method 거리측정 방법, “euclidean”,“maximum”,“manhattan”,“canberra”,“binary”,“minkowski”가 있음 함수사용법 hclust(data, method) 인자 설명 data dist 함수로 거리가 측정된 데이터 method 거리측정 방법, “single”, “complete”, “average”, “median”, “ward.D”가 있음 Q. USArrests 데이터는 미국 주(State)별 강력 범죄율 정보를 담고 있다. USArrests 데이터의 정보로 거리를 구하고 최단, 최장, 평균연결법을 실시해보자. US&lt;-USArrests US.dist&lt;-dist(US, &quot;euclidean&quot;) USArrests 데이터를 US라는 변수에 저장하고, dist 함수로 유클리디안 거리를 구한 뒤 US.dist 변수에 저장하여 데이터를 확인하면 위와 같다. US.single&lt;-hclust(US.dist^2, method=&quot;single&quot;) plot(US.single) US.complete&lt;-hclust(US.dist^2, method=&quot;complete&quot;) plot(US.single) US.average&lt;-hclust(US.dist^2, method=&quot;average&quot;) plot(US.single) US.dist 데이터를 hclust 함수를 활용하여 최단, 최장, 평균 거리법으로 군집화하고 덴드로그램을 그려보자. hclust 함수 안의 US.dist 데이터를 제곱한 이유는 거리의 차이를 많이 두어 군집이 나니ㅜ는 것을 쉽게 확인하기 위해서이다. method를 “single”, “complete”, “average”로 지정하여 최단, 최장, 평균 거리법을 실행할 수 있다. 그리고 해당 결과를 plot 함수로 덴드로그램을 그릴 수 있다. 덴드로그램에서 Height 값에 따라 선을 그어 적절한 군집수를 선정할 수 있다. group&lt;-cutree(US.average, k=6) group ## Alabama Alaska Arizona Arkansas California ## 1 1 1 2 1 ## Colorado Connecticut Delaware Florida Georgia ## 2 3 1 4 2 ## Hawaii Idaho Illinois Indiana Iowa ## 5 3 1 3 5 ## Kansas Kentucky Louisiana Maine Maryland ## 3 3 1 5 1 ## Massachusetts Michigan Minnesota Mississippi Missouri ## 6 1 5 1 2 ## Montana Nebraska Nevada New Hampshire New Jersey ## 3 3 1 5 6 ## New Mexico New York North Carolina North Dakota Ohio ## 1 1 4 5 3 ## Oklahoma Oregon Pennsylvania Rhode Island South Carolina ## 6 6 3 6 1 ## South Dakota Tennessee Texas Utah Vermont ## 5 2 2 3 5 ## Virginia Washington West Virginia Wisconsin Wyoming ## 6 6 5 5 6 cutree함수로 계층적 군집의 결과를 이용하여 tree의 높이나 그룹의 수를 옵션으로 지정하여 원하는 수의 그룹으로 나눌 수 있다. plot(US.average) rect.hclust(US.average, k=6, border=&quot;red&quot;) 덴드로그램은 plot함수와 rect.hclust 함수를 이용하여 각각의 그룹을 사각형으로 구분지어 나타낼 수 있다. 4.3.3 비계층적 군집분석 4.3.3.1 개요 n개의 개체를 k개의 군집으로 나눌 수 있는 모든 가능한 방법을 점검해 최적화한 군집을 형성하는 것이다. 4.3.3.2 K-평균 군집분석 주어진 데이터를 k개의 클러스터로 묶는 알고리즘으로, 각 클러스터와 거리 차이의 분산을 최소화하는 방식으로 동작한다. 4.3.3.2.1 K-평균 군집분석 과정 원하는 군집의 개수와 초기값들을 정해 seed 중심으로 군집을 형성한다. 각 데이터를 거리가 가장 가까운 seed가 있는 군집으로 분류한다. 각 군집의 seed값을 다시 계산한다. 4.3.3.2.2 K-평균 군집분석의 특징 거리 계산을 통해 군집화가 이루어지므로 연속형 변수에 활용이 가능하다. K개의 초기 중심값은 임의로 선택이 가능하며 가급적이면 멀리 떨어지는 것이 바람직하다. 초기 중심값을 임의로 선택할 때 일렬(위아래, 좌우)로 선택하면 군집이 혼합되지 ㅇ낳고 층으로 나누어질 수 있어 주의하여야 한다. 초기 중심값의 선정에 따라 결과가 달라질 수 있다. 초기 중심으로부터의 오차 제곱합을 최소화하는 방향으로 군집이 형성되는 탐욕적(greedy) 알고리즘이므로 안정된 군집은 보장하나 최적이라는 보장은 없다. 장점 알고리즘이 단순하며, 빠르게 수행되어 분석방법 적용이 용이하다. 계층적 군집분석에 비해 많은 양의 데이터를 다룰 수 있다. 내부 구조에 대한 사전정보가 없어도 의미있는 자료구조를 찾을 수 있다. 다양한 형태의 데이터에 적용이 가능하다. 단점 군집의 수, 가중치와 거리 정의가 어렵다. 사전에 주어진 목적이 없으므로 결과해석이 어렵다. 잡음이나 이상값의 영향을 많이 받는다. 블록한 형태가 아닌 군집이 존재할 경우에는 성능이 떨어진다. 초기 군집 수 결정에 어려움이 있다. 4.3.3.2.3 R을 활용한 K-means 군집분석 함수사용법 kmeans(data, centers, ...) 인자 설명 data 분석하고자 하는 데이터 centers 군집의 개수를 설정 Nbclust(data, min.nc, max.nc, method, ...) 인자 설명 data 분석하고자 하는 데이터 min.nc 최소군집의 수 max.nc 최대군집의 수 method 군집분석 방법을 정함. “kmeans”, “median”, “single”, “complete”, “average” 등이 있음 Q. 앞서 분할한 credit 데이터의 train 데이터로 kmenas 군집분석을 해보자. train.data&lt;-train[,-1] credit.kmeans&lt;-kmeans(train.data, centers=2) kmeans 군집 분석을 실시하기 전에 종속변수를 제외한 데이터로 군집분석을 실시해 원래 종속 변수와 군집분석 결과의 정분류율을 확인해 본다. Click for Result ## K-means clustering with 2 clusters of sizes 117, 583 ## ## Cluster means: ## account.balance credit.duration.months previous.credit.payment.status ## 1 2.111111 35.73504 2.384615 ## 2 2.212693 17.52316 2.286449 ## credit.purpose credit.amount savings employment.duration installment.rate ## 1 2.752137 8369.906 2.000000 2.512821 2.495726 ## 2 2.974271 2169.919 1.830189 2.439108 3.087479 ## marital.status guarantor residence.duration current.assets age ## 1 2.470085 1.102564 2.752137 2.974359 35.71795 ## 2 2.312178 1.099485 2.859348 2.228130 35.27101 ## other.credits apartment.type bank.credits occupation dependents telephone ## 1 1.811966 2.059829 1.401709 3.145299 1.153846 1.606838 ## 2 1.825043 1.879931 1.341338 2.838765 1.150943 1.351630 ## foreign.worker ## 1 1.017094 ## 2 1.039451 ## ## Clustering vector: ## 415 463 179 526 195 938 818 118 299 229 244 14 374 665 602 603 ## 2 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 ## 768 709 91 953 348 649 355 840 26 519 426 979 766 211 932 590 ## 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 ## 593 555 871 373 844 143 544 490 621 775 905 937 842 23 923 309 ## 1 2 2 1 1 2 2 2 1 2 2 2 2 2 2 2 ## 135 821 954 224 166 217 290 581 72 588 575 141 722 865 859 153 ## 2 1 1 1 2 2 2 2 2 2 1 2 2 1 2 1 ## 294 277 999 41 431 90 316 223 528 116 606 774 747 456 598 854 ## 2 2 1 2 2 2 2 2 2 1 2 1 2 1 2 2 ## 39 159 752 209 988 994 34 516 13 69 895 755 409 308 278 89 ## 2 1 2 1 2 2 2 2 2 2 2 1 2 2 2 2 ## 537 291 424 880 286 671 121 110 158 64 483 477 480 711 67 663 ## 2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 ## 847 85 165 648 51 74 178 362 236 610 330 726 127 212 686 785 ## 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 ## 814 310 744 243 862 888 792 113 619 893 151 666 614 767 160 391 ## 2 1 1 2 2 1 2 2 2 2 2 2 2 2 2 2 ## 155 974 5 326 784 280 800 789 567 843 238 764 339 920 822 137 ## 2 1 2 2 2 2 2 2 1 2 2 1 2 1 1 2 ## 455 738 560 589 83 696 867 196 769 680 900 926 500 852 344 966 ## 2 2 2 2 1 1 2 1 2 2 2 2 2 2 2 1 ## 459 20 996 164 52 534 177 554 84 523 633 392 302 597 706 864 ## 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## 837 430 710 761 712 428 672 250 429 398 928 381 545 40 522 473 ## 2 2 1 2 2 2 2 2 2 2 2 2 2 2 1 1 ## 200 125 265 959 186 573 252 458 152 54 538 235 289 185 765 413 ## 2 2 2 2 1 1 1 2 2 2 2 2 2 2 2 2 ## 627 794 981 783 205 904 564 857 908 727 346 858 468 509 57 457 ## 2 2 2 1 2 2 2 2 2 2 2 2 1 2 2 2 ## 617 357 279 270 646 347 129 218 618 698 337 976 539 975 861 553 ## 1 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 ## 724 390 498 222 899 657 421 762 660 163 846 673 578 913 878 225 ## 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 ## 389 117 771 885 55 947 811 557 658 682 1000 134 891 688 447 104 ## 2 2 2 2 1 2 1 1 2 2 1 2 1 2 2 1 ## 716 845 210 349 401 258 915 386 941 24 466 130 886 943 377 170 ## 2 1 2 2 2 2 2 2 1 2 2 2 1 2 2 2 ## 445 234 422 508 910 80 894 548 475 903 343 323 479 838 450 111 ## 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 ## 317 741 287 585 292 226 297 605 637 834 237 700 809 33 836 396 ## 2 2 2 1 2 2 2 2 2 2 2 2 2 1 1 2 ## 935 917 76 94 30 723 175 916 685 115 751 608 465 358 902 96 ## 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 ## 782 397 404 148 813 968 714 338 869 106 11 625 364 705 403 461 ## 2 1 2 2 2 2 2 2 2 2 2 1 2 2 2 1 ## 704 31 655 661 16 420 882 417 464 412 810 524 437 732 562 204 ## 2 2 1 2 2 2 2 2 2 2 2 2 2 1 2 1 ## 720 965 624 384 122 399 634 315 259 494 780 48 331 100 108 301 ## 1 2 2 2 1 2 2 2 2 2 2 2 2 1 2 2 ## 10 697 851 980 402 889 804 925 395 986 8 261 541 306 853 883 ## 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 ## 282 267 262 760 219 352 119 452 36 870 961 240 304 600 694 105 ## 2 2 2 2 1 2 2 2 2 2 1 2 2 2 2 2 ## 388 934 180 906 615 241 703 559 37 303 19 378 549 990 733 188 ## 1 2 1 2 1 2 1 1 2 2 2 1 2 2 2 2 ## 860 393 139 992 371 189 311 547 418 382 38 816 319 596 120 604 ## 2 2 2 1 1 2 2 2 1 2 2 2 2 1 2 2 ## 533 441 199 499 944 609 81 942 717 650 6 128 49 476 239 340 ## 2 2 2 2 1 2 2 2 1 2 2 2 2 2 2 2 ## 193 824 561 645 190 191 446 668 630 571 512 59 305 832 61 570 ## 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 ## 591 676 770 955 510 962 88 132 438 777 788 471 251 203 246 481 ## 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## 574 440 435 626 492 817 131 667 478 162 322 692 168 442 276 78 ## 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## 957 819 527 835 95 406 552 379 342 221 184 161 504 448 242 181 ## 2 2 2 2 1 1 2 2 2 2 2 2 2 2 1 2 ## 718 812 918 930 414 407 991 949 273 187 535 171 501 753 601 136 ## 2 1 2 1 2 2 2 2 2 2 2 1 2 2 1 2 ## 79 951 670 890 295 802 505 443 284 595 87 805 651 138 365 803 ## 2 2 2 1 2 2 1 1 2 2 2 2 2 2 2 2 ## 933 644 754 336 739 232 334 987 433 328 876 470 264 779 201 820 ## 2 1 2 2 2 2 2 1 2 2 2 2 2 2 2 2 ## 729 620 815 707 65 507 29 823 206 124 691 263 228 45 332 281 ## 1 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 ## 982 632 427 629 577 268 969 327 271 746 781 167 255 807 612 71 ## 2 1 1 2 2 2 2 2 2 2 1 1 1 2 2 2 ## 260 530 623 451 636 734 757 798 841 872 927 46 863 558 719 98 ## 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 ## 434 220 566 679 324 931 721 877 275 687 231 197 831 169 659 960 ## 2 2 2 2 2 2 1 2 2 2 2 1 2 2 2 2 ## 341 911 320 662 2 274 376 639 681 532 881 638 207 56 750 613 ## 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## 394 958 503 647 68 725 419 53 922 230 333 677 256 742 318 940 ## 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 ## 640 745 678 173 702 351 873 266 372 172 298 778 ## 2 2 2 2 1 2 2 2 2 2 2 2 ## ## Within cluster sum of squares by cluster: ## [1] 801760259 789310923 ## (between_SS / total_SS = 70.2 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; ## [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; 군집의 수를 2개로 분할하여 kmeans 군집분석을 실시한 결과, 117,583의 개체가 모인 군집으로 나누어졌다. 그리고 전체 변동에서 군집간 변동이 차지하는 비율인 (between_SS/total_SS)이 1에 가까울수록 군집이 잘 분류되었다고 판단할 수 있지만, 70.2%이므로 좋은 모델이라고 할 수는 없다. (kmeans.table&lt;-table(train$credit.rating, credit.kmeans$cluster)) ## ## 1 2 ## 0 45 160 ## 1 72 423 (kmeans.table[1,1]+kmeans.table[2,2]) / sum(kmeans.table) ## [1] 0.6685714 정분류율은 0.67로 나타나 좋은 성능을 가지지 않았다고 판단할 수 있다. install.packages(setdiff(&quot;NbClust&quot;, rownames(installed.packages()))) library(NbClust) nc&lt;-NbClust(train.data, min.nc=2, max.nc=15, method=&quot;kmeans&quot;) ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ## ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 6 proposed 2 as the best number of clusters ## * 3 proposed 3 as the best number of clusters ## * 2 proposed 4 as the best number of clusters ## * 4 proposed 5 as the best number of clusters ## * 2 proposed 8 as the best number of clusters ## * 1 proposed 9 as the best number of clusters ## * 1 proposed 10 as the best number of clusters ## * 1 proposed 11 as the best number of clusters ## * 4 proposed 15 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 2 ## ## ## ******************************************************************* “According to the majority rule, the best number of clusters is 2”를 통해 최적의 k는 2로 나타났음을 확인할 수 있다. 분석 전에 최적의 k를 찾고 kmeans 분석을 실시하는 것이 필요하다. 4.3.4 혼합 분포 군집 4.3.4.1 개요 모형 기반의 군집 방법이며, 데이터가 k개의 모수적 모형의 가중합으로 표현되는 모집단 모형으로부터 나왔다는 가정하에서 모수와 함께 가중치를 자료로부터 추정하는 방법을 사용한다. K개의 각 모형은 군집을 의미하며, 각 데이터는 추정된 k개의 모형중 어느 모형으로부터 나왔을 확률이 높은지에 따라 군집의 분류가 이루어진다. 흔히 혼합모형에서의 모수와 가중치의 추정(최대가능도추정)에는 EM 알고리즘이 사용된다. 4.3.4.2 특징 K-평균군집의 절차와 유사하지만 확률분포를 도입하여 군집을 수행한다. 군집을 몇개의 모수로 표현할 수 있으며, 서로 다른 크기나 모양의 군집을 찾을 수 있다. EM 알고리즘을 이용한 모수 추정에서 데이터가 커지면 수렵에 시간이 걸릴 수 있다. 군집의 크기가 너무 작으면 추정의 정도가 떨어지거나 어려울 수 있다. K-평균군집과 같이 이상치 자료에 민감하므로 사전에 조치가 필요하다. 4.3.4.3 혼합 분포모형으로 설명할 수 있는 데이터의 형태 4.3.4.4 EM(Expectation-Maximization) 알고리즘의 진행 과정 E단계: 잠재변수 Z의 기대치 계산 M단계: 잠재변수 Z의 기대치를 이용하여 파라미터를 추정 4.3.4.5 R을 이용한 혼합 분포 군집분석 함수사용법 Mclust(data, G, ...) 인자 설명 data 분석하고자 하는 데이터(벡터, 매트릭스, 데이터프레임의 관측치). 단, 수치형 변수만 가능함 G BIC를 계산할 혼합분포 클러스터의 수를 지정, Default는 1:9 Q. iris 데이터의 Species를 제외하고 혼합 분포 군집분석을 실시해 보자. install.packages(setdiff(&quot;mclust&quot;, rownames(installed.packages()))) library(mclust) ## Package &#39;mclust&#39; version 5.4.7 ## Type &#39;citation(&quot;mclust&quot;)&#39; for citing this R package in publications. mc&lt;-Mclust(iris[,1:4], G=3) summary(mc, parameters=TRUE) ## ---------------------------------------------------- ## Gaussian finite mixture model fitted by EM algorithm ## ---------------------------------------------------- ## ## Mclust VEV (ellipsoidal, equal shape) model with 3 components: ## ## log-likelihood n df BIC ICL ## -186.074 150 38 -562.5522 -566.4673 ## ## Clustering table: ## 1 2 3 ## 50 45 55 ## ## Mixing probabilities: ## 1 2 3 ## 0.3333333 0.3005423 0.3661243 ## ## Means: ## [,1] [,2] [,3] ## Sepal.Length 5.006 5.915044 6.546807 ## Sepal.Width 3.428 2.777451 2.949613 ## Petal.Length 1.462 4.204002 5.482252 ## Petal.Width 0.246 1.298935 1.985523 ## ## Variances: ## [,,1] ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 0.13320850 0.10938369 0.019191764 0.011585649 ## Sepal.Width 0.10938369 0.15495369 0.012096999 0.010010130 ## Petal.Length 0.01919176 0.01209700 0.028275400 0.005818274 ## Petal.Width 0.01158565 0.01001013 0.005818274 0.010695632 ## [,,2] ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 0.22572159 0.07613348 0.14689934 0.04335826 ## Sepal.Width 0.07613348 0.08024338 0.07372331 0.03435893 ## Petal.Length 0.14689934 0.07372331 0.16613979 0.04953078 ## Petal.Width 0.04335826 0.03435893 0.04953078 0.03338619 ## [,,3] ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 0.42943106 0.10784274 0.33452389 0.06538369 ## Sepal.Width 0.10784274 0.11596343 0.08905176 0.06134034 ## Petal.Length 0.33452389 0.08905176 0.36422115 0.08706895 ## Petal.Width 0.06538369 0.06134034 0.08706895 0.08663823 MClust 함수에서 클러스터의 수를 3으로 지정했으며, summary 함수의 parameter 인자를 True로 하여 혼합분포의 모수추정치와 함께 각 군집별 해당 자료에 대한 요약 결과를 확인할 수 있다. plot.Mclust(mc) mc$classification ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 2 3 2 ## [75] 2 2 2 3 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 ## [112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [149] 3 3 plot.Mclust 함수를 통해 다양한 방식으로 군집 결과를 시각화할 수 있으며, $classification 인자를 통해 각 개체가 어느 그룹으로 분류되었는지를 확인할 수 있다. 4.4 연관분석 연관 분석은 기업의 데이터베이스에 상품의 구매, 서비스 등 일련의 거래 또는 사건들 간의 규칙을 발견하여 If-Then의 구조로 분석 결과의 연관성을 파악하는 데이터마이닝 방법론이다. 연관 분석은 효율적인 상품 진열, 패키지 상품 개발, 교차판매 전략, 기획 상품의 결정 등에 사용되고 있다. 4.4.1 연관규칙 4.4.1.1 연관규칙분석의 개념 연관성 분석은 흔히 장바구니분석 또는 서열분석이라고 불린다. 기업의 데이터베이스에서 상품의 구매, 서비스 등 일련의 거래 또는 사건들 간의 규칙을 발견하기 위해 적용한다. 장바구니 분석 : 장바구니에 무엇이 같이 들어 있는지에 대한 분석 서열분석 : A를 산 다음에 B를 산다. 4.4.1.2 연관규칙의 형태 조건과 반응 (If-Then) 의 형태로 이루어져 있다. 4.4.1.3 연관규칙의 측도 4.4.1.3.1 지지도 전체 거래 중 항목 A와 항목 B를 동시에 포함하는 거래의 비율로 정의된다. \\(지지도=P(A \\cap B)=\\frac{A와B가 동시에 포함된 거래수}{전체거래수}=\\frac{A \\cap B}{전체}\\) 4.4.1.3.2 신뢰도 항목 A를 포함한 거래 중에서 항목 A와 항목 B가 같이 포함될 확률이다. 연광성의 정도를 파악할 수 있다. \\(신뢰도 = \\frac{P(A \\cap B)}{P(A)}=\\frac{A와B가동시에 포함된 거래수}{A를 포함하는 거래수}=\\frac{지지도}{P(A)}\\) 4.4.1.3.3 향상도 A가 구매되지 않았을 때 품목 B의 구매확률에 비해 A가 구매됐을 때 품목 B의 구매확률의 증가비이다. 연관규칙 A-&gt;B는 품목 A와 품목 B의 구매가 서로 관련이 없는 경우에 향상도가 1이 된다. \\(향상도=\\frac{P(B|A)}{P(B)}=\\frac{P(A\\cap B)}{P(A)P(B)}=\\frac{A와B가 동시에 포함된 거래수}{A를 포함하는 거래수\\times B를 포함하는 거래수}=\\frac{신뢰도}{P(B)}\\) 4.4.1.3.4 Apriori 알고리즘 최소지지도보다 큰 지지도값을 갖는 품목의 집합을 빈발항목집합이라고 한다. Apriori 알고리즘은 모든 품목집합에 대한 지지도를 전부 계산하는 것이 아니라, 최소 지지도 이상의 빈발항목집합을 찾은 후 그것들에 대해서만 연관규칙을 계산하는 것이다. Apriori는 1994년에 발표된 1세대 알고리즘으로 구현과 이해가 쉽다는 장점이 있으나, 지지도가 낮은 후보 집합 생성시 아이템의 개수가 많아지면 계산 복잡도가 증가한다는 문제점을 가지고 있다. 4.4.1.3.5 R을 이용한 연관분석 R에서 연관분석을 수행할 수 있는 함수는 apriori이다. apriori함수는 arules 패키지에서 사용이 가능하며, 트랜잭션 데이터를 다루고 데이터 세트 내에서 최소 N번의 트랜잭션이 일어난 아이템 집합들을 찾아 연관규칙을 계산하는 알고리즘이다. apriori 함수를 이용해 연관분석을 수행하기 전에 as함수를 통해 데이터를 트랜잭션 형태로 변경해야 하며, apriori함수를 이용한 연관분석 결과는 arules 패키지의 inspect함수를 통해 확인할 수 있다. 함수사용법 as(data, class, ...) 인자 설명 data class를 변경하고자 하는 object class object를 변경할 클래스 이름, 연관분석에서는 “transactions” inspect(x, ...) 인자 설명 x 연관규칙 또는 트랜잭션 또는 아이템 매트릭스 데이터 Q. 통신사의 고객 데이터를 입력하고 as함수로 데이터를 변형하고 inspect 함수로 데이터를 확인해 보자. install.packages(setdiff(&quot;arules&quot;, rownames(installed.packages()))) library(arules) ## Loading required package: Matrix ## ## Attaching package: &#39;arules&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following objects are masked from &#39;package:base&#39;: ## ## abbreviate, write id &lt;- c(1, 2, 3, 4, 5, 6) gender &lt;- c(&quot;FEMALE&quot;, &quot;MALE&quot;, &quot;FEMALE&quot;, &quot;FEMALE&quot;, &quot;MALE&quot;, &quot;FEMALE&quot;) age &lt;- c(&quot;age_20&quot;, &quot;age_20&quot;, &quot;age_40&quot;, &quot;age_30&quot;, &quot;age_40&quot;, &quot;age_30&quot;) rank &lt;- c(&quot;Gold&quot;, &quot;Silver&quot;, &quot;Silver&quot;, &quot;VIP&quot;, &quot;Gold&quot;, &quot;Gold&quot;) mobile_app_use &lt;- c(&quot;YES&quot;, &quot;YES&quot;, &quot;NO&quot;, &quot;YES&quot;, &quot;NO&quot;, &quot;YES&quot;) re_order &lt;- c(&quot;YES&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;YES&quot;, &quot;NO&quot;, &quot;YES&quot;) cust_tel &lt;- cbind(id, gender, age, rank, mobile_app_use, re_order) cust_tel &lt;- as.data.frame(cust_tel) cust_tel_1 &lt;- subset(cust_tel, select = -c(id)) Click for Result ## gender age rank mobile_app_use re_order ## 1 FEMALE age_20 Gold YES YES ## 2 MALE age_20 Silver YES NO ## 3 FEMALE age_40 Silver NO NO ## 4 FEMALE age_30 VIP YES YES ## 5 MALE age_40 Gold NO NO ## 6 FEMALE age_30 Gold YES YES tran.cust&lt;-as(cust_tel_1, &quot;transactions&quot;) inspect(tran.cust) ## items transactionID ## [1] {gender=FEMALE, ## age=age_20, ## rank=Gold, ## mobile_app_use=YES, ## re_order=YES} 1 ## [2] {gender=MALE, ## age=age_20, ## rank=Silver, ## mobile_app_use=YES, ## re_order=NO} 2 ## [3] {gender=FEMALE, ## age=age_40, ## rank=Silver, ## mobile_app_use=NO, ## re_order=NO} 3 ## [4] {gender=FEMALE, ## age=age_30, ## rank=VIP, ## mobile_app_use=YES, ## re_order=YES} 4 ## [5] {gender=MALE, ## age=age_40, ## rank=Gold, ## mobile_app_use=NO, ## re_order=NO} 5 ## [6] {gender=FEMALE, ## age=age_30, ## rank=Gold, ## mobile_app_use=YES, ## re_order=YES} 6 as함수를 이용하여 데이터프레임을 transactions 형태로 변환하였으며, 변환된 데이터를 입력하면 데이터의 행과 열의 개수만 나타난다. inspect 함수로 트랜잭션데이터 변환결과를 확인할 수 있다. 함수사용법 apriori(data, parameter, appearance, control) 인자 설명 data 연관규칙 또는 트랜잭션 또는 아이템 매트릭스 데이터 parameter 최소 지지도(supp), 신뢰도(conf), 최대아이템개수(maxlen), 최소아이템개수(minlen) 입력 appearance 특정 연관규칙 결과를 찾을 수 있음. control 결과 보여주기 등의 알고리즘의 성능을 조정할 수 있음 Q. R프로그램의 내장데이터인 Groceries 데이터셋으로 연관규칙분석을 실시해 보자. install.packages(setdiff(&quot;apriori&quot;, rownames(installed.packages()))) ## Installing package into &#39;/home/tingyuan/R/x86_64-pc-linux-gnu-library/3.6&#39; ## (as &#39;lib&#39; is unspecified) ## Warning: package &#39;apriori&#39; is not available (for R version 3.6.3) library(arules) data(&quot;Groceries&quot;) Groceries ## transactions in sparse format with ## 9835 transactions (rows) and ## 169 items (columns) inspect(Groceries[1:3]) ## items ## [1] {citrus fruit, ## semi-finished bread, ## margarine, ## ready soups} ## [2] {tropical fruit, ## yogurt, ## coffee} ## [3] {whole milk} rules&lt;-apriori(Groceries, parameter=list(support=0.01, confidence=0.3)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.3 0.1 1 none FALSE TRUE 5 0.01 1 ## maxlen target ext ## 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 98 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[169 item(s), 9835 transaction(s)] done [0.01s]. ## sorting and recoding items ... [88 item(s)] done [0.00s]. ## creating transaction tree ... done [0.01s]. ## checking subsets of size 1 2 3 4 done [0.00s]. ## writing ... [125 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. parameter값을 최소 지지도는 0.01, 최소신뢰도는 0.3으로 지정했으며, 분석 결과를 확인했을 때, 125 rule(s)를 통해 총 125개의 연관규칙이 생성되었음을 알 수 있다. parameter specification을 통해 구체적인 매개변수 값을 확인할 수 있고 규칙의 수에 따라 지지도와 신뢰도를 높이거나 낮추어 규칙을 조정할 수 있다. inspect(sort(rules, by=c(&quot;confidence&quot;), decreasing=TRUE)[1:5]) ## lhs rhs support ## [1] {citrus fruit,root vegetables} =&gt; {other vegetables} 0.01037112 ## [2] {tropical fruit,root vegetables} =&gt; {other vegetables} 0.01230300 ## [3] {curd,yogurt} =&gt; {whole milk} 0.01006609 ## [4] {other vegetables,butter} =&gt; {whole milk} 0.01148958 ## [5] {tropical fruit,root vegetables} =&gt; {whole milk} 0.01199797 ## confidence coverage lift count ## [1] 0.5862069 0.01769192 3.029608 102 ## [2] 0.5845411 0.02104728 3.020999 121 ## [3] 0.5823529 0.01728521 2.279125 99 ## [4] 0.5736041 0.02003050 2.244885 113 ## [5] 0.5700483 0.02104728 2.230969 118 sort 함수를 통해 confidence에 따라 내림차순 정리했으며, 1~5위까지의 연관규칙을 확인할 수 있다. lhs는 좌항, rhs는 우항을 뜻하고 좌항을 구매했을 때 우항을 구매하는 규칙이다. 각각의 지지도, 신뢰도, 향상도도 확인이 가능하며, count는 해당 규칙의 개수를 나타낸다. 해당 규칙에서 confidence가 높다는 뜻은 구매 품목들의 연관성이 높음을 뜻하며, lift가 높다는 뜻은 좌항의 제품을 구매할 때, 우항의 제품을 구매할 확률이 약 n배 가량 높음을 뜻한다. prune.dup.rules&lt;-function(rules) { rule.subset.matrix&lt;-is.subset(rules, rules, sparse=FALSE) rule.subset.matrix[lower.tri(rule.subset.matrix, diag=TRUE)]&lt;-NA dup.rules&lt;-colSums(rule.subset.matrix, na.rm=TRUE) &gt;= 1 pruned.rules&lt;-rules[!dup.rules] return(pruned.rules) } 좌항에서 우항, 우항에서 좌항의 규칙이 겹치는 경우가 있으므로, 중복 규칙은 없애야 한다. arules 패키지에서는 중복 가지치기 함수를 제공하지 않아 함수를 직접 구현한다. metric.params&lt;-list(supp=0.001, conf=0.5, minlen=2) rules&lt;-apriori(data=Groceries, parameter=metric.params, appearance=list(default=&quot;lhs&quot;, rhs=&quot;soda&quot;), control=list(verbose=FALSE)) rules&lt;-prune.dup.rules(rules) rules&lt;-sort(rules, decreasing=TRUE, by=&quot;confidence&quot;) inspect(rules[1:5]) ## lhs rhs support confidence ## [1] {coffee,misc. beverages} =&gt; {soda} 0.001016777 0.7692308 ## [2] {sausage,bottled water,bottled beer} =&gt; {soda} 0.001118454 0.7333333 ## [3] {sausage,white bread,shopping bags} =&gt; {soda} 0.001016777 0.6666667 ## [4] {rolls/buns,bottled water,chocolate} =&gt; {soda} 0.001321810 0.6500000 ## [5] {pastry,misc. beverages} =&gt; {soda} 0.001220132 0.6315789 ## coverage lift count ## [1] 0.001321810 4.411303 10 ## [2] 0.001525165 4.205442 11 ## [3] 0.001525165 3.823129 10 ## [4] 0.002033554 3.727551 13 ## [5] 0.001931876 3.621912 12 parameter를 리스트 형태로 저장하여 apriori 함수에 적용했으며, appearance를 통해 우측의 soda를 사기 위해 좌항의 아이템을 찾는 것으로 설정했다. control 인자에서 verbose는 apriori 함수 실행 결과를 나타낼지의 여부를 묻는 인자로 FALSE를 지정하여 나타내지 않게했다. 생성된 규칙을 중복 규칙 가지치기를 실기하고, confidence를 기준으로 정렬하여 결과를 확인한다. 분석 결과에서 coffee와 misc.beverages를 함께 구매할 경우 soda를 구매한다는 규칙의 confidence가 가장 높게 나타났다. metric.params&lt;-list(supp=0.001, conf=0.3, minlen=2) rules&lt;-apriori(data=Groceries, parameter=metric.params, appearance=list(default=&quot;rhs&quot;, lhs=c(&quot;yogurt&quot;,&quot;sugar&quot;)), control=list(verbose=FALSE)) rules&lt;-prune.dup.rules(rules) rules&lt;-sort(rules, decreasing=TRUE, by=&quot;confidence&quot;) inspect(rules[1:5]) ## lhs rhs support confidence coverage ## [1] {sugar} =&gt; {whole milk} 0.015048297 0.4444444 0.033858668 ## [2] {yogurt} =&gt; {whole milk} 0.056024403 0.4016035 0.139501779 ## [3] {sugar} =&gt; {other vegetables} 0.010777834 0.3183183 0.033858668 ## [4] {yogurt} =&gt; {other vegetables} 0.043416370 0.3112245 0.139501779 ## [5] {yogurt,sugar} =&gt; {whipped/sour cream} 0.002135231 0.3088235 0.006914082 ## lift count ## [1] 1.739400 148 ## [2] 1.571735 551 ## [3] 1.645119 106 ## [4] 1.608457 427 ## [5] 4.308198 21 최소 지지도는 0.001, 최소 신뢰도는 0.3, 최소 물품수는 2로 지정하고, appearance를 통해 yogurt, sugar를 구매했을 때, 우항의 아이템을 찾는다. 분석 결과를 확인했을 때, sugar를 구매하면 whole milk를 구매한다는 규칙의 confidence가 가장 높게 나타남을 확인할 수 있다. "],["비정형-데이터마이닝.html", "5 비정형 데이터마이닝 5.1 텍스트마이닝", " 5 비정형 데이터마이닝 5.1 텍스트마이닝 텍스트마이닝은 텍스트로부터 고품질의 정보를 도출하는 분석방법으로, 입력된 텍스트를 구조화해 그 데이터에서 패턴을 도출한 후 결과를 평가 및 해석하는 일련의 과정을 의미한다. 주로 구조화된 정형 데이터 속에서 정보나 패턴을 발견하는 데이터마이닝과는 달리 텍스트마이닝은 인터넷 데이터, 소셜 미디어 데이터 등과 같은 자연어로 구성된 비정형 텍스트 데이터 속에서 정보나 관계를 발견하는 분석 기법이다. 단어들 간의 관계를 이용해 감성분석, 워드 클라우드 분석 등을 수행할 후 이 정보를 클러스터링, 분류, 사회연결망 분석 등에 활용한다. 5.1.1 데이터 전처리 5.1.1.1 tm 패키지 tm 패키지는 문서를 관리하는 기본 구조인 Corpus를 생성하여 tm_map 함수를 통해 데이터들을 전처리 및 가공한다. Corpus와 VCorpus 중 VCorpus에서 에러가 적게 나타나므로 주로 VCorpus를 활용한다. 5.1.1.1.1 Corpus Corpus는 데이터마이닝의 절차중 데이터의 정제, 통합, 선택, 변환의 과정을 거친 구조화된 단계로 더 이상 추가적인 절차없이 데이터마이닝 알고리즘 실험에 활용될 수 있는 상태이다. R프로그램의 텍스트마이닝 패키지인 tm에서 문서를 관리하는 기본 구조이며, 텍스트 문서들의 집합을 의미한다. VCorpus: 문서를 Corpus class로 만들어 주는 함수로, 결과는 메모리에 저장되어 현재 구동중인 R메모리에서만 유지된다. 5.1.1.1.2 tm 패키지를 활용한 Corpus 만들기 텍스트 마이닝을 수행하기 전에 tm패키지를 활용해 Corpus를 만들고 생성된 Corpus를 전처리하고 분석에 활용하여야 한다. 텍스트 데이터를 문서로 만들기 위해 VectorSource() 함수를 사용하고 문서로 완성된 데이터를 VCorpus()함수를 이용하여 Corpus로 만든다. 함수 사용법 VectorSource(text) VCorpus(data) Q. 아래의 내장 데이터로 Corpus를 살펴보자. install.packages(setdiff(&quot;tm&quot;, rownames(installed.packages()))) library(tm) ## Loading required package: NLP ## ## Attaching package: &#39;NLP&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## annotate ## ## Attaching package: &#39;tm&#39; ## The following object is masked from &#39;package:arules&#39;: ## ## inspect data(crude) summary(crude)[1:6,] ## Length Class Mode ## 127 2 PlainTextDocument list ## 144 2 PlainTextDocument list ## 191 2 PlainTextDocument list ## 194 2 PlainTextDocument list ## 211 2 PlainTextDocument list ## 236 2 PlainTextDocument list crude 데이터는 로이터 뉴스 기사 중 원유와 관련된 기사 20개가 저장된 데이터이다. summary 결과에서 class는 TextDocument 형태임을 알 수 있고, list 형태로 저장되어 있다. inspect(crude[1]) ## &lt;&lt;VCorpus&gt;&gt; ## Metadata: corpus specific: 0, document level (indexed): 0 ## Content: documents: 1 ## ## $`reut-00001.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 15 ## Content: chars: 527 inspect 함수로 문서의 정보(파일형태, 글자수 등)를 파악할 수 있으며, 문서의 내용은 $content를 통해 확인이 가능하다. 5.1.1.1.3 tm 패키지를 활용한 데이터 전처리 tm_map 함수를 활용하여 코퍼스로 변환된 데이터에 텍스트 전처리를 수행할 수 있다. 공백 제거, 문장부호 제거, 숫자 제거, 불용어 제거 등의 전처리를 통해 텍스트마이닝을 수행할 수 있는 형태로 데이터를 전처리 한다. 함수사용법 tm_map(x, FUN, ...) 인자 설명 x 코퍼스로 변환된 데이터 FUN 변환에 사용할 함수를 입력 tm_map의 function 종류는 아래와 같음 함수 기능 tm_map(x, tolower) 소문자로 만들기 tm_map(x, stemDocument) 어근만 남기기 tm_map(x, stripWhitespace) 공백제거 tm_map(x, removePunctuation) 문장부호 제거 tm_map(x, removeNumbers) 숫자 제거 tm_map(x, removeWords, “word”) 단어 제거 tm_map(x, removeWords, stopWords(“english”)) 불용어 제거 tm_map(x, PlainTextDocument) TextDocument로 변환 Q. 데이터 분석 전문가라는 키워드로 뉴스 기사를 검색하여 10개의 기사를 수집하였다. 수집 데이터를 Corpus로 만들고, tm_map 함수로 데이터를 전처리해 보자. install.packages(setdiff(&quot;tm&quot;, rownames(installed.packages()))) library(tm) news&lt;-readLines(file(&quot;./data/키워드_뉴스.txt&quot;, encoding=&quot;EUC-KR&quot;)) Click for Result news ## [1] &quot;동아대학교(총장 한석정)가 &#39;수요자 데이터기반 스마트헬스케어 서비스&#39;분야 ‘4차 산업혁명 혁신선도대학으로 최종선정됐습니다. 동아대가 혁신선도대학으로 펼치게 될 ‘수요자 데이터기반 스마트헬스케어 서비스’ 산업은 리빙데이터(운동·영양·약물)와 메디컬데이터(생체계측·진료기록)를 종합 분석, 다양한 헬스케어 서비스를 제공하는 것입니다. 동아대는 건강과학대학과 의료원, 재활요양병원 등 경쟁력 있는 인프라를 바탕으로 신뢰도 높은 정밀 분석을 실시, ‘헬스케어 기획 전문가’와 ‘헬스케어 데이터분석 전문가’ 등 수요자 맞춤형 헬스케어 서비스 분야를 선도하는 전문 인재를 키워나갈 계획입니다. ‘스마트헬스케어 융합전공’을 신설, 경영정보학과를 중심으로 한 빅데이터 분석, 식품영양학과·의약생명공학과·건강관리학과 중심의 헬스케어 등 학문 간 경계는 교육혁신도 이뤄나갈 방침입니다. &quot; ## [2] &quot;첨단 정보통신기술에 AI 등이 더해지면서 무기체계가 날로 지능화하는 가운데 &#39;국방개혁 2.0&#39; 추진과 4차 산업기술의 군사분야 접목 속도가 빨라지고 있다. 국방부가 2025년까지 군 내부의 데이터베이스를 기초로 전장의 모든 변수를 모아 최선의 작전계획을 영위하는 ‘AI지휘체계’를 구축한다고 발표한 이래 빅데이터 활용에 대한 관심은 날로 증가하고 있다. 15일 국회의원회관에서 열린 ‘미래국방과 4차산업혁명 교육혁신 포럼’에서 2miles 윤혜식 대표는 ‘빅데이터&amp;블록체인을 통한 스마트 국방’을 주제로 발표하면서 데이터, 클라우드, 인텔리전스를 강조했다. 윤 대표는 “빅데이터 시대의 핵심 기술인 모바일, 인공지능, 로봇, 사물인터넷, 클라우드, 블록체인을 새로운 도구로 활용해 서비스를 혁신하는 국방기술의 디지타이징이 필요하다”며 “국방 서비스의 각 영역들의 통합운영이 필요하고 언제 어디서나 정보에 안전하게 접근하는 모빌리티를 확보해야 한다. 분산되어 있는 정보를 한 곳에 모으고 운영상 발생하는 보안 이슈를 통합 모니터링을 통해 해결하는 클라우드 서비스가 중요하다”고 말한 미 육군 CIO Bruce Crawford 장군의 인터뷰 내용을 인용했다. 그는 미사일 대응체계를 사례로 들며 “적군의 미사일 발사를 감지해 아군에게 정보가 전달되는 과정에서 발생될 수 있는 해커의 방해작업을, 블록체인을 기반으로 한 탈 중앙화 미사일 대응체계를 갖추면 다면화된 내부 블록체인의 보안화로 막을수 있어 효과적으로 상대편 미사일에 대응할 수 있게 된다”고 설명했다. 그러면서 “빅테이터를 활용하게 되면 전략자산에 연결된 자동화된 사전 알람으로 원격 모니터링을 이용해 빠르게 공정설비의 이슈를 파악하고 적합한 엔지니어를 투입할 수 있다”며, “이로써 예방보전과 유지보수 속도가 빨라지고 전투력을 증강시킬 수 있다”고 말했다. 이어 윤 대표는 “가상/증강(VR/AR) 등 첨단기술을 활용한 실감형 과학화 훈련체계를 구축함은 물론 물론 내부망으로 연결된 실시간 화면 송수신으로 부상자의 응급처치도 가능하다”며 “신속하고 스마트한 의사결정으로 전투력을 극대화할 수 있다”고 전했다. 그는 이러한 빅데이터 시대의 성공요인으로 고품질의 데이터, 분석에 대한 전방위의 관점, 분석지향의 리더십, 전략적 타깃, 그리고 데이터 분석 전문가 양성이 무엇보다 중요하다고 덧붙였다. 한편 이 날 포럼을 주최한 백승주 국회 국방위원은 축사를 통해 “미래 대한민국의 국방과 안보를 위해 4차 산업혁명 도입의 필요성을 되짚어보면서 향후 4차 산업혁명 기술을 활용한 국방 및 안보 보안강화 방안을 살펴보고자 한다”며, “미래의 변화에 걸맞은 국방과 안보, 교육과 경제제도가 마련되는 본격적인 계기가 되기 바란다”고 말했다.(konas)&quot; ## [3] &quot;SK하이닉스는 김영한(사진) 미국 샌디에이고 캘리포니아대(UCSD) 종신 교수를 수석 연구위원(전무급)으로 영입했다고 10일 밝혔다. 미 스탠퍼드대에서 통계학 석사, 전기공학 석·박사 학위를 취득한 김 연구위원은 2015년 전자업계 최고 권위의 미국 전기전자공학회(IEEE) 석학회원(Fellow)에도 이름을 올린 세계적인 데이터 과학 전문가다. SK하이닉스는 김 수석 연구위원의 영입에 맞춰 2016년 데이터 분석을 전문적으로 담당하기 위해 설립한 데이터 사이언스 조직 산하에 데이터 리서치 조직을 만들었다. 또 데이터 리서치 하에 ‘MIDAS(Machine Intelligence and Data Analytics Solutions)랩’을 신설했다. MIDAS 랩은 반도체 제조·개발 미세공정뿐 아니라 인사·기업문화 등에까지 인공지능(AI) 기반 시스템을 구축하는 역할을 맡게 된다. SK하이닉스 관계자는 \\&quot;최근 반도체 산업은 제조·개발의 미세공정 난도 증가로 처리해야 하는 데이터의 양이 기하급수적으로 늘어났다\\&quot;며 \\&quot;이에 엔지니어가 직관적으로 판단하는 것이 아니라 AI를 통해 수율(투입량 대비 정상제품 생산 비율)을 올릴 수 있는 최적의 해결 방법을 찾는 것이 경쟁력의 핵심이 되고 있다\\&quot;고 설명했다. SK하이닉스는 AI 경쟁력 확보의 일환으로 현지 시각 10일부터 15일까지 미국 캘리포니아에서 열리는 세계 최고 권위의 AI 학회 국제머신러닝학회(ICML)에도 처음으로 참가해 홍보부스를 열 예정이다. 이를 통해 국내·외 우수 빅데이터, AI 전문가를 적극적으로 유치한다는 계획이다. SK하이닉스 CIO(최고정보관리책임자) 송창록 전무는 \\&quot;올해부터 반도체 제조·개발 현장에 엔지니어뿐 아니라 뛰어난 데이터 분석 전문가들을 투입할 계획\\&quot;이라면서 \\&quot;전문가들이 역량을 발휘할 수 있는AI 기반의 업무 시스템을 확장해나가겠다\\&quot;고 말했다. &quot; ## [4] &quot;삼성SDS(대표 홍원표)는 국내·외 대학(원)생을 대상으로 데이터 분석 경진대회인 ‘브라이틱스 아카데미’ 공모전을 개최한다고 28일 밝혔다. 브라이틱스 아카데미(Brightics Academy)는 데이터 분석 전문가 양성과 빅데이터 분석 플랫폼의 저변 확대를 위해 삼성SDS가 자사 데이터 분석 시스템 플랫폼인 ‘브라이틱스 스튜디오’를 활용하여 강의와 연구를 지원하는 산학 협력 프로그램이다. 이번 공모전은 학생들에게 비전공자라도 이용 가능한 브라이틱스 스튜디오(Brightics Studio)를 통해, 데이터 분석을 해볼 수 있는 기회를 제공하고자 마련됐다. 브라이틱스 스튜디오는 삼성SDS의 기업용 대용량 데이터 분석 플랫폼인 브라이틱스 AI의 오픈소스 버전으로, 지난해 11월 공개 이후 홈페이지 등에서 매주 1,000명 이상 다운로드 받고 있다. 공모 분야는 ▲데이터 사업기획 부문 ▲데이터 분석 부문 ▲연구 혁신 부문 등 3개 부문이며, 심사를 거쳐 각 부문별 대상 500만원(1팀), 최우수상 300만원(2팀), 우수상 200만원(3팀)의 상금이 수여된다. 이 중 대상 수상팀에게는 삼성SDS의 다양한 빅데이터 분석 사례를 경험해 볼 수 있는 4주간의 현장실습 기회가 주어진다. 이은주 삼성SDS 빅데이터분석팀장(상무)은 “이번 브라이틱스 아카데미 공모전을 통해 학생들이 브라이틱스 스튜디오를 활용한 데이터 분석을 경험해 보고, 분석 역량을 향상시키는 좋은 기회가 되기를 바란다”고 밝혔다. 참가 접수는 5월 27일부터 6월 21일까지 브라이틱스 AI 홈페이지에서 진행되며, 최종 과제는 8월 30일까지 제출하면 된다.&quot; ## [5] &quot;푸드테크 스타트업들이 빅데이터 기반 시스템 배달 체계로 전환한다. 일선 상점과 배달기사 수익 향상에 초점을 맞춘 게 특징이다. 고객 입장에서도 도착 시간을 알 수 있다. 배달업체 간 경쟁이 &#39;속도&#39;에서 &#39;빅데이터&#39;로 옮겨 붙는 양상이다. 13일 업계에 따르면 배달 스타트업들이 빅데이터 기반 배송 효율화 작업에 속도를 낸다. 우아한형제들(대표 김봉진)이 운영하는 음식 배달 브랜드 배민라이더스는 고객 편의성을 높이기 위해 빅데이터를 도입했다. 배달 음식 도착 예상 시간을 알려준다. 주문 데이터를 축적하며 예측 오차를 줄이고 있다. 배달기사를 위한 추천 배차 시스템도 개발했다. 실시간 데이터 분석 기술이 적용됐다. 배달기사별 맞춤형 배달 주문이 연결되도록 돕는다. 이동 경로도 제시한다. 배달 시간을 줄이는 것은 물론 배달기사 간 경쟁을 완화한다. 우아한형제들은 지난해 기준 전년 대비 데이터서비스 팀 인력을 두 배 늘렸다. 메쉬코리아(대표 유정범)도 가맹점, 배달기사가 상생(윈윈)할 수 있는 방안을 빅데이터에서 찾는다. 빅데이터 분석 전문가인 데이터 사이언티스트를 영입했다. 배달 주문 접수에서 음식물을 운송, 소비자에게 전달하는 단계별 비효율을 없앤다. 회사 관계자는 “낭비되는 시간, 비효율을 최소화하는 데 연구개발(R&amp;D) 초점이 맞춰져 있다”고 설명했다. 최근 첫발을 뗀 사륜차 기반의 배송 사업 붐업도 빅데이터에 맡겼다. 거점 배송 효율을 높여 기존의 이륜차 물류망과 시너지를 극대화한다. 자동주문 추천, 배달 예상 시간 알림 서비스도 빅데이터를 활용해 고도화한다. 소상공인과의 상생·발전 해법으로 빅데이터를 제시한 업체도 있다. 스파이더크래프트(대표 유현철)는 소상공인 대상 경영 컨설팅 서비스를 개발하고 있다. 주문 내용, 고객 나이·성별, 매출 발생 시간·지역, 인기 메뉴 정보 등을 종합적으로 진단해 가게별 맞춤형 성공 전략을 세워 줄 계획이다. 안전한 배달 환경 조성에도 빅데이터를 접목한다. 내년도 예산은 올해보다 30% 넘게 증액할 방침이다. 신규 지점 개소, 메뉴 개발에 필요한 상권 분석 서비스도 추가한다. 바로고(대표 이태권)도 빅데이터와의 접점을 지속 확대한다. 주문 관련 데이터는 물론 가맹점 피드백까지 수집, 바로고만의 차별화한 서비스를 선보일 계획이다. 데이터 확보에 자신감이 충만하다. 업계 최대 규모의 고객사를 보유했기 때문이다. 공유주방 음식 배달도 업계에서 가장 먼저 시작했다. 바로고 관계자는 “빅데이터를 활용해 가맹점 매출 증대 방안을 찾고 있다”면서 “이 같은 시스템이 자리 잡히면 배달 기사별 예상 수익도 뽑아볼 수 있다”며 웃었다. 사륜차 기반 배송 업계의 최대 화두도 빅데이터다. 화물 운송 플랫폼 센디 운영사 벤디츠(대표 선현국·염상준)는 빅데이터로 차별화한 경쟁력을 키운다. 인공지능(AI) 상담 서비스 출시에 이어 카카오택시처럼 화물차를 부를 수 있는 &#39;물류 클라우드&#39;를 구축했다. 화물 차주가 출근해서 퇴근할 때까지 일정도 짜 준다. 물건을 싣지 않고 다니는 공차 시간을 줄이기 위해서다. 센디 기사들은 10시간 운행 시 최소 7시간 동안 짐을 싣고 다닌다. 업계 평균 대비 약 30% 높은 수치다. 투자도 확충한다. 빅데이터 개발자 인력을 올해 두 배로 늘릴 예정으로 있다. 업계 관계자는 “아직은 데이터베이스(DB)에서 일부 항목 값으로 통계를 내는 수준에 가깝다”면서 “배송 데이터가 쌓이면서 지금보다 더 편리하고 효율적인 빅데이터 기반 서비스가 빠르게 확산될 것”이라며 기대감을 내비쳤다.&quot; ## [6] &quot;코오롱그룹 IT서비스업체 코오롱베니트(대표 이진용)는 정부 &#39;2019 데이터바우처 지원사업’ 참여사로서 머신러닝을 활용한 &#39;데이터 AI 가공 서비스&#39;를 제공한다고 17일 밝혔다. 서비스는 제조현장 시계열 데이터를 분석해 제조 생산성 향상을 돕는 데 쓰일 수 있다. 머신러닝 솔루션 &#39;팔콘리 LRS&#39;를 활용한 생산 및 설비 데이터 패턴 분석을 통해 공정 모니터링, 품질 개선, 설비 고장 예측 등을 수행할 수 있다. 데이터바우처 지원사업은 중소·벤처기업, 소상공인, 스타트업에 데이터 구매와 가공 바우처를 제공하는 사업이다. 지원 대상 기업에 데이터 활용 기회를 높인다는 취지다. 과학기술정보통신부 산하 한국데이터산업진흥원 주관으로 운영된다. 코오롱베니트 측은 팔콘리 LRS를 데이터 분석 전문가 없이 현장 작업자가 직접 쓸 수 있을 정도로 간단해 전문 인력이 부족한 중소기업도 지능형 제조공정을 구축하고 운영할 수 있다고 주장했다. 코오롱베니트 이종찬 본부장은 \\&quot;대기업 제조현장에서는 이미 데이터 분석을 통해 품질, 수율, 가동률 등의 생산성 개선 효과를 체감하고 있다\\&quot;며 \\&quot;도입비용과 전문기술 등 문제로 부담을 느끼는 중소기업들도 데이터를 활용해 제조 경쟁력을 높일 수 있도록 지원하겠다\\&quot;고 말했다. 코오롱베니트는 오는 21일까지 한국데이터산업진흥원 데이터스토어에서 신청을 받아 데이터 AI 가공 서비스를 제공한다.&quot; ## [7] &quot;대구광역시가 DGB대구은행과 공동으로 &#39;제1회 대구 빅데이터 분석 경진대회&#39;를 개최한다고 밝혔습니다. 대한민국 국민이면 누구나 자격 제한 없이 개인 또는 3인 이하의 팀을 구성하여 참여할 수 있으며, 분석 주제는 행정, 교통, 관광, 복지?의료, 금융에 대한 5개의 지정과제와 공공, 금융 부문의 자율과제로 선택할 수 있습니다. 지정과제는 ▲행정(시·도 공공데이터 분석에 따른 대구시 공공데이터 개발 및 행정혁신 방안), ▲교통(노선개편 등을 통한 대중교통 서비스 개선 방안), ▲관광(관광객 증대를 위한 관광자원 발굴 및 개선 방안), ▲복지의료(폭염, 미세먼지 등이 노인 건강에 미치는 영향 분석), ▲금융(상권분석을 통한 소상공인(예비창업자 포함) 지원방안)입니다. 대회 진행은 7월 22일부터 8월 2일까지 참가접수를 통해 1차 서류심사에서 20개 팀을 선발하여 10월 30일 본선 대회에서 최종 5개 팀을 선발해 시상합니다. 상금 규모는 총 3,000만원으로 대상 1팀 1,500만원(대구광역시장상), 최우수상 1팀 1,000만원(대구은행장상), 우수상 1팀 300만원(대구디지털산업진흥원장상) 등으로, 5개의 팀에게 시상할 예정입니다. 관계자는 “이번 경진대회는 대구 지역에서 발생하는 공공부문에 대한 현안 해결을 주제로, 참신한 아이디어를 제공 받아 정책에 적극 활용하고자 올해 처음으로 개최하는 것”이라며, “전국의 데이터 분석 전문가들의 참여를 유도하기 위해 상금을 전국 최고 수준으로 정하여 참여도 및 분석 결과물의 양적?질적인 수준을 높이고자 했다”고 덧붙였습니다. 대구디지털산업진흥원 이승협 원장은 “‘제1회 대구 빅데이터 경진 대회’를 통해 전국의 역량 있는 분석가들이 모여 대구 현안 해결을 위한 좋은 아이디어와 해결책을 제시해 줬으면 좋겠다”고 말했습니다. &quot; ## [8] &quot;광주과학기술원(GIST·총장 김기선)은 김준하 지구·환경공학부 교수가 1일부터 4주간 매주 월요일마다 GIST 오룡관에서 열리는 환경분야 전문가 양성프로그램인 &#39;환경통계 및 데이터 분석 전문가 교육(E-DAP)&#39;에서 재능 기부로 강연한다고 밝혔다. E-DAP은 지난 2014년 7월부터 GIST 국제환경연구소가 전국 대학생과 공무원, 기업인, 연구원 등을 대상으로 실시해온 교육 프로그램이다. 지난해까지 매년 선착순 50여 명을 선발했으며 올해는 75명의 수강생이 참여한다. 김준하 교수는 지난 15여년간 강의하며 집필한 &#39;환경통계 및 데이터 분석(R과 SPSS를 활용한 자기주도 학습서)&#39;을 기반으로 강연한다. 환경공학 관련 실험과 현장연구를 통해 얻은 다양한 종류의 데이터를 논리에 맞게 해석하는 능력을 함양할 수 있도록 할 예정이다. 국산 시뮬레이션 소프트웨어인 &#39;에드슨(EDISON)&#39;과 빅 데이터 분석에 최적화된 &#39;R&#39;을 활용해 수강생들이 학습내용을 현장에서 쉽게 적용할 수 있도록 할 계획이다. 김 교수는 “4차 산업혁명 시대에서 빅데이터 분석과 최신 과학기술을 활용한 실용적 연구 능력을 갖추는 것은 더 이상 선택이 아닌 필수”라며 “차세대 환경 리더를 양성하고, 더 나아가 대한민국의 빅 데이터 및 인공지능 분야 경쟁력 제고에 기여하겠다”고 밝혔다.&quot; ## [9] &quot;포스코 광양제철소(소장 이시우)가 지난 27일 광양제철소본부 대강당에서 ‘2019년 상반기 빅데이터 경진대회’를 개최하고 우수 과제를 공유했다. 빅데이터 경진대회는 4차 산업혁명이라는 시대의 변화를 반영해 엔지니어들이 가진 조업 현장의 기술과 데이터 활용기법을 결합해 스마트팩토리 구현을 가속화하고자 2017년부터 개최해오고 있다. 이번 경진대회에 참여한 광양제철소 엔지니어들은 올해 1월부터 대회 참가를 위한 인원 선발과 과제 선정을 시작으로 사내 교육 콘텐츠를 통해 데이터 통계 및 분석 기초 등 빅데이터 관련 교육을 받았다. 또한, 과제 수행 중에는 사내 데이터 분석 전문가를 투입해 과제를 수행하는 엔지니어들이 빅데이터에 대해 체계적으로 학습하고 데이터 분석 능력을 향상시킬 수 있도록 지원했다. 이날 총 6명이 경연을 펼쳐 ‘두께 불량 및 통판성 향상을 위한 마무리 압연 온도 모델 개발’을 주제로 △효과성 △활용성 △발표력 전 부분에서 가장 높은 점수를 받은 열연부 김경수 대리가 최우수상의 영광을 안았다. 우수상은 압연설비부 임용호 대리와 제강부 윤선혁 대리, 장려상은 선강설비부 황민수 사원, 도금부 윤지선 사원, 발전부 문현익 대리가 차지했다. 이시우 광양제철소장은 강평을 통해 “오늘 발표한 내용들이 실제로 현장에 적용된다면 가까운 미래에 스마트 제철소가 완성될 날도 머지않은 것 같다”며 “광양제철소가 다가올 미래에 경쟁력을 확보할 수 있도록 데이터 분석 능력과 함께 조업 현장의 지식 습득을 위한 학습에도 힘써달라”고 당부했다. 한편, 광양제철소는 매년 상반기와 하반기 빅데이터 경진대회를 개최해오고 있으며 올해 하반기에는 빅데이터를 활용한 조업 모델이 실제 현장에 적용한 사례를 중심으로 경진대회를 개최할 계획이다. &quot; ## [10] &quot;KB금융그룹(회장 윤종규)이 역량별 맞춤교육, 연수과정 확대 등 인력 육성 프로그램을 통해 그룹 내 &#39;데이터 분석&#39; 역량을 강화하고 있다. KB금융그룹에 따르면 그룹 내 데이터 분석 초급자들은 각 계열사에 도입된 초급분석 교육과정과 &#39;그룹 데이터분석 CoP(Community of Practice)&#39;를 통해 분석의 기초를 다질 수 있다. 또 빅데이터분석 플랫폼과 시각화툴을 접하도록 해 관련분야에 직원들이 보다 친근하게 다가가게 했다. 중·고급 분석인력에는 연세대 정보대학원과 개설한 &#39;KB데이터분석 아카데미&#39;, 카이스트 전자공학과와 개설한 &#39;AI 인텐시브 코스&#39;를 들을 수 있다. 이러한 외부교육을 통해 KB금융그룹이 지난 3년간 육성한 데이터 분석 전문가는 총 150여명에 달한다. KB금융그룹은 교육에 그치지 않고, 직원별 데이터 분석 역량에 기반해 인사이동을 실시하고, 프로젝트에 투입시키는 등 인력 운용도 하고 있다. 이는 가시적인 성과로 이어졌다. 대표 사례가 위 교육과정을 수료한 직원이 참여해 개발한 KBotSAM(케이봇쌤)이다. 케이봇쌤은 전세계 수많은 금융 빅데이터를 AI가 매일 분석, 학습해 최적 포트폴리오를 제안하는 머신러닝 알고리즘 기반 로보어드바이저 자산관리서비스다. 또 KB금융그룹은 빅데이터를 활용해 모든 계열사가 신용평가모델을 고도화, 머신러닝과 강화학습 기반 신용평가모형도 개발중이다. 금융사기 방지를 위해서 딥러닝 기반 FDS시스템은 24시간 가동되고 있으며, 성능 향상을 위해 지속적으로 시스템도 고도화하고 있다. KB금융 관계자는 “내부직원 육성을 통해 역량 강화를 도모하고 기술을 내재화해 KB만의 차별적인 AI와 빅데이터 서비스를 통해 초격차 리딩 금융그룹을 만들어 갈 것”이라고 밝혔다. KB금융그룹은 인공지능 기반으로 더욱 정교하고 개인화된 고객 마케팅, 최적 금융상품 추천, 중고차 시세 예측, 아파트 가격 추정 모델 등을 개발해 조만간 서비스를 선보일 예정이다.&quot; news.corpus&lt;-VCorpus(VectorSource(news)) news.corpus[[1]]$content ## [1] &quot;동아대학교(총장 한석정)가 &#39;수요자 데이터기반 스마트헬스케어 서비스&#39;분야 ‘4차 산업혁명 혁신선도대학으로 최종선정됐습니다. 동아대가 혁신선도대학으로 펼치게 될 ‘수요자 데이터기반 스마트헬스케어 서비스’ 산업은 리빙데이터(운동·영양·약물)와 메디컬데이터(생체계측·진료기록)를 종합 분석, 다양한 헬스케어 서비스를 제공하는 것입니다. 동아대는 건강과학대학과 의료원, 재활요양병원 등 경쟁력 있는 인프라를 바탕으로 신뢰도 높은 정밀 분석을 실시, ‘헬스케어 기획 전문가’와 ‘헬스케어 데이터분석 전문가’ 등 수요자 맞춤형 헬스케어 서비스 분야를 선도하는 전문 인재를 키워나갈 계획입니다. ‘스마트헬스케어 융합전공’을 신설, 경영정보학과를 중심으로 한 빅데이터 분석, 식품영양학과·의약생명공학과·건강관리학과 중심의 헬스케어 등 학문 간 경계는 교육혁신도 이뤄나갈 방침입니다. &quot; 예제 텍스트 파일을 readLines 함수로 news라는 변수에 저장한다. 텍스트 데이터를 VectorSource함수를 통해 문서를 만들고, VCorpus 함수로 Corpus 형태로 변환한다. Corpus로 변환된 데이터는 리스트 형태이고 $content를 통해 내용을 확인할 수 있다. Corpus 데이터를 전처리하기 위해 사용자 지정함수를 제작하여 데이터 전처리를 수행한다. clean_txt&lt;-function(txt){ txt&lt;-tm_map(txt, removeNumbers) txt&lt;-tm_map(txt, removePunctuation) txt&lt;-tm_map(txt, stripWhitespace) return(txt) } tm_map 함수를 통해 숫자 제거, 문장부호 제거, 공백 제거를 진행하고 txt에 데이터를 저장한다. clean.news&lt;-clean_txt(news.corpus) clean.news[[1]]$content ## [1] &quot;동아대학교총장 한석정가 수요자 데이터기반 스마트헬스케어 서비스분야 ‘차 산업혁명 혁신선도대학으로 최종선정됐습니다 동아대가 혁신선도대학으로 펼치게 될 ‘수요자 데이터기반 스마트헬스케어 서비스’ 산업은 리빙데이터운동·영양·약물와 메디컬데이터생체계측·진료기록를 종합 분석 다양한 헬스케어 서비스를 제공하는 것입니다 동아대는 건강과학대학과 의료원 재활요양병원 등 경쟁력 있는 인프라를 바탕으로 신뢰도 높은 정밀 분석을 실시 ‘헬스케어 기획 전문가’와 ‘헬스케어 데이터분석 전문가’ 등 수요자 맞춤형 헬스케어 서비스 분야를 선도하는 전문 인재를 키워나갈 계획입니다 ‘스마트헬스케어 융합전공’을 신설 경영정보학과를 중심으로 한 빅데이터 분석 식품영양학과·의약생명공학과·건강관리학과 중심의 헬스케어 등 학문 간 경계는 교육혁신도 이뤄나갈 방침입니다 &quot; txt2&lt;-gsub(&quot;[[:punct:]]&quot;, &quot;&quot;, clean.news[[1]]) 전처리 결과에서 숫자와 구두점 등을 제거했으나, ’,. 와 같은 부호는 제거되지 않아 gsub 함수를 통해 제거할 수 있다. gsub에서 [[:punct:]]와 같은 용어를 통해 전체 대체가 가능하다. 5.1.1.2 자연어 처리 자연어 처리는 기본적으로 형태소 분석을 하는 과정을 포함하고 있다. 문장의 품사를 구분하여 분석에 필요한 품사만 추출하여 활용할 수 있다. R에서 한글 자연어 분석을 하기 위해 KoNLP 패키지를 이용한다. 25개의 함수가 들어 있으며, 형태소 분석 등의 자연어 처리 및 텍스트 마이닝을 수행할 수 있다. 5.1.1.2.1 KoNLP 패키지를 활용한 한글처리 함수 사용법 buildDictionary(ext_dic, data) 인자 설명 ext_dic 단어를 추가하고자 하는 사전을 선택. “woorimalsam”, “sejong”, “insighter”이 있음. data 추가하고자 하는 단어와 품사가 들어간 data frame 또는 txt 파일 extraNoun(text) 인자 설명 text 명사를 추출하고자 하는 문장 또는 문서 SimplePos22(text) 인자 설명 text 형태소 분석을 하고자 하는 문장 또는 문서 Q. 간단한 문장으로 명사추출, 사전 단어추가, 품사를 확인해 보자. install.packages(setdiff(&quot;KoNLP&quot;, rownames(installed.packages()))) library(KoNLP) useSejongDic() KoNLP 라이브러리를 활성화하고 useSejongDic() 함수를 실행하여 사용하고자 하는 사전을 설정한다. setence&lt;-&#39;아버지가 방에 스르륵 들어가신다&#39; extractNoun(sentence) buildDictionary(ext_dic=&quot;sejong&quot;, user_dic=data.frame(c(&#39;스르륵&#39;), c(&#39;mag&#39;))) extractNoun(sentence) 명사를 추출하기 위해 예제 문장을 setence라는 데이터에 저장하고 extractNoun 함수로 명사를 추출했다. 결과에서 ’스르륵’은 명사가 아니라 부사인데, ’스르륵’이라는 단어가 세종사전에 포함되어 있지 않으므로 ’스르륵’을 부사로 세종사전에 추가한다. 사전을 추가하고 다시 extractNoun함수를 사용해 결과를 확인하면 ’스르륵’이 제외된 것을 확인할 수 있다. SimplePos22(sentence) SimplePos22 함수로 sentence의 문장을 형태소 분석을 하여 분리된 단어마다 품사를 확인할 수 있으며, NC은 명사, PV는 동사, PA는 형용사를 의미한다. Q. 위의 new 데이터에서 corpus로 변환하지 않고 전처리 및 명사추출, 사전추가, 품사확인을 하고 형용사를 추출해 보자. clean_txt2&lt;-function(txt) { txt&lt;-removeNumbers(txt) txt&lt;-removePunctuation(txt) txt&lt;-stripWhitespace(txt) txt&lt;-gsub(&quot;[^[:alnum:]]&quot;,&quot; &quot;,txt) return(txt) } clean.news2&lt;-clean_txt2(news) Noun.news[5] tm패키지의 tm_map 함수의 인자로 사용되는 FUN을 그대로 함수로 적용하여 사용이 가능하다. 사용자 정의함수에 숫자, 문장부호, 공백 제거 함수를 사용했고 gsub 함수를 활용해 영문자/숫자를 제외한 것들을 제거하는 전처리를 한다. 전처리를 마친 데이터를 Corpus로 반환하지 않고 데이터를 확인했을 때, ‘푸드테크, ’스타트업’ 등과 같은 복합명사가 분리되어 출력되는 것을 확인할 수 있다. buildDictionary(ext_dic=&quot;sejong&quot;, user_dic=data.frame(c(read.table(&quot;food.txt&quot;)))) extractNoun(clean.news2[5]) 복합명사를 명사로 인식할 수 있도록 사전에 등록하고 다시 분석 결과를 확인하면 복합명사도 하나의 명사로 추가된 것으로 확인할 수 있다. 단어사전은 txt파일 형태로도 추가가 가능하며 형태는 ‘단어’, ’품사’로 저장하여 추가할 수 있다. install.packages(&quot;stringr&quot;) library(stringr) doc1&lt;-paste(SimplePos22(clean.news2[[2]])) doc1 stringr 패키지는 R에서 문자열을 처리할 수 있는 패키지로, str_match 함수로 문자열 중 특정 부분이 해당하는 데이터를 선별할 수 있다. SimplePos22 함수를 실행한 결과가 리스트 형태로 나타나므로, 이를 paste 함수를 사용해 character형 벡터로 변형하여 doc1에 저장한다. str_match 함수를 활용해 품사 중 PA(형용사)인 데이터만 뽑아낸다. doc2&lt;-str_match(doc1, &quot;([가-힣]+)/PA&quot;) doc2 doc3&lt;-doc2[,2] doc3[!is.na(doc3)] doc2 데이터에서 1열은 PA를 포함한 단어가 있는 열이며, 2열은 PA를 제외한 단어만 있는 열이 생성되고 PA가 없는 행은 NA로 채워진다. doc2의 2열의 데이터를 doc3에 저장하고 is.na 함수로 NA를 제외한 데이터만 추출한다. 5.1.1.2.2 Stemming 어간 추출(Stemming)은 형태학적 분석을 단순화한 버전이라고 할 수 있으며, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고 할 수 있다. 즉 공통 어간을 가지는 단어를 묶는 작업을 Stemming이라고 한다. R프로그램에서는 tm패키지에서 stemDocument() 함수를 통해 공통으로 들어가지 않은 부분을 제외하고 stemCompletion() 함수를 통해 stemming된 단어와 완성을 위한 dictionary를 함께 넣으면 가장 기본적인 어휘로 완성해주는 역할을 한다. stemDocument(text) stemCompletion(text, dictionary) Q. analyze, analyzed, analyzing 단어의 어간을 추출하고 가장 기본단어로 만들어 보자. test&lt;-stemDocument(c(&#39;analyze&#39;, &#39;analyzed&#39;, &#39;analyzing&#39;)) test completion&lt;-stemCompletion(test, dictionary=c(&#39;analyze&#39;, &#39;analyzed&#39;, &#39;analyzing&#39;)) completion stemDocument 함수를 통해 앞 어간을 제외한 나머지 부분을 잘려 나가게 만들어 각 단어가 서로 다르지만 사실 모두 analyz-라는 어간을 가지므로 위와 같이 도출된다. stemCompletion 함수를 통해 analyz로 stemming 되었던 단어들이 dictionary에 포함된 단어중 가장 기본 어휘로 완성된 것을 확인할 수 있다. 가장 중요한 것은 stemCompletion을 할 때는 단어의 완성을 위해 반드시 dictionary가 필요하다. 5.1.2 Term-Document Matrix 앞선 과정을 통해 읽어 들인 문서의 빈 공간을 제거하고, 대문자를 소문자로 변환, 문장부호 제거, 불용어 처리 등의 과정을 수행했다. 이렇게 전처리된 데이터에서 각 문서와 단어 간의 사용 여부를 이용해 만들어진 matrix가 바로 TDM(Term-Document Matrix)이다. TDM을 보면 문서마다 등장한 단어의 빈도수를 쉽게 파악할 수 있다는 장점이 있다. 5.1.2.1 R을 활용한 TDM 구축하기 함수사용법 TermDocumentMatrix(data, control) 인자 설명 data Corpus 형태의 데이터 control 사전 변경, 가중치 부여 등의 옵션 추가기능 지원 Q. 앞서 전처리가 완료된 clean.news2 데이터를 Vcorpus로 변환하여 TDM을 생성해 보자. VC.news&lt;-VCorpus(VectorSource(clean.news2)) VC.news[[1]]$content TDM.news&lt;-TermDocumentMatrix(VC.news) dim(TDM.news) inspect(TDM.news[1:5, ]) 전처리가 완료된 clean.news2 데이터를 VC.news에 Corpus 형태로 저장하고 VC.news 데이터를 TermDocumentMatrix() 함수를 활용하여 TDM을 구축하였다. dim 함수를 통해 10개의 기사에서 1011개의 단어가 추출되었다는 것을 확인할 수 있으며, inspect 함수로 TDM 구축 결과를 확인할 수 있다. TDM 결과를 확인하면 10개의 문서에서 1~5번째 단어의 분포를 확인할 수 있다. 여기서 ’academy는’이 4번 문서에서 1번 사용됐음을 확인할 수 있다. 대부분의 단어가 모든 문서에서 이용되지 않기 때문에 조회한 내용의 10개 문서와 5개 단어에 대해 사용된 단어는 0 이상의 숫자로 빈도를 나타내고 사용되지 않은 단어는 0으로 표시된다. sparsity는 전체 행렬에서 0이 차지하는 비중을 의미하므로, 45/50로 90%에 해당한다. words&lt;-function(doc) { doc&lt;-as.character(doc) extractNoun(doc) } TDM.news2&lt;-TermDocumentMatrix(VC.news, control=list(tokenize=words)) dim(TDM.news2) tdm2&lt;-as.matrix(TDM.news2) tdm3&lt;-rowSums(tdm2) tdm4&lt;-tdm3[order(tdm3, decreasing=TRUE)] tdm4[1:10] ‘academy는’과 같이 명사 뒤에 조사가 붙는 경우가 있다. extractNoun 함수를 통해 명사만 추출하여 TDM을 다시 구축하면 위와 같은 결과가 나타나게 된다. 모든 문서의 단어 빈도를 분석하여 상위 10개를 추출하면 ’데이터’, ‘빅데이터’ 등 순서로 단어의 빈도를 확인할 수 있다. Q. 단어 사전을 정의하여 해당 단어들에 대해서만 분석 결과를 확인해 보자. mydict&lt;-c(&quot;빅데이터&quot;, &quot;스마트&quot;, &quot;산업혁명&quot;, &quot;인공지능&quot;, &quot;사물인터넷&quot;, &quot;AI&quot;, &quot;스타트업&quot;, &quot;머신러닝&quot;) my.news&lt;-TermDocumentMatrix(VC.news, control=list(tokenize=words, dictionary=mydict)) inspect(my.news) 빅데이터와 관련된 단어를 mydict에 저장하여 TermDocumentMatrix함수의 control 인자에 적용하여 해당 단어들에 대해서만 분석 결과를 확인할 수 있다. 5.1.2.2 TDM을 활용한 분석 및 시각화 5.1.2.2.1 연관성 분석 작성된 TDM에서 특정 단어와의 연관성에 따라 단어를 조회할 수 있다. findAssocs 함수를 통해 TDM과 연관된 단어와의 연관성이 일정 수치 이상인 단어들만 표시할 수 있다. 함수사용법 findAssocs(data, terms, corlimit) |인자|설명| |data|TDM 형태의 데이터| |terms|연관성을 확인할 단어| |corlimit|최소 연관성| Q. VC.news 데이터를 명사만 추출하는 TDM으로 변경하여 TDM에서 ’빅데이터’라는 단어와의 연관성이 0.9 이상인 단어들만 추출해 보자. words&lt;-function(doc) { doc&lt;-as.character(doc) extractNoun(doc) } TDM.news2&lt;-TermDocumentMatrix(VC.news, control=list(tokenize=words)) findAssocs(TDM.news2, &#39;빅데이터&#39;, 0.9) 구축된 TDM과 ‘빅데이터’라는 단어와의 연관성을 파악한 결과, ’가맹점’, ‘개발자’ 등의 단어들이 연관된 단어로 나타나며, 연관성에 대한 수치도 해당 단어 아래에 같이 표시됨을 확인할 수 있다. 5.1.2.2.2 워드 클라우드 문서에 포함되는 단어의 사용 빈도를 효과적으로 보여주기 위한 막대그래프 등의 시각화 도구가 있지만, 워드 클라우드를 이용하면 더 효과적으로 표시할 수 있다. 함수사용법 wordcloud(words, freq, min.freq, random.order, colors, ...) |인자|설명| |words|워드클라우드를 만들고자하는 단어| |freq|단어의 빈도| |min.freq|시각화하려는 단어의 최소 빈도| |random.order|단어의 배치를 랜덤으로 할지 정함. F일때, 빈도순으로 그려짐.| |colors|빈도에 따라 단어의 색을 지정| Q. TDM.news2 데이터를 워드 클라우드로 만들어보자. library(wordcloud) tdm2&lt;-as.matrix(TDM.news2) term.freq&lt;-sort(rowSums(tdm2), decreasing=TRUE) head(term.freq, 15) wordcloud(words=names(term.freq), freq=term.freq, min.freq=5, random.order=FALSE, colors=brewer.pal(8, &#39;Dark2&#39;)) TDM을 Matrix 형태로 변환하여 행 결합을 통해 각 단어마다 빈도를 합쳐 내림차순으로 정렬하여 term.freq 데이터에 저장하고 head 함수를 통해 확인이 가능하다. wordcloud 함수로 term.freq 데이터에서 단어만 가져오고, 빈도는 term.freq의 빈도로 지정하고, 최소 빈도 5로 지정하여 워드클라우드를 그린다. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
