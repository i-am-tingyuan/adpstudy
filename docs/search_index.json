[["정형-데이터마이닝.html", "4 정형 데이터마이닝 4.1 데이터 분할과 성과분석 4.2 분류 분석", " 4 정형 데이터마이닝 4.1 데이터 분할과 성과분석 4.1.1 데이터 분할 4.1.1.1 sample [함수사용법] sample(x, size, replace=FALSE, prob...) Q. credit 데이터를 train, validation, test로 분할해보자. credit.df&lt;-read.csv(&quot;./data/german_credit_dataset.csv&quot;, header=TRUE, sep=&quot;,&quot;) str(credit.df) ## &#39;data.frame&#39;: 1000 obs. of 21 variables: ## $ credit.rating : int 1 1 1 1 1 1 1 1 1 1 ... ## $ account.balance : int 1 1 2 1 1 1 1 1 4 2 ... ## $ credit.duration.months : int 18 9 12 12 12 10 8 6 18 24 ... ## $ previous.credit.payment.status: int 4 4 2 4 4 4 4 4 4 2 ... ## $ credit.purpose : int 2 0 9 0 0 0 0 0 3 3 ... ## $ credit.amount : int 1049 2799 841 2122 2171 2241 3398 1361 1098 3758 ... ## $ savings : int 1 1 2 1 1 1 1 1 1 3 ... ## $ employment.duration : int 2 3 4 3 3 2 4 2 1 1 ... ## $ installment.rate : int 4 2 2 3 4 1 1 2 4 1 ... ## $ marital.status : int 2 3 2 3 3 3 3 3 2 2 ... ## $ guarantor : int 1 1 1 1 1 1 1 1 1 1 ... ## $ residence.duration : int 4 2 4 2 4 3 4 4 4 4 ... ## $ current.assets : int 2 1 1 1 2 1 1 1 3 4 ... ## $ age : int 21 36 23 39 38 48 39 40 65 23 ... ## $ other.credits : int 3 3 3 3 1 3 3 3 3 3 ... ## $ apartment.type : int 1 1 1 1 2 1 2 2 2 1 ... ## $ bank.credits : int 1 2 1 2 2 2 2 1 2 1 ... ## $ occupation : int 3 3 2 2 2 2 2 2 1 1 ... ## $ dependents : int 1 2 1 2 1 2 1 2 1 1 ... ## $ telephone : int 1 1 1 1 1 1 1 1 1 1 ... ## $ foreign.worker : int 1 1 1 2 2 2 2 2 1 1 ... set.seed(1111) idx&lt;-sample(3, nrow(credit.df), replace=TRUE, prob=c(0.5,0.3,0.2)) train&lt;-credit.df[idx==1,] validation&lt;-credit.df[idx==2,] test&lt;-credit.df[idx==3,] nrow(train) ## [1] 483 nrow(validation) ## [1] 293 nrow(test) ## [1] 224 4.1.1.2 createDataPartition caret 패키지에서 목적변수를 고려한 데이터 분리를 지원하며, 함수를 사용해 분리한 데이터는 변수값의 비율이 원본 데이터와 같게 유지된다. [함수사용법] createDataPartition(y, times, p, list=TRUE, ...) Q. credit 데이터를 train, test로 분할해 보자. install.packages(setdiff(&quot;caret&quot;, rownames(installed.packages()))) library(caret) ## Loading required package: lattice ## Loading required package: ggplot2 # 목적변수로 credit.rating을 지정, 생성할 데이터 분할은 1개로 지정, 훈련데이터는 70%로 설정 part&lt;-createDataPartition(credit.df$credit.rating, times=1, p=0.7) parts&lt;-as.vector(part$Resample1) train&lt;-credit.df[parts,] test&lt;-credit.df[-parts,] 4.1.2 성과분석 4.1.2.1 오분류표 (Confusion Matrix) 4.1.2.1.1 개념 목표 변수의 실제 범주와 모형에 의해 예측된 분류 범주 사이의 관계를 나타내는 표 TP (True Positive) TN (True Negative) FP (False Positive) FN (False Negative) 4.1.2.1.2 분석 지표 정분류율 : 전체 관측치 중 실제값과 예측치가 일치한 정도 \\(Accuracy=\\frac{TN+TP}{TN+TP+FN+FP}\\) 오분류율 : 전체 관측치 중 실제값과 예측치가 다른 정도 \\(1 - Accuracy\\) 민감도 (Sensitivity (TPR: True Positive Rate)) : 실제값이 True인 관측치 중 예측치가 적중한 정도 \\(Sensitivity=\\frac{TP}{TP+FN}\\) 특이도 (Specificity (TNR: True Negative Rate)) : 실제값이 False인 관측치 중 예측치가 적중한 정도 \\(Specificity=\\frac{TN}{TN+FP}\\) 정확도 (Precision) : True로 예측된 것 중 실제로 True인 것들의 비율 \\(Precision=\\frac{TP}{TP+FP}\\) 재현율 (Recall) : 실제 True인 값 중 True를 얼마나 찾았는지에 대한 비율 \\(Recall=\\frac{TP}{TP+FN} (=Sensitivity)\\) F1-score : 정확도와 재현율을 보정하여 하나의 지표로 나타낸 값 \\(F_{1}=2\\times \\frac{Precision \\times Recall}{Precision+Recall}\\) [함수사용법] confusionMatrix(data, reference) Q. 임의의 값을 활용하여 Confusion Matrix를 그려보자. install.packages(setdiff(&quot;caret&quot;, rownames(installed.packages()))) library(caret) predicted&lt;-factor(c(1,0,0,1,1,1,0,0,0,1,1,1)) actual&lt;-factor(c(1,0,0,1,1,0,1,1,0,1,1,1)) xtabs(~predicted + actual) ## actual ## predicted 0 1 ## 0 3 2 ## 1 1 6 sum(predicted==actual)/NROW(actual) # 정분류율을 직접 식으로 계산 ## [1] 0.75 confusionMatrix(predicted, actual) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 3 2 ## 1 1 6 ## ## Accuracy : 0.75 ## 95% CI : (0.4281, 0.9451) ## No Information Rate : 0.6667 ## P-Value [Acc &gt; NIR] : 0.3931 ## ## Kappa : 0.4706 ## ## Mcnemar&#39;s Test P-Value : 1.0000 ## ## Sensitivity : 0.7500 ## Specificity : 0.7500 ## Pos Pred Value : 0.6000 ## Neg Pred Value : 0.8571 ## Prevalence : 0.3333 ## Detection Rate : 0.2500 ## Detection Prevalence : 0.4167 ## Balanced Accuracy : 0.7500 ## ## &#39;Positive&#39; Class : 0 ## 4.1.2.2 ROC 그래프 ROC 그래프의 x축에는 FP Ratio(1-특이도)를 나타내며, y축에는 민감도를 나타내 이 두 평가값의 관계로 모형을 평가한다. 모형의 성과를 평가하는 기준은 ROC 그래프의 밑부분 면적이며, 면적이 넓을수록 좋은 모형으로 평가한다. [함수사용법] prediction(predictions, labels) performance(prediction.object, acc(accuracy), fpr(FP Rate), tpr(TP Rate), ...) Q. 임의의 값으로 ROC Curve를 그려보자. library(ROCR) set.seed(12345) probability&lt;-runif(100) (labels&lt;-ifelse(probability&gt;0.5&amp;runif(100)&lt;0.4, 1, 2)) ## [1] 1 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 1 2 2 1 2 2 2 2 1 2 2 1 2 2 2 2 2 2 ## [38] 1 1 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 1 2 2 2 2 1 2 1 2 2 ## [75] 2 2 2 2 2 2 1 2 2 2 2 2 1 2 1 2 2 2 2 1 2 1 1 2 2 2 pred&lt;-prediction(probability, labels) plot(performance(pred, &quot;tpr&quot;, &quot;fpr&quot;)) performance(pred, &quot;auc&quot;)@y.values # AUROC ## [[1]] ## [1] 0.1735986 4.2 분류 분석 분류 분석은 반응변수의 속성값에 대해 다양한 변수를 이용하여 모형을 구축하고 이를 사용해 새로운 자료에 대한 예측 및 분류를 수행하는 분석이다. 반응변수가 범주형인 경우의 예측 모형은 새로 입력되는 자료에 대한 분류가 주목적이며, 반응변수가 연속형인 경우에는 그 값을 예측하는 것이 주목적이다. 예측 민 분류 기법은 목표 마케팅, 성과예측, 의학진단, 사기검출, 제조 등 다양한 분야에 이용되고 있다. 4.2.1 로지스틱 회귀분석 로지스틱 회귀모형은 반응변수가 범주형인 경우에 적용되는 회귀분석 모형이다. 이 방법은 새로운 설명변수의 값이 주어질 때 반응변수의 각 범주에 속할 확률이 얼마인지를 추정하여, 추정 확률을 기준치에 따라분류하는 목적으로 활용된다. 이 때, 모형의 적합을 통해 추정된 확률을 사후확률 (Posterior Probability)라고 한다. 반응변수 y에 대한 다중 로지스틱 회귀모형은 다음과 같다. 로지스틱 회귀모형은 오즈(odds)의 관점에서 해석이 가능하다. exp(\\(\\beta_{1}\\))의 의미는 나머지 변수(x1, …,xk)가 주어질 때, 한단위 증가할 때마다 성공(y=1)의 오즈가 몇 배 증가하는지를 나타내는 값이다. 오즈비(odds ratio) : 오즈는 성공할 확률이 실패할 확률의 몇배인지를 나타내는 확률이며, 오즈비는 오즈의 비율이다. 4.2.1.1 R을 이용한 이항 로지스틱 회귀분석 [함수사용법] glm(formula, data, family=&quot;binomial&quot;...) 인자 설명 formula 수식(종속변수~독립변수) data 분석하고자 하는 데이터 family 분석에 따른 link function 선택, binomial(이항), gaussian(가우시안), Gamma(감마), poisson(포아송) 등이 있음. predict(model, newdata, type, ...) 인자 설명 model 개발한 모형 newdata 예측을 수행할 test 데이터 type 예측 결과의 유형 지정, link(log-odds값), class(범주형(factor)값), response(0~1 확률값) Q. credit 데이터를 분할하고, train 데이터로 로지스틱 회귀모델을 만들어 보자. credit&lt;-read.csv(&quot;./data/credit_final.csv&quot;) class(credit$credit.rating) # 종속변수 factor 변환 ## [1] &quot;integer&quot; credit$credit.rating&lt;-factor(credit$credit.rating) str(credit) ## &#39;data.frame&#39;: 1000 obs. of 21 variables: ## $ credit.rating : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ account.balance : int 1 1 2 1 1 1 1 1 3 2 ... ## $ credit.duration.months : int 18 9 12 12 12 10 8 6 18 24 ... ## $ previous.credit.payment.status: int 3 3 2 3 3 3 3 3 3 2 ... ## $ credit.purpose : int 2 4 4 4 4 4 4 4 3 3 ... ## $ credit.amount : int 1049 2799 841 2122 2171 2241 3398 1361 1098 3758 ... ## $ savings : int 1 1 2 1 1 1 1 1 1 3 ... ## $ employment.duration : int 1 2 3 2 2 1 3 1 1 1 ... ## $ installment.rate : int 4 2 2 3 4 1 1 2 4 1 ... ## $ marital.status : int 1 3 1 3 3 3 3 3 1 1 ... ## $ guarantor : int 1 1 1 1 1 1 1 1 1 1 ... ## $ residence.duration : int 4 2 4 2 4 3 4 4 4 4 ... ## $ current.assets : int 2 1 1 1 2 1 1 1 3 4 ... ## $ age : int 21 36 23 39 38 48 39 40 65 23 ... ## $ other.credits : int 2 2 2 2 1 2 2 2 2 2 ... ## $ apartment.type : int 1 1 1 1 2 1 2 2 2 1 ... ## $ bank.credits : int 1 2 1 2 2 2 2 1 2 1 ... ## $ occupation : int 3 3 2 2 2 2 2 2 1 1 ... ## $ dependents : int 1 2 1 2 1 2 1 2 1 1 ... ## $ telephone : int 1 1 1 1 1 1 1 1 1 1 ... ## $ foreign.worker : int 1 1 1 2 2 2 2 2 1 1 ... set.seed(123) idx&lt;-sample(1:nrow(credit), nrow(credit)*0.7, replace=FALSE) train&lt;-credit[idx,] test&lt;-credit[-idx,] logistic&lt;-glm(credit.rating~.,data=train,family=&quot;binomial&quot;) summary(logistic) ## ## Call: ## glm(formula = credit.rating ~ ., family = &quot;binomial&quot;, data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4763 -0.7811 0.4133 0.7147 2.0078 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.249e+00 1.419e+00 -2.994 0.002754 ** ## account.balance 8.687e-01 1.224e-01 7.096 1.28e-12 *** ## credit.duration.months -2.145e-02 1.072e-02 -2.000 0.045501 * ## previous.credit.payment.status 5.635e-01 1.897e-01 2.971 0.002973 ** ## credit.purpose -4.133e-01 1.111e-01 -3.721 0.000198 *** ## credit.amount -7.722e-05 5.011e-05 -1.541 0.123341 ## savings 3.531e-01 9.689e-02 3.645 0.000268 *** ## employment.duration 1.311e-01 1.003e-01 1.307 0.191067 ## installment.rate -1.986e-01 1.002e-01 -1.983 0.047357 * ## marital.status 1.724e-01 9.722e-02 1.774 0.076139 . ## guarantor 6.995e-01 3.548e-01 1.971 0.048679 * ## residence.duration -2.940e-02 9.385e-02 -0.313 0.754063 ## current.assets -2.963e-01 1.075e-01 -2.757 0.005828 ** ## age 1.587e-02 1.009e-02 1.573 0.115623 ## other.credits 4.845e-01 2.480e-01 1.953 0.050801 . ## apartment.type 4.437e-01 2.061e-01 2.152 0.031369 * ## bank.credits -2.773e-01 2.391e-01 -1.160 0.246186 ## occupation -1.608e-01 1.672e-01 -0.962 0.335964 ## dependents -1.087e-01 2.831e-01 -0.384 0.700955 ## telephone 4.068e-01 2.257e-01 1.803 0.071425 . ## foreign.worker 1.433e+00 8.141e-01 1.760 0.078390 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 846.57 on 699 degrees of freedom ## Residual deviance: 651.47 on 679 degrees of freedom ## AIC: 693.47 ## ## Number of Fisher Scoring iterations: 5 회귀계수의 p-value가 유의수준 0.05보다 높게 나타나는 변수가 많으므로, step 함수에서 단계적 선택법을 이용하여 로지스틱 회귀분석을 다시 실시한다. step.logistic&lt;-step(glm(credit.rating~1, data=train, family=&quot;binomial&quot;), scope=list(lower~1, upper=~account.balance+credit.duration.months+previous.credit.payment.status+credit.purpose+credit.amount+savings+employment.duration+installment.rate+marital.status+guarantor+residence.duration+current.assets+age+other.credits+apartment.type+bank.credits+occupation+dependents+telephone+foreign.worker), direction=&quot;both&quot;) ## Start: AIC=848.57 ## credit.rating ~ 1 ## ## Df Deviance AIC ## + account.balance 1 761.97 765.97 ## + savings 1 817.12 821.12 ## + credit.duration.months 1 818.40 822.40 ## + previous.credit.payment.status 1 821.29 825.29 ## + current.assets 1 832.86 836.86 ## + credit.amount 1 835.30 839.30 ## + age 1 835.37 839.37 ## + credit.purpose 1 837.72 841.72 ## + employment.duration 1 837.80 841.80 ## + other.credits 1 839.28 843.28 ## + foreign.worker 1 839.45 843.45 ## + installment.rate 1 842.94 846.94 ## + marital.status 1 843.24 847.24 ## + telephone 1 843.75 847.75 ## + apartment.type 1 844.24 848.24 ## &lt;none&gt; 846.57 848.57 ## + occupation 1 845.33 849.33 ## + guarantor 1 845.60 849.60 ## + bank.credits 1 845.79 849.79 ## + dependents 1 846.34 850.34 ## + residence.duration 1 846.39 850.39 ## ## Step: AIC=765.97 ## credit.rating ~ account.balance ## ## Df Deviance AIC ## + credit.duration.months 1 738.84 744.84 ## + previous.credit.payment.status 1 747.38 753.38 ## + current.assets 1 748.44 754.44 ## + savings 1 749.48 755.48 ## + foreign.worker 1 751.18 757.18 ## + credit.purpose 1 751.40 757.40 ## + age 1 752.09 758.09 ## + credit.amount 1 752.24 758.24 ## + other.credits 1 754.34 760.34 ## + employment.duration 1 756.99 762.99 ## + guarantor 1 757.16 763.16 ## + marital.status 1 759.13 765.13 ## + installment.rate 1 759.48 765.48 ## &lt;none&gt; 761.97 765.97 ## + occupation 1 760.38 766.38 ## + apartment.type 1 760.65 766.65 ## + telephone 1 760.86 766.86 ## + residence.duration 1 760.91 766.91 ## + dependents 1 761.72 767.72 ## + bank.credits 1 761.95 767.95 ## - account.balance 1 846.57 848.57 ## ## Step: AIC=744.84 ## credit.rating ~ account.balance + credit.duration.months ## ## Df Deviance AIC ## + previous.credit.payment.status 1 724.73 732.73 ## + savings 1 725.43 733.43 ## + credit.purpose 1 727.04 735.04 ## + age 1 730.32 738.32 ## + foreign.worker 1 731.34 739.34 ## + employment.duration 1 732.40 740.40 ## + current.assets 1 733.16 741.16 ## + other.credits 1 733.23 741.23 ## + guarantor 1 733.69 741.69 ## + marital.status 1 734.34 742.34 ## + apartment.type 1 735.60 743.60 ## + telephone 1 735.74 743.74 ## + installment.rate 1 736.81 744.81 ## &lt;none&gt; 738.84 744.84 ## + residence.duration 1 737.75 745.75 ## + occupation 1 738.60 746.60 ## + dependents 1 738.69 746.69 ## + bank.credits 1 738.81 746.81 ## + credit.amount 1 738.84 746.84 ## - credit.duration.months 1 761.97 765.97 ## - account.balance 1 818.40 822.40 ## ## Step: AIC=732.73 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status ## ## Df Deviance AIC ## + savings 1 709.53 719.53 ## + credit.purpose 1 713.08 723.08 ## + foreign.worker 1 717.65 727.65 ## + guarantor 1 718.72 728.72 ## + age 1 719.12 729.12 ## + current.assets 1 719.90 729.90 ## + employment.duration 1 720.59 730.59 ## + other.credits 1 720.85 730.85 ## + bank.credits 1 721.21 731.21 ## + marital.status 1 721.60 731.60 ## + apartment.type 1 722.13 732.13 ## + telephone 1 722.61 732.61 ## + installment.rate 1 722.70 732.70 ## &lt;none&gt; 724.73 732.73 ## + residence.duration 1 724.10 734.10 ## + occupation 1 724.21 734.21 ## + dependents 1 724.63 734.63 ## + credit.amount 1 724.70 734.70 ## - previous.credit.payment.status 1 738.84 744.84 ## - credit.duration.months 1 747.38 753.38 ## - account.balance 1 793.25 799.25 ## ## Step: AIC=719.53 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings ## ## Df Deviance AIC ## + credit.purpose 1 697.86 709.86 ## + guarantor 1 701.87 713.87 ## + foreign.worker 1 703.31 715.31 ## + current.assets 1 704.12 716.12 ## + age 1 704.83 716.83 ## + other.credits 1 705.56 717.56 ## + marital.status 1 705.89 717.89 ## + employment.duration 1 706.35 718.35 ## + apartment.type 1 706.49 718.49 ## + bank.credits 1 706.94 718.94 ## + installment.rate 1 707.42 719.42 ## &lt;none&gt; 709.53 719.53 ## + telephone 1 708.50 720.50 ## + occupation 1 709.13 721.13 ## + residence.duration 1 709.29 721.29 ## + dependents 1 709.40 721.40 ## + credit.amount 1 709.43 721.43 ## - savings 1 724.73 732.73 ## - previous.credit.payment.status 1 725.43 733.43 ## - credit.duration.months 1 733.39 741.39 ## - account.balance 1 760.85 768.85 ## ## Step: AIC=709.86 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings + credit.purpose ## ## Df Deviance AIC ## + current.assets 1 690.41 704.41 ## + foreign.worker 1 690.93 704.93 ## + guarantor 1 690.96 704.96 ## + age 1 692.75 706.75 ## + marital.status 1 692.89 706.89 ## + employment.duration 1 694.44 708.44 ## + apartment.type 1 694.56 708.56 ## + other.credits 1 694.89 708.89 ## &lt;none&gt; 697.86 709.86 ## + bank.credits 1 696.00 710.00 ## + installment.rate 1 696.12 710.12 ## + occupation 1 696.52 710.52 ## + telephone 1 696.92 710.92 ## + credit.amount 1 697.40 711.40 ## + dependents 1 697.63 711.63 ## + residence.duration 1 697.77 711.77 ## - credit.purpose 1 709.53 719.53 ## - savings 1 713.08 723.08 ## - previous.credit.payment.status 1 713.72 723.72 ## - credit.duration.months 1 722.75 732.75 ## - account.balance 1 751.20 761.20 ## ## Step: AIC=704.41 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings + credit.purpose + current.assets ## ## Df Deviance AIC ## + apartment.type 1 682.70 698.70 ## + age 1 683.31 699.31 ## + foreign.worker 1 684.57 700.57 ## + guarantor 1 684.94 700.94 ## + marital.status 1 685.00 701.00 ## + employment.duration 1 686.34 702.34 ## + other.credits 1 688.09 704.09 ## + telephone 1 688.35 704.35 ## &lt;none&gt; 690.41 704.41 ## + bank.credits 1 688.64 704.64 ## + installment.rate 1 689.04 705.04 ## + occupation 1 690.02 706.02 ## + residence.duration 1 690.11 706.11 ## + dependents 1 690.13 706.13 ## + credit.amount 1 690.34 706.34 ## - current.assets 1 697.86 709.86 ## - credit.purpose 1 704.12 716.12 ## - previous.credit.payment.status 1 705.30 717.30 ## - savings 1 706.36 718.36 ## - credit.duration.months 1 706.49 718.49 ## - account.balance 1 744.08 756.08 ## ## Step: AIC=698.7 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings + credit.purpose + current.assets + apartment.type ## ## Df Deviance AIC ## + foreign.worker 1 676.21 694.21 ## + guarantor 1 677.12 695.12 ## + age 1 678.76 696.76 ## + marital.status 1 679.37 697.37 ## + employment.duration 1 679.61 697.61 ## + other.credits 1 679.81 697.81 ## + installment.rate 1 680.47 698.47 ## + telephone 1 680.58 698.58 ## &lt;none&gt; 682.70 698.70 ## + bank.credits 1 681.07 699.07 ## + occupation 1 682.35 700.35 ## + residence.duration 1 682.39 700.39 ## + dependents 1 682.63 700.63 ## + credit.amount 1 682.63 700.63 ## - apartment.type 1 690.41 704.41 ## - current.assets 1 694.56 708.56 ## - previous.credit.payment.status 1 696.22 710.22 ## - credit.purpose 1 697.55 711.55 ## - savings 1 699.58 713.58 ## - credit.duration.months 1 700.14 714.14 ## - account.balance 1 735.18 749.18 ## ## Step: AIC=694.21 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings + credit.purpose + current.assets + apartment.type + ## foreign.worker ## ## Df Deviance AIC ## + guarantor 1 672.46 692.46 ## + age 1 672.50 692.50 ## + other.credits 1 673.01 693.01 ## + employment.duration 1 673.02 693.02 ## + marital.status 1 673.43 693.43 ## + telephone 1 673.89 693.89 ## &lt;none&gt; 676.21 694.21 ## + installment.rate 1 674.68 694.68 ## + bank.credits 1 674.79 694.79 ## + residence.duration 1 675.92 695.92 ## + occupation 1 675.94 695.94 ## + credit.amount 1 675.98 695.98 ## + dependents 1 676.17 696.17 ## - foreign.worker 1 682.70 698.70 ## - apartment.type 1 684.57 700.57 ## - current.assets 1 687.01 703.01 ## - previous.credit.payment.status 1 689.51 705.51 ## - credit.duration.months 1 691.63 707.63 ## - credit.purpose 1 691.85 707.85 ## - savings 1 692.44 708.44 ## - account.balance 1 730.93 746.93 ## ## Step: AIC=692.46 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings + credit.purpose + current.assets + apartment.type + ## foreign.worker + guarantor ## ## Df Deviance AIC ## + age 1 668.86 690.86 ## + other.credits 1 669.00 691.00 ## + employment.duration 1 669.15 691.15 ## + marital.status 1 669.93 691.93 ## + telephone 1 670.01 692.01 ## &lt;none&gt; 672.46 692.46 ## + bank.credits 1 670.79 692.79 ## + installment.rate 1 671.11 693.11 ## + credit.amount 1 672.07 694.07 ## + occupation 1 672.19 694.19 ## + residence.duration 1 672.20 694.20 ## - guarantor 1 676.21 694.21 ## + dependents 1 672.43 694.43 ## - foreign.worker 1 677.12 695.12 ## - apartment.type 1 680.81 698.81 ## - current.assets 1 682.08 700.08 ## - previous.credit.payment.status 1 686.53 704.53 ## - credit.purpose 1 687.00 705.00 ## - credit.duration.months 1 688.83 706.83 ## - savings 1 689.97 707.97 ## - account.balance 1 729.18 747.18 ## ## Step: AIC=690.86 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings + credit.purpose + current.assets + apartment.type + ## foreign.worker + guarantor + age ## ## Df Deviance AIC ## + other.credits 1 665.40 689.40 ## + marital.status 1 666.21 690.21 ## + bank.credits 1 666.82 690.82 ## &lt;none&gt; 668.86 690.86 ## + telephone 1 666.95 690.95 ## + employment.duration 1 667.09 691.09 ## + installment.rate 1 667.18 691.18 ## - age 1 672.46 692.46 ## + credit.amount 1 668.46 692.46 ## - guarantor 1 672.50 692.50 ## + occupation 1 668.56 692.56 ## + dependents 1 668.86 692.86 ## + residence.duration 1 668.86 692.86 ## - foreign.worker 1 673.34 693.34 ## - apartment.type 1 673.96 693.96 ## - current.assets 1 678.94 698.94 ## - previous.credit.payment.status 1 680.46 700.46 ## - credit.duration.months 1 683.44 703.44 ## - credit.purpose 1 683.80 703.80 ## - savings 1 685.13 705.13 ## - account.balance 1 726.24 746.24 ## ## Step: AIC=689.4 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings + credit.purpose + current.assets + apartment.type + ## foreign.worker + guarantor + age + other.credits ## ## Df Deviance AIC ## + marital.status 1 662.52 688.52 ## &lt;none&gt; 665.40 689.40 ## + telephone 1 663.54 689.54 ## + employment.duration 1 663.56 689.56 ## + installment.rate 1 663.60 689.60 ## + bank.credits 1 663.93 689.93 ## + credit.amount 1 664.86 690.86 ## - other.credits 1 668.86 690.86 ## + occupation 1 664.96 690.96 ## - age 1 669.00 691.00 ## - guarantor 1 669.27 691.27 ## + dependents 1 665.39 691.39 ## + residence.duration 1 665.39 691.39 ## - foreign.worker 1 670.15 692.15 ## - apartment.type 1 671.10 693.10 ## - current.assets 1 674.92 696.92 ## - previous.credit.payment.status 1 675.51 697.51 ## - credit.purpose 1 678.94 700.94 ## - credit.duration.months 1 679.13 701.13 ## - savings 1 681.77 703.77 ## - account.balance 1 723.42 745.42 ## ## Step: AIC=688.52 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings + credit.purpose + current.assets + apartment.type + ## foreign.worker + guarantor + age + other.credits + marital.status ## ## Df Deviance AIC ## + installment.rate 1 659.86 687.86 ## &lt;none&gt; 662.52 688.52 ## + telephone 1 660.70 688.70 ## + bank.credits 1 661.03 689.03 ## + employment.duration 1 661.22 689.22 ## - marital.status 1 665.40 689.40 ## + credit.amount 1 662.07 690.07 ## - guarantor 1 666.10 690.10 ## + occupation 1 662.11 690.11 ## - other.credits 1 666.21 690.21 ## - age 1 666.25 690.25 ## + dependents 1 662.49 690.49 ## - apartment.type 1 666.51 690.51 ## + residence.duration 1 662.52 690.52 ## - foreign.worker 1 666.90 690.90 ## - current.assets 1 671.86 695.86 ## - previous.credit.payment.status 1 671.87 695.87 ## - credit.purpose 1 676.96 700.96 ## - credit.duration.months 1 677.03 701.03 ## - savings 1 679.14 703.14 ## - account.balance 1 720.47 744.47 ## ## Step: AIC=687.86 ## credit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + ## savings + credit.purpose + current.assets + apartment.type + ## foreign.worker + guarantor + age + other.credits + marital.status + ## installment.rate ## ## Df Deviance AIC ## &lt;none&gt; 659.86 687.86 ## + credit.amount 1 657.88 687.88 ## + telephone 1 658.20 688.20 ## + bank.credits 1 658.44 688.44 ## + employment.duration 1 658.44 688.44 ## - installment.rate 1 662.52 688.52 ## - guarantor 1 663.09 689.09 ## - foreign.worker 1 663.52 689.52 ## + occupation 1 659.54 689.54 ## - marital.status 1 663.60 689.60 ## - other.credits 1 663.74 689.74 ## + dependents 1 659.77 689.77 ## + residence.duration 1 659.85 689.85 ## - age 1 664.07 690.07 ## - apartment.type 1 664.22 690.22 ## - previous.credit.payment.status 1 668.62 694.62 ## - current.assets 1 669.29 695.29 ## - credit.purpose 1 673.93 699.93 ## - credit.duration.months 1 674.24 700.24 ## - savings 1 676.80 702.80 ## - account.balance 1 716.56 742.56 summary(step.logistic) ## ## Call: ## glm(formula = credit.rating ~ account.balance + credit.duration.months + ## previous.credit.payment.status + savings + credit.purpose + ## current.assets + apartment.type + foreign.worker + guarantor + ## age + other.credits + marital.status + installment.rate, ## family = &quot;binomial&quot;, data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4734 -0.8128 0.4436 0.7404 1.8425 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.453085 1.295853 -3.436 0.000589 *** ## account.balance 0.877274 0.120850 7.259 3.89e-13 *** ## credit.duration.months -0.030855 0.008188 -3.768 0.000164 *** ## previous.credit.payment.status 0.484631 0.165356 2.931 0.003381 ** ## savings 0.377087 0.095484 3.949 7.84e-05 *** ## credit.purpose -0.395226 0.108290 -3.650 0.000263 *** ## current.assets -0.314207 0.103329 -3.041 0.002359 ** ## apartment.type 0.423587 0.202868 2.088 0.036798 * ## foreign.worker 1.371175 0.809628 1.694 0.090344 . ## guarantor 0.608202 0.347239 1.752 0.079853 . ## age 0.019056 0.009441 2.018 0.043540 * ## other.credits 0.483427 0.243663 1.984 0.047256 * ## marital.status 0.181255 0.093959 1.929 0.053720 . ## installment.rate -0.146927 0.090550 -1.623 0.104674 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 846.57 on 699 degrees of freedom ## Residual deviance: 659.86 on 686 degrees of freedom ## AIC: 687.86 ## ## Number of Fisher Scoring iterations: 5 총 20개의 독립변수 중 13개의 독립변수가 선택되었으며, *과 .은 각 유의확률에서 채택이 되는지를 알 수 있다. 로지스틱 회귀식은 아래와 같이 나타난다. \\(P(credit.rating)=\\frac{1}{1+exp[-(-1.45+0.88account.balance+...-0.15installment.rate)]}\\) estimate가 양수이면 독립변수가 1단위 증가할 때 확률이 1에 가까워지고, estimate가 음수이면 독립변수가 1단위 증가할 때 확률이 0에 가까워진다. library(caret) (pred&lt;-predict(step.logistic, test[,-1], type=&quot;response&quot;)) # 예측값을 &quot;response&quot;로 지정하여 확률값을 출력 ## 1 3 4 7 9 12 15 17 ## 0.4120408 0.6118378 0.8287752 0.9181769 0.8756254 0.6843918 0.6123956 0.7257466 ## 18 21 22 25 27 28 32 35 ## 0.8545862 0.6562587 0.8187122 0.9117270 0.3548313 0.8912199 0.6963481 0.5354274 ## 42 43 44 47 50 58 60 62 ## 0.5585623 0.8364706 0.7470300 0.7531220 0.5600975 0.8353797 0.8895306 0.7449967 ## 63 66 70 73 75 77 82 86 ## 0.9429548 0.9387643 0.8516873 0.8224228 0.9481515 0.9478558 0.9323424 0.6739541 ## 92 93 97 99 101 102 103 107 ## 0.7073080 0.2550983 0.8217615 0.6815670 0.9686422 0.2506009 0.7734371 0.9082236 ## 109 112 114 123 126 133 140 142 ## 0.5705113 0.5738746 0.8735354 0.6018558 0.9023466 0.9552089 0.9404364 0.9864618 ## 144 145 146 147 149 150 154 156 ## 0.9329562 0.7825584 0.9806404 0.7563203 0.7958647 0.7759120 0.5112201 0.8989627 ## 157 174 176 182 183 192 194 198 ## 0.2000154 0.6635239 0.8362179 0.7691743 0.2697873 0.7822958 0.9230847 0.7904401 ## 202 208 213 214 215 216 227 233 ## 0.9506486 0.8442133 0.4700817 0.9926162 0.8831600 0.9248918 0.4090421 0.8773947 ## 245 247 248 249 253 254 257 269 ## 0.9013957 0.8224225 0.9496794 0.7450681 0.8175941 0.6385456 0.6716667 0.8695574 ## 272 283 285 288 293 296 300 307 ## 0.9139702 0.9389755 0.3110148 0.9274089 0.9865352 0.7869555 0.9441139 0.7054924 ## 312 313 314 321 325 329 335 345 ## 0.9424433 0.9347673 0.4857308 0.9393010 0.9600646 0.9739688 0.9446268 0.8643433 ## 350 353 354 356 359 360 361 363 ## 0.8665074 0.7143366 0.6647453 0.9745540 0.6030884 0.9571675 0.9343947 0.9881441 ## 366 367 368 369 370 375 380 383 ## 0.8746333 0.8839657 0.6299201 0.1938767 0.4109438 0.9539802 0.9581369 0.8369876 ## 385 387 400 405 408 410 411 416 ## 0.9426122 0.7129784 0.6004036 0.7937358 0.6749529 0.8698886 0.9479245 0.7891121 ## 423 425 432 436 439 444 449 453 ## 0.8673777 0.8854057 0.9597774 0.9703585 0.2658115 0.9508754 0.8628265 0.6940124 ## 454 460 462 467 469 472 474 482 ## 0.6250801 0.9624819 0.7661106 0.9162244 0.3083106 0.8821974 0.9332950 0.7324789 ## 484 485 486 487 488 489 491 493 ## 0.9949158 0.9599063 0.8572638 0.8723732 0.9409401 0.8786549 0.8639081 0.1861704 ## 495 496 497 502 506 511 513 514 ## 0.8758798 0.8549234 0.8996813 0.8678649 0.9368716 0.9470541 0.5835893 0.7205346 ## 515 517 518 520 521 525 529 531 ## 0.7173306 0.5639953 0.5906169 0.7641548 0.6460617 0.8607718 0.7875209 0.9619511 ## 536 540 542 543 546 550 551 556 ## 0.3886664 0.4620445 0.6683025 0.7880538 0.7462993 0.9224374 0.5281521 0.6557149 ## 563 565 568 569 572 576 579 580 ## 0.2330092 0.8570038 0.8865552 0.9570444 0.9035184 0.4413908 0.6281038 0.3626785 ## 582 583 584 586 587 592 594 599 ## 0.6094103 0.8412318 0.4426206 0.6207627 0.5037450 0.4947543 0.1509256 0.6393267 ## 607 611 616 622 628 631 635 641 ## 0.9444137 0.4469861 0.4350562 0.6942851 0.9373949 0.4534486 0.1898027 0.6636863 ## 642 643 652 653 654 656 664 669 ## 0.4613032 0.6911431 0.9049479 0.8419885 0.9659676 0.8858289 0.6496323 0.8331154 ## 674 675 683 684 689 690 693 695 ## 0.8500149 0.7906134 0.9087419 0.9736985 0.8522894 0.3264172 0.7586834 0.8787157 ## 699 701 708 713 715 728 730 731 ## 0.7674790 0.7948799 0.8674318 0.9289444 0.8646000 0.7547228 0.7213844 0.5971793 ## 735 736 737 740 743 748 749 756 ## 0.5683151 0.9734926 0.6673813 0.6835835 0.7376732 0.8792291 0.7720190 0.1485896 ## 758 759 763 772 773 776 786 787 ## 0.1935449 0.2411321 0.7705442 0.3751020 0.6321914 0.1383704 0.8161353 0.5249610 ## 790 791 793 795 796 797 799 801 ## 0.5824022 0.3912481 0.3479640 0.8999023 0.3893990 0.9279937 0.2878022 0.5014722 ## 806 808 825 826 827 828 829 830 ## 0.1469686 0.2080182 0.2501811 0.7994888 0.2773739 0.6401503 0.6154802 0.4419988 ## 833 839 848 849 850 855 856 866 ## 0.7936517 0.4876081 0.1777989 0.6991339 0.7273259 0.9482981 0.1826371 0.9643884 ## 868 874 875 879 884 887 892 896 ## 0.6638910 0.6762806 0.2602226 0.3118001 0.5363162 0.2179213 0.5033415 0.7543552 ## 897 898 901 907 909 912 914 919 ## 0.9062067 0.5762438 0.5157371 0.2894535 0.6484000 0.5229902 0.1543145 0.2554326 ## 921 924 929 936 939 945 946 948 ## 0.2209400 0.3185682 0.3902414 0.7428232 0.4296006 0.6761346 0.5554851 0.9205723 ## 950 952 956 963 964 967 970 971 ## 0.9513133 0.9508658 0.6010311 0.1464600 0.3378311 0.6942551 0.3984775 0.4066565 ## 972 973 977 978 983 984 985 989 ## 0.2523158 0.9263933 0.4497004 0.4379014 0.5426059 0.1238137 0.6028227 0.2606977 ## 993 995 997 998 ## 0.9482636 0.2613449 0.5866442 0.9168300 pred1&lt;-as.data.frame(pred) pred1$grade&lt;-ifelse(pred1$pred&lt;0.5, pred1$grade&lt;-0, pred1$grade&lt;-1) confusionMatrix(data=as.factor(pred1$grade), reference=test[,1], positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 43 23 ## 1 52 182 ## ## Accuracy : 0.75 ## 95% CI : (0.697, 0.798) ## No Information Rate : 0.6833 ## P-Value [Acc &gt; NIR] : 0.006892 ## ## Kappa : 0.3708 ## ## Mcnemar&#39;s Test P-Value : 0.001224 ## ## Sensitivity : 0.8878 ## Specificity : 0.4526 ## Pos Pred Value : 0.7778 ## Neg Pred Value : 0.6515 ## Prevalence : 0.6833 ## Detection Rate : 0.6067 ## Detection Prevalence : 0.7800 ## Balanced Accuracy : 0.6702 ## ## &#39;Positive&#39; Class : 1 ## 구축된 로지스틱 회귀모형으로 test 데이터의 기존 credit.rating 열을 제외한 데이터로 예측을 한다. 정분류율을 확인하기 전에 예측값이 확률로 나타나기 때문에 기준이 되는 확률보다 크면 1, 작으면 0으로 범주를 추가한다. 정분류율(Accuracy)은 0.75이며, 민감도는 0.8878로 높게 나타났다. 또 특이도는 0.4526이다. 정확도가 높다고 해서 무조건 좋은 모형은 아니며, 분석 분야에 따라 다양한 지표들을 활용하여 분석 모형을 선택할 수 있다. install.packages(setdiff(&quot;ROCR&quot;, rownames(installed.packages()))) library(ROCR) pred.logistic.roc&lt;-prediction(as.numeric(pred1$grade), as.numeric(test[,1])) plot(performance(pred.logistic.roc, &quot;tpr&quot;, &quot;fpr&quot;)) abline(a=0, b=1, lty=2, col=&quot;black&quot;) performance(pred.logistic.roc,&quot;auc&quot;)@y.values ## [[1]] ## [1] 0.6702182 prediction 함수와 performance 함수로 값을 구하여 plot 함수로 ROC 커브를 그렸으며, AUC값은 @y.values값으로 확인할 결과 0.67로 나타났다. 4.2.1.2 R을 이용한 다항 로지스틱 회귀분석 예측하고자 하는 분류가 3개 이상이 된다면 다항 로지스틱 회귀분석을 사용한다. R에서는 nnet 패키지의 multinom 등의 함수로 분석을 한다. multinom(formula, data) Q. iris 데이터의 Species를 분류하는 다항 로지스틱 회귀분석을 실시하고 오분류표를 만들어 보자. idx&lt;-sample(1:nrow(iris), nrow(iris)*0.7, replace=FALSE) train.iris&lt;-iris[idx,] test.iris&lt;-iris[-idx,] library(nnet) mul.iris&lt;-multinom(Species~., train.iris) ## # weights: 18 (10 variable) ## initial value 115.354290 ## iter 10 value 11.814376 ## iter 20 value 5.835729 ## iter 30 value 5.729057 ## iter 40 value 5.720449 ## iter 50 value 5.715789 ## iter 60 value 5.711880 ## iter 70 value 5.708283 ## iter 80 value 5.708073 ## iter 90 value 5.707557 ## iter 100 value 5.707382 ## final value 5.707382 ## stopped after 100 iterations # 예측을 통한 정분류율 확인 pred.mul&lt;-predict(mul.iris, test.iris[,-5]) confusionMatrix(pred.mul, test.iris[,5]) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 10 0 ## virginica 0 0 20 ## ## Overall Statistics ## ## Accuracy : 1 ## 95% CI : (0.9213, 1) ## No Information Rate : 0.4444 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 1 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 1.0000 1.0000 ## Specificity 1.0000 1.0000 1.0000 ## Pos Pred Value 1.0000 1.0000 1.0000 ## Neg Pred Value 1.0000 1.0000 1.0000 ## Prevalence 0.3333 0.2222 0.4444 ## Detection Rate 0.3333 0.2222 0.4444 ## Detection Prevalence 0.3333 0.2222 0.4444 ## Balanced Accuracy 1.0000 1.0000 1.0000 4.2.2 의사결정나무 의사결정나무는 분류함수를 의사결정 규칙으로 이뤄진 나무 모양으로 그리는 방법이다. 계산 결과가 의사결정나무에 직접 나타나기 때문에 해석이 간편하다. 의사결정나무는 주어진 입력값에 대하여 출력값을 예측하는 모형으로 분류나무와 회귀나무 모형이 있다. 4.2.2.1 의사결정나무의 분석 과정 의사결정나무의 형성과정은 크게 성장, 가지치기, 타당성 평가, 해석 및 예측으로 이루어진다. 4.2.2.1.1 성장단계 각 마디에서 적절한 최적의 분류규칙을 찾아서 나무를 성장시키는 과정으로 적절한 정지규칙을 만족하면 중단한다. 분리 규칙을 설정하는 분리 기준은 이산형 목표변수, 연속형 목표변수에 따라 나뉘며 아래와 같은 기준값을 사용한다. 이산형 목표변수 기준값 분리기준 카이제곱 통계량 p값 p값이 가장 작은 예측변수와 그때의 최적분리에 의해서 자식마디를 형성 지니 지수 지니 지수를 감소시켜주는 예측변수와 그 때의 최적 분리에 의해서 자식 마디를 형성 엔트로피 지수 엔트로피 지수가 가장 작은 예측 변수와 이 때의 최적분리에 의해 자식 마디를 형성 연속형 목표변수 기준값 분리기준 분산분석에서 F통계량 p값이 가장 작은 예측변수와 그때의 최적분리에 의해서 자식마디를 형성 분산의 감소량 분산의 감소량을 최대화 하는 기준의 최적분리에 의해서 자식마디를 형성 정지규칙은 더 이상 분리가 일어나지 않고, 현재의 마디가 끝마디가 되도록 하는 규칙이며, 의사결정나무의 깊이를 지정하거나 끝마디의 레코드 수의 최소 개수를 지정한다. 4.2.2.1.2 가지치기 단계 오차를 크게 할 위험이 높거나 부적절한 추론 규칙을 가지고 있는 가지 또는 불필요한 가지를 제거하는 단계이다. 나무의 크기를 모형의 복잡도로 볼 수 있으며, 최적의 나무 크기는 자료로부터 추정하게 된다. 일반적으로 사용되는 방법은 마디에 속하는 자료가 일정수 이하일 때 분할을 정지하고 비용-복잡도 가지치기를 이용하여 성장시킨 나무를 가지치기하게 된다. 4.2.2.1.3 타당성 평가 단계 이익도표, 위험도표 혹은 시험자료를 이용하여 의사결정나무를 평가하는 단계이다. 4.2.2.1.4 해설 및 예측 단계 구축된 나무모형을 해석하고 예측모형을 설정한 후 예측에 적용하는 단계이다. 4.2.2.2 의사결정나무 알고리즘 4.2.2.2.1 CART (Classification and Regression Tree) 4.2.2.2.2 C4.5와 C5.0 4.2.2.2.3 CHAID (SHi-squared Automatic Interaction Detection) 4.2.2.3 R을 이용한 의사결정나무 분석 [함수사용법] rpart(formula, data, method, control=rpart.control(), ...) Q. 앞서 분할한 credit 데이터의 train 데이터로 의사결정나무 모델을 만들어 보자. library(rpart) library(rpart.plot) dt.model&lt;-rpart(credit.rating~., method=&quot;class&quot;, data=train, control=rpart.control(maxdepth=5, minsplit=15)) prp(dt.model, type=4, extra=2) 총 700개의 관측치 중 495개의 관측치를 1로 분류했으며, account.balance &gt;= 3인 325개의 노드 중 288이 1로 분류되었음을 의미한다. prp 함수는 rpart.plot 패키지에 속한 함수이며, type, extra 등의 인자를 사용하여 그래프의 모양을 바꿀 수 있다. # rpart 함수를 활용하여 의사결정나무분석 실시 (최적 나무 선정) dt.model$cptable ## CP nsplit rel error xerror xstd ## 1 0.05365854 0 1.0000000 1.0000000 0.05873225 ## 2 0.04390244 3 0.8341463 0.9853659 0.05847732 ## 3 0.03414634 4 0.7902439 0.9804878 0.05839093 ## 4 0.01000000 5 0.7560976 0.9756098 0.05830383 (opt&lt;-which.min(dt.model$cptable[,&quot;xerror&quot;])) ## 4 ## 4 (cp&lt;-dt.model$cptable[opt, &quot;CP&quot;]) ## [1] 0.01 (prune.c&lt;-prune(dt.model, cp=cp)) ## n= 700 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 700 205 1 (0.2928571 0.7071429) ## 2) account.balance&lt; 2.5 375 168 1 (0.4480000 0.5520000) ## 4) credit.duration.months&gt;=22.5 160 69 0 (0.5687500 0.4312500) ## 8) savings&lt; 2.5 128 47 0 (0.6328125 0.3671875) ## 16) credit.purpose&gt;=1.5 111 35 0 (0.6846847 0.3153153) * ## 17) credit.purpose&lt; 1.5 17 5 1 (0.2941176 0.7058824) * ## 9) savings&gt;=2.5 32 10 1 (0.3125000 0.6875000) * ## 5) credit.duration.months&lt; 22.5 215 77 1 (0.3581395 0.6418605) ## 10) previous.credit.payment.status&lt; 1.5 15 3 0 (0.8000000 0.2000000) * ## 11) previous.credit.payment.status&gt;=1.5 200 65 1 (0.3250000 0.6750000) * ## 3) account.balance&gt;=2.5 325 37 1 (0.1138462 0.8861538) * cptable 인자를 통해서 교차타당성 오차를 제공하여 의사결정나무 모델의 가지치기, 트리의 최대 크기조절에 사용한다. nsplit은 분할횟수, xerror는 해당 CP에서 cross validation 했을 때 오류율, xstd는 해당 CP에서 cross validation 했을 때 편차를 나타낸다. cptable에서 xerror가 가장 낮은 split 개수를 선택한다. 위 결과를 확인했을 때, xerror가 가장 낮을 때 nsplit은 5이며, 앞선 모형의 그래프를 봤을 때 의사 결정나무 모델이 분할을 5번까지 한다고 할 수 있다. plotcp(dt.model) plotcp의 결과에서도 xerror가 가장 낮을 때 결과에 따라 교차타당성오차를 최소로 하는 트리를 형성한다. 결과적으로 나무의 크기가 6일 때 최적의 나무라고 할 수 있다. install.packages(setdiff(&quot;caret&quot;, rownames(installed.packages()))) library(caret) pred.dt&lt;-predict(dt.model, test[,-1], type=&quot;class&quot;) confusionMatrix(data=pred.dt, reference=test[,1], positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 42 21 ## 1 53 184 ## ## Accuracy : 0.7533 ## 95% CI : (0.7005, 0.8011) ## No Information Rate : 0.6833 ## P-Value [Acc &gt; NIR] : 0.0047614 ## ## Kappa : 0.3734 ## ## Mcnemar&#39;s Test P-Value : 0.0003137 ## ## Sensitivity : 0.8976 ## Specificity : 0.4421 ## Pos Pred Value : 0.7764 ## Neg Pred Value : 0.6667 ## Prevalence : 0.6833 ## Detection Rate : 0.6133 ## Detection Prevalence : 0.7900 ## Balanced Accuracy : 0.6698 ## ## &#39;Positive&#39; Class : 1 ## 정분류율(Accuracy)은 0.7533며, 민감도는 0.8976로 높게 나타났다. 또, 특이도는 0.4421이다. 정확도가 높다고 해서 무조건 좋은 모형은 아니며, 분석 분야에 따라 다양한 지표들을 활용하여 분석모형을 선택할 수 있다. # ROC 커브 그리기 및 AUC 산출 install.packages(setdiff(&quot;ROCR&quot;, rownames(installed.packages()))) library(ROCR) pred.dt.roc&lt;-prediction(as.numeric(pred.dt), as.numeric(test[,1])) plot(performance(pred.dt.roc,&quot;tpr&quot;, &quot;fpr&quot;)) abline(a=0,b=1,lty=2,col=&quot;black&quot;) performance(pred.dt.roc,&quot;auc&quot;)@y.values ## [[1]] ## [1] 0.6698331 prediction 함수와 performance 함수로 값을 구하여 plot 함수로 ROC 커브를 그렸으며, AUC값은 @y.values값으로 확인한 결과 0.6698로 나타났다. Q. 앞서 분리한 iris 데이터의 Species를 분류하는 의사결정나무분석을 실시하고 오분류표를 만들어 보자. install.packages(setdiff(&quot;rpart&quot;, rownames(installed.packages()))) library(rpart) library(rpart.plot) dt.model2&lt;-rpart(Species~., data=train.iris) prp(dt.model2, type=4, extra=2) pred.dt2&lt;-predict(dt.model2, test.iris[,-5], type=&quot;class&quot;) confusionMatrix(data=pred.dt2, reference=test.iris[,5]) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 10 1 ## virginica 0 0 19 ## ## Overall Statistics ## ## Accuracy : 0.9778 ## 95% CI : (0.8823, 0.9994) ## No Information Rate : 0.4444 ## P-Value [Acc &gt; NIR] : 8.12e-15 ## ## Kappa : 0.9656 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 1.0000 0.9500 ## Specificity 1.0000 0.9714 1.0000 ## Pos Pred Value 1.0000 0.9091 1.0000 ## Neg Pred Value 1.0000 1.0000 0.9615 ## Prevalence 0.3333 0.2222 0.4444 ## Detection Rate 0.3333 0.2222 0.4222 ## Detection Prevalence 0.3333 0.2444 0.4222 ## Balanced Accuracy 1.0000 0.9857 0.9750 4.2.3 앙상블 기법 앙상블 기법은 주어진 자료로부터 여러개의 예측모형들을 만든 후 예측모형들을 조합하여 하나의 최종 예측모형을 만드는 방법이다. 학습방법이 가장 불안전한 의사결정나무에 주로 사용한다. 4.2.3.1 배깅 (Bagging) 4.2.3.1.1 개념 주어진 자료에서 여러개의 부트스트랩 자료를 생성하고 각 부트스트랩 자료에 예측모형을 만든후 결합하여 최종 예측모형을 만드는 방법이다. 보팅은 여러개의 모형으로부터 산출된 결과 중 다수결에 의해서 최종 결과를 선정하는 과정이다. 최적의 의사결정나무를 구축할 때 가장 어려운 부분이 가지치기이지만 배깅에서는 가지치기를 하지 않고 최대로 성정한 의사결정나무들을 활용한다. 훈련자료의 모집단의 분포를 모르기 때문에 실제 문제에서는 평균예측모형을 구할 수 없다. 배깅은 이러한 문제를 해결하기 위해 훈련자료를 모집단으로 생각하고 평균예측모형을 구하여 분산을 줄이고 예측력을 향상시킬 수 있다. 4.2.3.1.2 R을 이용한 Bagging 분석 bagging(formula, data, mfinal, control=, ...) 인자 설명 formula 수식 data 분석하고자하는 데이터 mfinal 반복수 또는 사용할 트리의 수 (default=100) control 의사결정나무를 만들 때 사용할 option을 설정 Q. 앞서 분할한 credit 데이터의 train 데이터로 Bagging 모델을 만들어 보자. install.packages(setdiff(&quot;adabag&quot;, rownames(installed.packages()))) library(adabag) ## Loading required package: foreach ## Loading required package: doParallel ## Loading required package: iterators ## Loading required package: parallel bag&lt;-bagging(credit.rating~., data=train, mfinal=15) names(bag) ## [1] &quot;formula&quot; &quot;trees&quot; &quot;votes&quot; &quot;prob&quot; &quot;class&quot; ## [6] &quot;samples&quot; &quot;importance&quot; &quot;terms&quot; &quot;call&quot; names 함수를 통해 bagging 함수로 생성된 결과들에 어떤 것들이 있는지 확인이 가능하다. 주로 사용하는 인자들에 대한 설명은 아래와 같다. trees: bagging을 통해 생성된 의사결정나무들을 확인할 수 있다. votes: 각 의사결정나무들이 1행 데이터에 대해 1 또는 2열의 분류를 가진다는 것에 대한 투표를 진행한 것이다. prob: 각 행에 대해 1 또는 2열의 특징으로 분류되는 확률을 나타내는 것이다. class: bagging 기법을 활용해 각 행의 분류를 예측한 것이다. samples: 각 의사결정나무에 사용된 부트스트랩 데이터의 레코드 번호를 나타낸다. importance: 변수의 상대적인 중요도를 나타내며, 지니지수의 gain을 고려한 측도이다. bag$importance ## account.balance age ## 32.1612585 9.6931394 ## apartment.type bank.credits ## 0.5240433 0.7862148 ## credit.amount credit.duration.months ## 9.5477783 9.9671607 ## credit.purpose current.assets ## 4.7106811 4.0205891 ## dependents employment.duration ## 0.0000000 2.6329518 ## foreign.worker guarantor ## 0.3666136 3.3424176 ## installment.rate marital.status ## 1.8668982 1.7141508 ## occupation other.credits ## 1.5483235 1.0551176 ## previous.credit.payment.status residence.duration ## 6.2876622 3.1676583 ## savings telephone ## 5.1833824 1.4239586 importance 인자에서 변수의 상대적 중요도를 봤을 때, account.balance, credit.duration.months, age 순서로 변수 중요도가 크다는 것을 파악할 수 있다. library(caret) pred.bg&lt;-predict(bag, test, type=&quot;class&quot;) confusionMatrix(data=as.factor(pred.bg$class), reference=test$credit.rating, positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 50 31 ## 1 45 174 ## ## Accuracy : 0.7467 ## 95% CI : (0.6935, 0.7949) ## No Information Rate : 0.6833 ## P-Value [Acc &gt; NIR] : 0.009816 ## ## Kappa : 0.3905 ## ## Mcnemar&#39;s Test P-Value : 0.135908 ## ## Sensitivity : 0.8488 ## Specificity : 0.5263 ## Pos Pred Value : 0.7945 ## Neg Pred Value : 0.6173 ## Prevalence : 0.6833 ## Detection Rate : 0.5800 ## Detection Prevalence : 0.7300 ## Balanced Accuracy : 0.6875 ## ## &#39;Positive&#39; Class : 1 ## 로지스틱 회귀모형, 의사결정나무 모형과 동일한 형태로 정분류율을 확인할 수 있으며, 분석 결과에서 예측한 값의 class가 numeric형이므로 as.factor 함수를 이용하여 factor로 변형을 해야 한다. 정분류율은 0.7467이며, 민감도는 0.8488로 높게 나타났다. 또, 특이도는 0.5263이다. 정확도가 높다고 해서 무조건 좋은 모형은 아니며, 분석 분야에 따라 다양한 지표들을 활용하여 분석모형을 선택할 수 있다. library(ROCR) pred.bg.roc&lt;-prediction(as.numeric(pred.bg$class), as.numeric(test[,1])) plot(performance(pred.bg.roc, &quot;tpr&quot;, &quot;fpr&quot;)) abline(a=0, b=1, lty=2, col=&quot;black&quot;) performance(pred.bg.roc, &quot;auc&quot;)@y.values ## [[1]] ## [1] 0.6875481 prediction 함수와 performance 함수로 값을 구하여 plot 함수로 ROC 커브를 그렸으며, AUC값은 @y.values값으로 확인한 결과 0.6875로 나타났다. 4.2.3.2 부스팅 (Boosting) 4.2.3.2.1 개념 예측력이 약한 모형들을 결합하여 강한 예측모형을 만드는 방법으로 Adaboost는 이진분류 문제에서 랜덤 분류기보다 조금 더 좋은 분류기 n개에 각각 가중치를 설정하고 n개의 분류기를 결합하여 최종 분류기를 만드는 방법을 제안하였다. 훈련오차를 빨리, 쉽게 줄일 수 있고 배깅에 비해 많은 경우 예측오차가 향상되어 Adaboost의 성능이 배깅보다 뛰어난 경우가 많다. 4.2.3.2.2 R을 이용한 Boosting 분석 [함수사용법] boosting(formula, data, boos=TRUE/FALSE, control=, ...) Q. 앞서 분할한 credit 데이터의 train 데이터로 Boosting 모델을 만들어 보자. library(adabag) boost&lt;-boosting(credit.rating ~ ., data=train, boos=TRUE, mfinal=80) names(boost) ## [1] &quot;formula&quot; &quot;trees&quot; &quot;weights&quot; &quot;votes&quot; &quot;prob&quot; ## [6] &quot;class&quot; &quot;importance&quot; &quot;terms&quot; &quot;call&quot; names 함수를 통해 boosting 함수로 생성된 결과들에 어떤 것들이 있는지 확인이 가능하다. 주로 사용하는 인자들에 대한 설명은 아래와 같다. trees: boosting을 통해 생성된 의사결정나무들을 확인할 수 있다. (80개) weitgts: 각 의사결정나무에 부여된 가중치값을 확인할 수 있다. votes: 각 의사결정나무들이 1행 데이터에 대해 1 또는 2열의 분류를 가진다는 것에 대한 투표를 진행한 것이다. prob: 각 행에 대해 1 또는 2열의 특징으로 분류되는 확률을 나타내는 것이다. class: boosting 기법을 활용해 각 행의 분류를 예측한 것이다. importance: 변수의 상대적인 중요도를 나타내며, 지니지수의 gain을 고려한 측도이다. boost$importance ## account.balance age ## 5.0653036 14.7869602 ## apartment.type bank.credits ## 1.8398062 1.5915217 ## credit.amount credit.duration.months ## 23.0980549 10.0939561 ## credit.purpose current.assets ## 4.3067863 4.6719210 ## dependents employment.duration ## 1.3666935 5.2701945 ## foreign.worker guarantor ## 0.3727978 1.4688110 ## installment.rate marital.status ## 3.6999734 2.2674559 ## occupation other.credits ## 4.7645403 1.6476463 ## previous.credit.payment.status residence.duration ## 3.0414926 4.7966697 ## savings telephone ## 4.2742639 1.5751511 importance 인자에서 변수의 상대적 중요도를 봤을 때, credit.amount, age, credit.duration.months 순서로 변수 중요도가 크다는 것을 파악할 수 있다. library(caret) pred.boos&lt;-predict(boost, test, type=&quot;class&quot;) confusionMatrix(data=as.factor(pred.boos$class), reference=test$credit.rating, positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 48 39 ## 1 47 166 ## ## Accuracy : 0.7133 ## 95% CI : (0.6586, 0.7638) ## No Information Rate : 0.6833 ## P-Value [Acc &gt; NIR] : 0.1455 ## ## Kappa : 0.3223 ## ## Mcnemar&#39;s Test P-Value : 0.4504 ## ## Sensitivity : 0.8098 ## Specificity : 0.5053 ## Pos Pred Value : 0.7793 ## Neg Pred Value : 0.5517 ## Prevalence : 0.6833 ## Detection Rate : 0.5533 ## Detection Prevalence : 0.7100 ## Balanced Accuracy : 0.6575 ## ## &#39;Positive&#39; Class : 1 ## 로지스틱 회귀모형, 의사결정나무 모형과 동일한 형태로 정분류율을 확인할 수 있으며, 분석 결과에서 예측한 값의 class가 numeric형이므로 as.factor 함수를 이용하여 factor로 변형을 해야 한다. 정분류율은 0.7133이며, 민감도는 0.8098로 높게 나타났다. 또, 특이도는 0.5053이다. 정확도가 높다고 해서 무조건 좋은 모형은 아니며, 분석분야에 따라 다양한 지표들을 활용하여 분석모형을 선택할 수 있다. library(ROCR) pred.boos.roc&lt;-prediction(as.numeric(pred.boos$class), as.numeric(test[,1])) plot(performance(pred.boos.roc, &quot;tpr&quot;, &quot;fpr&quot;)) abline(a=0, b=1, lty=2, col=&quot;black&quot;) performance(pred.boos.roc,&quot;auc&quot;)@y.values ## [[1]] ## [1] 0.6575096 prediction 함수와 performance 함수로 값을 구하여 plot 함수로 ROC 커브를 그렸으며, AUC값은 @y.values값으로 확인한 결과 0.6575로 나타났다. 4.2.3.3 랜덤포레스트 (Random Forest) 4.2.3.3.1 개념 의사결정나무의 특징인 분산이 크다는 점을 고려하여 배깅과 부스팅보다 더 많은 무작위성을 주어 약한 학습기들을 생성한 후 이를 선형 결합하여 최종 학습기를 만드는 방법이다. R프로그램에서는 randomForest 패키지로 구현이 가능하다. randomForest 함수를 사용하고 random input에 따른 forest of tree를 생성하여 이를 이용한 분류를 한다. 수천개의 변수를 통해 변수 제거없이 실행되므로 정확도 측면에서 좋은 성과를 보인다. 이론적 설명이나 최종 결과에 대한 해석이 어렵다는 단점이 있지만 예측력이 매우 높은 것으로 알려져 있다. 특히 입력변수가 많은 경우, 배깅/부스팅과 비슷하거나 좋은 예측력을 보인다. 4.2.3.3.2 R을 이용한 RandomForest 분석 R에서 RandomForest 분석을 수행할 수 있는 함수는 randomForest 패키지의 randomForest 함수이며, 이를 이용하여 분류분석을 실시한다. [함수사용법] randomForest(formula, data, ntree, mtry, ...) 인자 설명 formula 수식(종속변수 ~ 독립변수) data 분석하고자 하는 데이터 ntree 사용할 트리의 수, 너무 작은 숫자를 입력하면 예측 불가 mtry 각 분할에서 랜덤으로 뽑힌 변수의 개수보통 classification은 sqrt(변수 개수), regression은 (변수 개수/3) Q. 앞서 분할한 credit 데이터의 train 데이터로 randomforest 모델을 만들어 보자. library(randomForest) ## randomForest 4.6-14 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attaching package: &#39;randomForest&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## margin (rf.model&lt;-randomForest(credit.rating ~ ., data=train, ntree=50, # 나무 50개 사용 mtry=sqrt(20), # 사용할 변수의 개수 (classification이므로 sqrt(20)개) importance=TRUE) # 변수중요도를 결과를 확인 ) ## ## Call: ## randomForest(formula = credit.rating ~ ., data = train, ntree = 50, mtry = sqrt(20), importance = TRUE) ## Type of random forest: classification ## Number of trees: 50 ## No. of variables tried at each split: 4 ## ## OOB estimate of error rate: 25.43% ## Confusion matrix: ## 0 1 class.error ## 0 88 117 0.5707317 ## 1 61 434 0.1232323 랜덤포레스트 분석 결과에서 “OOB estimate of error rate”의 값은 에러 추정치로서 값이 낮을수록 분류모델의 성능이 좋다고 판단할 수 있다. Confusion matrix의 결과에서 class.error값으로 분류 에러를 통해 모델 성능을 확인할 수 있다. names(rf.model) ## [1] &quot;call&quot; &quot;type&quot; &quot;predicted&quot; &quot;err.rate&quot; ## [5] &quot;confusion&quot; &quot;votes&quot; &quot;oob.times&quot; &quot;classes&quot; ## [9] &quot;importance&quot; &quot;importanceSD&quot; &quot;localImportance&quot; &quot;proximity&quot; ## [13] &quot;ntree&quot; &quot;mtry&quot; &quot;forest&quot; &quot;y&quot; ## [17] &quot;test&quot; &quot;inbag&quot; &quot;terms&quot; names 함수를 통해 randomForest 함수로 생성된 결과들에 어떤 것들이 있는지 확인이 가능하다. 주로 사용하는 인자들에 대한 설명은 아래와 같다. predicted: Out-of-bag samples에 기초한 예측값을 확인할 수 있다. err.rate: 입력데이터 각각에 대한 예측 오류율을 확인할 수 있다. importance: 변수 중요도를 나타내며 Gini값을 기준으로 한다. MeanDecreaseAccuracy와 MeanDecreaseGini 모두 값이 클수록 중요도가 높다고 해석할 수 있다. varImpPlot(rf.model) varImpPlot 함수로 importance 인자 결과를 시각화할 수 있다. 변수의 상대적 중요도를 Mean DecreaseGini를 기준으로 봤을 때, credit.amout, age, account.balance 순서로 변수 중요도가 크다는 것을 파악할 수 있다. library(caret) pred.rf&lt;-predict(rf.model, test[,-1], type=&quot;class&quot;) confusionMatrix(data=pred.rf, reference=test[,1], positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 44 21 ## 1 51 184 ## ## Accuracy : 0.76 ## 95% CI : (0.7076, 0.8072) ## No Information Rate : 0.6833 ## P-Value [Acc &gt; NIR] : 0.0021620 ## ## Kappa : 0.3941 ## ## Mcnemar&#39;s Test P-Value : 0.0006316 ## ## Sensitivity : 0.8976 ## Specificity : 0.4632 ## Pos Pred Value : 0.7830 ## Neg Pred Value : 0.6769 ## Prevalence : 0.6833 ## Detection Rate : 0.6133 ## Detection Prevalence : 0.7833 ## Balanced Accuracy : 0.6804 ## ## &#39;Positive&#39; Class : 1 ## 정분류율은 0.76이며, 민감도는 0.8976으로 높게 나타났다. 또 특이도는 0.4632이다. 정확도가 높다고 해서 무조건 좋은 모형은 아니며, 분석분야에 따라 다양한 지표들을 활용하여 분석 모형을 선택할 수 있다. library(ROCR) pred.rf.roc&lt;-prediction(as.numeric(pred.rf), as.numeric(test[,1])) plot(performance(pred.rf.roc,&quot;tpr&quot;,&quot;fpr&quot;)) abline(a=0,b=1,lty=2,col=&quot;black&quot;) performance(pred.rf.roc, &quot;auc&quot;)@y.values[[1]] ## [1] 0.6803594 prediction 함수와 performance 함수로 값을 구하여 plot 함수로 ROC 커브를 그렸으며, AUC값은 @y.values값으로 확인한 결과 0.6804로 나타났다. Q. 앞서 분리한 iris 데이터의 Species를 분류하는 랜덤포레스트분석을 실시하고 오분류표를 만들어 보자. library(randomForest) (rf.model2&lt;-randomForest(Species ~ ., data=train.iris, ntree=50, mtry=sqrt(4), importance=TRUE)) ## ## Call: ## randomForest(formula = Species ~ ., data = train.iris, ntree = 50, mtry = sqrt(4), importance = TRUE) ## Type of random forest: classification ## Number of trees: 50 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 6.67% ## Confusion matrix: ## setosa versicolor virginica class.error ## setosa 35 0 0 0.0000000 ## versicolor 0 37 3 0.0750000 ## virginica 0 4 26 0.1333333 pred.rf2&lt;-predict(rf.model2, test.iris[,-5], type=&quot;class&quot;) confusionMatrix(data=pred.rf2, reference=test.iris[,5], positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 10 1 ## virginica 0 0 19 ## ## Overall Statistics ## ## Accuracy : 0.9778 ## 95% CI : (0.8823, 0.9994) ## No Information Rate : 0.4444 ## P-Value [Acc &gt; NIR] : 8.12e-15 ## ## Kappa : 0.9656 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 1.0000 0.9500 ## Specificity 1.0000 0.9714 1.0000 ## Pos Pred Value 1.0000 0.9091 1.0000 ## Neg Pred Value 1.0000 1.0000 0.9615 ## Prevalence 0.3333 0.2222 0.4444 ## Detection Rate 0.3333 0.2222 0.4222 ## Detection Prevalence 0.3333 0.2444 0.4222 ## Balanced Accuracy 1.0000 0.9857 0.9750 4.2.4 SVM (Support Vector Machine) 서포트 벡터 머신은 기계학습 분야 중 하나로 패턴인식, 자료 분석 등을 위한 지도학습 모델이며 주로 회귀와 분류 문제 해결에 사용된다. 서포트 벡터 머신 알고리즘은 주어진 데이터 집합을 바탕으로 하여 새로운 데이터가 어떤 범주에 속할 것인지를 판단하는 비확률적 이진 선형 분류 모델을 생성한다. 4.2.4.1 작동 원리 데이터의 각 그룹을 구분하는 분류자를 결정 초평면, 각 그룹에 속한 데이터들 중에서도 초평면에 가장 가까이에 붙어 있는 최정방 데이터들을 서포트 벡터, 서포트 벡터와 초평면 사이의 수직거리를 마진이라고 한다. SVM은 고차원 혹은 무한 차원의 공간에서 마진을 최대화하는 초평면 (MMH, Maximum Margin Hyperplane: 최대마진 초평면) 을 찾아 분류와 회귀를 수행한다. SVM 모형은 선형 분류뿐만 아니라 비선형 분류에서도 사용되는데, 비선형 분류에서는 입력자료를 다차원 공간상으로 매핑할 때 커널 트릭을 사용하기도 한다. 4.2.4.2 R을 이용한 SVM 분석 [함수사용법] svm(formula, data, kernel, gamma, cost, ...) 인자 설명 formula 수식(종속변수 ~ 독립변수) data 분석하고자 하는 데이터 kernel 훈련과 예측에 사용되는 커널“radial”,“linear”,“polynomial”,“sigmoid”가 있음.실제 문제에서 커널의 선택이 결과의 정확도에 큰 영향을 주지 않음. gamma 초평면의 기울기, default=1/(데이터차원) cost 과적합을 막는 정도, default=1 tune.svm(formula, data, kernel, gamma, cost, ...) 인자 설명 formula 수식(종속변수 ~ 독립변수) data 분석하고자 하는 데이터 gamma 초평면의 기울기 cost 과적합을 막는 정도 Q. 앞서 분할한 credit 데이터의 train 데이터를 이용하여 tune.svm 함수로 최적의 파라미터를 찾고 SVM 모델을 만들어 보자. install.packages(setdiff(&quot;e1071&quot;, rownames(installed.packages()))) library(e1071) tune.svm(credit.rating ~ ., data=credit, gamma = 10^(-6:-1), cost = 10^(1:2)) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## gamma cost ## 0.01 10 ## ## - best performance: 0.229 tune.svm 함수에서 gamma와 cost의 주어진 범위 내에서 최적값을 찾아준다. 여기서는 gamma 6개, cost 2개, 즉 6 * 12개의 조합에서 모수조율이 이루어진다. 분석결과에서 best parameters를 통해 gamma는 0.01, cost는 10이 최적의 파라미터임을 확인할 수 있다. svm.model&lt;-svm(credit.rating~., data=train, kernel=&quot;radial&quot;, gamma=0.01, cost=10) summary(svm.model) ## ## Call: ## svm(formula = credit.rating ~ ., data = train, kernel = &quot;radial&quot;, ## gamma = 0.01, cost = 10) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: radial ## cost: 10 ## ## Number of Support Vectors: 389 ## ## ( 212 177 ) ## ## ## Number of Classes: 2 ## ## Levels: ## 0 1 svm 함수에서 gamma와 cost를 설정하고, kernel을 “radial”으로 지정한다. kernel은 radial (가우시안 RBF)이 default로 되어 있다. summary 함수로 svm 모델의 cost값과 Support Vectors의 수(train 데이터 수)를 확인할 수 있다. # 예측을 통한 정분류를 확인 install.packages(setdiff(&quot;caret&quot;, rownames(installed.packages()))) library(caret) pred.svm&lt;-predict(svm.model, test, type=&quot;class&quot;) confusionMatrix(data=pred.svm, reference=test[,1], positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 52 31 ## 1 43 174 ## ## Accuracy : 0.7533 ## 95% CI : (0.7005, 0.8011) ## No Information Rate : 0.6833 ## P-Value [Acc &gt; NIR] : 0.004761 ## ## Kappa : 0.41 ## ## Mcnemar&#39;s Test P-Value : 0.200994 ## ## Sensitivity : 0.8488 ## Specificity : 0.5474 ## Pos Pred Value : 0.8018 ## Neg Pred Value : 0.6265 ## Prevalence : 0.6833 ## Detection Rate : 0.5800 ## Detection Prevalence : 0.7233 ## Balanced Accuracy : 0.6981 ## ## &#39;Positive&#39; Class : 1 ## 정분류율을 0.7533이며, 민감도는 0.8488로 높게 나타났다. 또, 특이도는 0.5474이다. 정확도가 높다고 해서 무조건 좋은 모형은 아니며, 분석 분야에 따라 다양한 지표들을 활용하여 분석모형을 선택할 수 있다. install.packages(setdiff(&quot;ROCR&quot;, rownames(installed.packages()))) library(ROCR) pred.svm.roc&lt;-prediction(as.numeric(pred.svm), as.numeric(test[,1])) plot(performance(pred.svm.roc, &quot;tpr&quot;, &quot;fpr&quot;)) abline(a=0, b=1, lty=2, col=&quot;black&quot;) performance(pred.svm.roc, &quot;auc&quot;)@y.values ## [[1]] ## [1] 0.6980745 prediction 함수와 performance 함수로 값을 구하여 plot함수로 ROC 커브를 그렸으며, AUC값은 @y.values 값으로 확인한 결과 0.6981로 나타났다. Q. 앞서 분리한 iris 데이터의 Species를 분류하는 SVM 분석을 실시하고 오분류표를 만들어 보자. install.packages(setdiff(&quot;e1071&quot;, rownames(installed.packages()))) library(e1071) tune.svm(Species ~ ., data=iris, gamma=2^(-1:1), cost=2^(2:4)) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## gamma cost ## 0.5 16 ## ## - best performance: 0.04 svm.model2&lt;-svm(Species~., data=train.iris, kernel=&quot;radial&quot;, gamma=0.5, cost=16) pred.svm2&lt;-predict(svm.model2, test.iris, type=&quot;class&quot;) confusionMatrix(data=pred.svm2, reference=test.iris[,5], positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 9 1 ## virginica 0 1 19 ## ## Overall Statistics ## ## Accuracy : 0.9556 ## 95% CI : (0.8485, 0.9946) ## No Information Rate : 0.4444 ## P-Value [Acc &gt; NIR] : 2.275e-13 ## ## Kappa : 0.9308 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 0.9000 0.9500 ## Specificity 1.0000 0.9714 0.9600 ## Pos Pred Value 1.0000 0.9000 0.9500 ## Neg Pred Value 1.0000 0.9714 0.9600 ## Prevalence 0.3333 0.2222 0.4444 ## Detection Rate 0.3333 0.2000 0.4222 ## Detection Prevalence 0.3333 0.2222 0.4444 ## Balanced Accuracy 1.0000 0.9357 0.9550 4.2.5 나이브 베이즈 분류 나이브 베이즈 분류는 데이터에서 변수들에 대한 조건부 독립을 가정하는 알고리즘으로 클래스에 대한 사전 정보와 데이터로부터 추출된 정보를 결합하고, 베이즈 정리를 이용하여 특정 데이터가 어떤 클래스에 속하는지를 분류하는 알고리즘이다. 텍스트 분류에서 문서를 여러 범주중 하나로 판단하는 문제에 대한 솔루션으로 사용될 수 있다. 4.2.5.1 Bayes theorem 나이브 베이즈 알고리즘의 기본이 되는 개념으로, 두 확률 변수의 사전 확률과 사후 확률 사이의 관계를 나타내는 정리이다. 사건 A와 B가 있을 때, 사건 B가 일어난 것을 전제로 한 사건 A의 조건부 확률을 구하고자 한다. 하지만 현재 가지고 있는 정보는 사건 A가 일어난 것을 전제로 한 사건 B의 조건부 확률, A의 확률, B의 확률뿐이다. 이때, 원래 구하고자 했던 ’사건 B가 일어난 것을 전제로 한 사건 A의 조건부 확률’을 다음과 같이 구할 수 있다는 것이 베이즈 정리이다. \\(P(A|B) = \\frac{P(B \\cap A)}{P(B)} = \\frac{P(A)P(B|A)}{P(B)}=\\frac{P(A)P(B|A)}{P(A)P(B|A)+P(A^{C}P(B|A^{C}))}\\) \\(P(A|B)\\) : 사건 B가 발생했을 때 사건 A가 발생할 확률 -&gt; 사후확률 (posterior) \\(P(B|A)\\) : 사건 A가 발생했을 때 사건 B가 발생할 확률 -&gt; 우도 (likelihood) \\(P(A \\cap B)\\) : 사건 A와 B가 동시에 발생할 확률 \\(P(A)\\) : 사건 A가 발생할 확률 -&gt; 사전확률 (prior) \\(P(B)\\) : 사건 B가 발생할 화률 -&gt; 관찰값 (evidence) 위 식을 다음과 같은 식으로도 표현이 가능하다. \\(posterior = \\frac{prior\\times likelihood}{evidence}\\) 4.2.5.2 나이브 베이즈 분류 나이브 베이즈 분류는 하나의 속성값을 기준으로 다른 속성이 독립적이라 전제했을 때 해당 속성 값이 클래스 분류에 미치는 영향을 측정한다. 속성값에 대해 다른 속성이 독립적이라는 가정은클래스 조건 독립성이라 한다. 4.2.5.3 R을 이용한 나이브 베이즈 분류 분석 함수사용법 naiveBayes(formula, data, laplace=0, ...) Q. 앞서 분할한 credit 데이터의 train 데이터로 나이브 베이즈 분류 모델을 만들어 보자. library(e1071) nb.model&lt;-naiveBayes(credit.rating~., data=train, laplace=0) nb.model ## ## Naive Bayes Classifier for Discrete Predictors ## ## Call: ## naiveBayes.default(x = X, y = Y, laplace = laplace) ## ## A-priori probabilities: ## Y ## 0 1 ## 0.2928571 0.7071429 ## ## Conditional probabilities: ## account.balance ## Y [,1] [,2] ## 0 1.746341 0.7436323 ## 1 2.381818 0.7983236 ## ## credit.duration.months ## Y [,1] [,2] ## 0 24.37073 13.31920 ## 1 18.99192 11.00404 ## ## previous.credit.payment.status ## Y [,1] [,2] ## 0 2.126829 0.6211250 ## 1 2.375758 0.5728491 ## ## credit.purpose ## Y [,1] [,2] ## 0 3.102439 0.8768504 ## 1 2.868687 0.9841504 ## ## credit.amount ## Y [,1] [,2] ## 0 3765.078 3338.421 ## 1 2974.749 2453.003 ## ## savings ## Y [,1] [,2] ## 0 1.497561 0.9631667 ## 1 2.008081 1.2288434 ## ## employment.duration ## Y [,1] [,2] ## 0 2.263415 1.088657 ## 1 2.529293 1.079178 ## ## installment.rate ## Y [,1] [,2] ## 0 3.112195 1.067385 ## 1 2.937374 1.130242 ## ## marital.status ## Y [,1] [,2] ## 0 2.224390 1.097408 ## 1 2.385859 1.046781 ## ## guarantor ## Y [,1] [,2] ## 0 1.082927 0.2764467 ## 1 1.107071 0.3095159 ## ## residence.duration ## Y [,1] [,2] ## 0 2.814634 1.077752 ## 1 2.852525 1.105960 ## ## current.assets ## Y [,1] [,2] ## 0 2.580488 1.043002 ## 1 2.258586 1.040575 ## ## age ## Y [,1] [,2] ## 0 33.16098 11.01730 ## 1 36.25051 11.45939 ## ## other.credits ## Y [,1] [,2] ## 0 1.760976 0.4275317 ## 1 1.848485 0.3589130 ## ## apartment.type ## Y [,1] [,2] ## 0 1.863415 0.5948126 ## 1 1.929293 0.4856759 ## ## bank.credits ## Y [,1] [,2] ## 0 1.326829 0.4702025 ## 1 1.361616 0.4809545 ## ## occupation ## Y [,1] [,2] ## 0 2.931707 0.6456639 ## 1 2.872727 0.6378446 ## ## dependents ## Y [,1] [,2] ## 0 1.141463 0.3493521 ## 1 1.155556 0.3628001 ## ## telephone ## Y [,1] [,2] ## 0 1.346341 0.4769683 ## 1 1.414141 0.4930714 ## ## foreign.worker ## Y [,1] [,2] ## 0 1.009756 0.09853057 ## 1 1.046465 0.21070209 분석 결과에서 A-priori probabilities는 사전확률을 나타내고 있으며, Conditional probabilities로 각 변수에 대해 조건부 확률을 표로 제공하고 있다. 수치형 변수의 경우 평균, 표준편차를 제공한다. # 예측을 통한 정분류율 확인 library(caret) pred.nb&lt;-predict(nb.model, test[, -1], type=&quot;class&quot;) confusionMatrix(data=pred.nb, reference=test[,1], positive=&#39;1&#39;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 65 53 ## 1 30 152 ## ## Accuracy : 0.7233 ## 95% CI : (0.669, 0.7732) ## No Information Rate : 0.6833 ## P-Value [Acc &gt; NIR] : 0.07551 ## ## Kappa : 0.3997 ## ## Mcnemar&#39;s Test P-Value : 0.01574 ## ## Sensitivity : 0.7415 ## Specificity : 0.6842 ## Pos Pred Value : 0.8352 ## Neg Pred Value : 0.5508 ## Prevalence : 0.6833 ## Detection Rate : 0.5067 ## Detection Prevalence : 0.6067 ## Balanced Accuracy : 0.7128 ## ## &#39;Positive&#39; Class : 1 ## 정분류율(Accuracy)은 0.7233이며, 민감도(Sensitivity)는 0.7415로 높게 나타났다. 또, 특이도 (Specificity)는 0.6842이다. 정확도가 높다고 해서 무조건 좋은 모형은 아니며, 분석 분야에 따라 다양한 지표들을 활용하여 분석 모형을 선택할 수 있다. install.packages(setdiff(&quot;ROCR&quot;, rownames(installed.packages()))) library(ROCR) pred.nb.roc&lt;-prediction(as.numeric(pred.nb), as.numeric(test[,1])) plot(performance(pred.nb.roc, &quot;tpr&quot;, &quot;fpr&quot;)) abline(a=0, b=1, lty=2, col=&quot;black&quot;) performance(pred.nb.roc, &quot;auc&quot;)@y.values ## [[1]] ## [1] 0.712837 prediction 함수와 performance 함수로 값을 구하여 plot 함수로 ROC 커브를 그렸으며, AUC값은 @y.values값으로 확인한 결과 0.7128로 나타났다. 4.2.6 K-NN (K-Nearest Neighbor) K-NN은 어떤 범주로 나누어져 있는 데이터셋이 있을 때, 새로운 데이터가 추가된다면 이를 어떤 범주로 분류할 것인지를 결정할 때 사용할 수 있는 분류 알고리즘으로 지도학습 (Supervised Learning)의 한 종류이다. 4.2.6.1 K-NN 알고리즘의 원리 K-NN 알고리즘에서는 새로운 데이터의 클래스를 해당 데이터와 가장 가까운 K개 데이터들의 클래스(범주)로 결정한다. K-NN 알고리즘에서는 최근접 이웃 간의 거리를 계산할 때 유클리디안 거리, 맨하탄 거리, 민코우스키 거리 등을 사용할 수 있으며, 대표적으로 유클리디안 거리를 사용한다. 4.2.6.2 K의 선택 K의 선택은 학습의 난이도와 데이터의 개수에 따라 결정될 수 있으며, 일반적으로는 훈련 데이터 개수의 제곱근으로 설정한다. 그리고 k를 짝수로 했을 때, 인접객체의 범주가 동률일 경우가 나오므로 반드시 홀수의 값으로 k를 선택하는 것이 중요하다. K를 너무 크게 설정할 경우 주변에 있는 데이터와 근접성이 떨어져 클러스터링이 잘 이루어지지 않고, 너무 작게 설정할 경우 이상치 혹은 잡음 데이터와 이웃이 될 가능성이 있으므로 적절한 k를 선택하는 것이 중요하다. 4.2.6.3 R을 이용한 K-NN 분석 knn 분석 이전에 훈련, 데스트 데이터의 종속변수를 제외한 뒤에 분석을 실시한다. 그리고 거리를 이용한 분석이므로 데이터의 형태가 범주형 변수가 아닌 수치형으로 변환되어야 한다. 함수사용법 knn(train, test, cl, k, ...) Q. 앞서 분할한 credit 데이터의 train 데이터로 K-NN 모델을 만들어 보자. library(class) train.data&lt;-train[,-1] head(train.data) ## account.balance credit.duration.months previous.credit.payment.status ## 415 3 12 3 ## 463 3 15 2 ## 179 3 18 2 ## 526 3 36 3 ## 195 3 12 2 ## 938 2 18 3 ## credit.purpose credit.amount savings employment.duration installment.rate ## 415 3 522 3 4 4 ## 463 1 3812 2 1 1 ## 179 4 1950 1 3 4 ## 526 3 9566 1 2 2 ## 195 3 1262 1 2 3 ## 938 4 884 1 4 4 ## marital.status guarantor residence.duration current.assets age ## 415 3 1 4 2 42 ## 463 1 1 4 3 23 ## 179 3 1 1 3 34 ## 526 1 1 2 3 31 ## 195 3 1 2 3 25 ## 938 3 1 4 3 36 ## other.credits apartment.type bank.credits occupation dependents telephone ## 415 2 2 2 3 2 2 ## 463 2 2 1 3 1 2 ## 179 1 2 2 3 1 2 ## 526 1 2 2 3 1 1 ## 195 2 2 1 3 1 1 ## 938 2 2 1 3 2 2 ## foreign.worker ## 415 1 ## 463 1 ## 179 1 ## 526 1 ## 195 1 ## 938 1 test.data&lt;-test[,-1] head(test.data) ## account.balance credit.duration.months previous.credit.payment.status ## 1 1 18 3 ## 3 2 12 2 ## 4 1 12 3 ## 7 1 8 3 ## 9 3 18 3 ## 12 1 30 3 ## credit.purpose credit.amount savings employment.duration installment.rate ## 1 2 1049 1 1 4 ## 3 4 841 2 3 2 ## 4 4 2122 1 2 3 ## 7 4 3398 1 3 1 ## 9 3 1098 1 1 4 ## 12 1 6187 2 3 1 ## marital.status guarantor residence.duration current.assets age other.credits ## 1 1 1 4 2 21 2 ## 3 1 1 4 1 23 2 ## 4 3 1 2 1 39 2 ## 7 3 1 4 1 39 2 ## 9 1 1 4 3 65 2 ## 12 4 1 4 3 24 2 ## apartment.type bank.credits occupation dependents telephone foreign.worker ## 1 1 1 3 1 1 1 ## 3 1 1 2 1 1 1 ## 4 1 2 2 2 1 2 ## 7 2 2 2 1 1 2 ## 9 2 2 1 1 1 1 ## 12 1 2 3 1 1 1 class&lt;-train[,1] head(class) ## [1] 1 1 1 1 1 0 ## Levels: 0 1 knn.3&lt;-knn(train.data, test.data, class, k=3) knn.7&lt;-knn(train.data, test.data, class, k=7) knn.10&lt;-knn(train.data, test.data, class, k=10) # 각각의 k에 대해 분류 table 작성과 분류 정확도 확인 (t.1&lt;-table(knn.3, test$credit.rating)) ## ## knn.3 0 1 ## 0 27 51 ## 1 68 154 (t.1[1,1]+t.1[2,2])/sum(t.1) ## [1] 0.6033333 (t.2&lt;-table(knn.7, test$credit.rating)) ## ## knn.7 0 1 ## 0 18 26 ## 1 77 179 (t.2[1,1]+t.2[2,2])/sum(t.2) ## [1] 0.6566667 (t.3&lt;-table(knn.10, test$credit.rating)) ## ## knn.10 0 1 ## 0 12 17 ## 1 83 188 (t.3[1,1]+t.3[2,2])/sum(t.3) ## [1] 0.6666667 분석 이전에 종속변수(credit.rating)을 제외한 데이터를 train.data와 test.data에 저장하고 class에 훈련 데이터의 종속변수를 저장한다 .그리고 k가 3, 7, 10일 때 각각 모델을 knn 함수를 사용하여 만든다. 분석 결과를 확인하기 위해서 각각의 k에 대해 분류 table과 정분류율을 계산하여 가장 정분류율이 높은 모델을 찾는다. 위의 결과에서 k를 10으로 했을 때 정분류율이 67%로 가장 높게 나타났다. result&lt;-numeric() k=3:22 for (i in k) { pred&lt;-knn(train.data, test.data, class, k=i-2) t&lt;-table(pred, test$credit.rating) result[i-2]&lt;-(t[1,1] + t[2,2])/sum(t) } result ## [1] 0.5966667 0.5666667 0.6033333 0.6000000 0.6300000 0.6500000 0.6566667 ## [8] 0.6633333 0.6766667 0.6500000 0.6700000 0.6700000 0.6800000 0.6766667 ## [15] 0.6733333 0.6833333 0.6900000 0.6766667 0.6900000 0.6866667 sort(result, decreasing=TRUE) ## [1] 0.6900000 0.6900000 0.6866667 0.6833333 0.6800000 0.6766667 0.6766667 ## [8] 0.6766667 0.6733333 0.6700000 0.6700000 0.6633333 0.6566667 0.6500000 ## [15] 0.6500000 0.6300000 0.6033333 0.6000000 0.5966667 0.5666667 which(result==max(result)) ## [1] 17 19 K-NN에서 최적의 K를 선정하는 것이 중요하다. 그렇기 때문에 최적의 k의 값을 선정해야 하며, 여기에서는 정분류율이 가장 높은 k가 최적의 k값이라고 선정하여 함수를 구현했다. 위의 결과에서 k가 17, 19일 때 분류 정확도가 가장 좋다고 나타나며, 정분류율은 69%이다. 4.2.7 인공신경망 모형 (Artificial Neural Network) 인공신경망 모형은 동물의 뇌신경계를 모방하여 분류 또는 예측하기 위해 만들어진 모형이다. 신경망에서는 입력은 인간의 뇌의 시냅스에 해당하며 개별 신호의 강도에 따라 가중되며, 활성 함수는 인공신경망의 출력을 계산한다. 인공신경망은 가중치를 반복적으로 조정하여 학습하며 뉴런들은 링크로 연결되어 있고, 각 링크에는 수치적인 가중치가 있다. 인공신경망은 신경망의 가중치를 초기화하고 훈련 데이터를 통해 가중치를 갱신하여 신경망의 구조를 선택하고, 활용할 학습 알고리즘을 결정한 후 신경망을 훈련 시킨다. 4.2.7.1 특징 4.2.7.1.1 구조 입력 링크에서 여러 신호를 받아서 새로운 활성화 수준을 계산하고, 출력 링크로 출력 신호를 보낸다. 입력신호는 미가공 데이터 또는 다른 뉴런으로부터의 출력이며, 출력신호는 문제의 최종해 (Solution)이 되거나 다른 뉴런의 입력이 될 수 있다. 4.2.7.1.2 뉴런의 계산 뉴런은 전이함수, 즉 활성화 함수 (activation function)를 사용하며, 활성화 함수를 이용해 출력을 결정하며 입력신호의 가중치 합을 계산하여 임계값과 비교한다. 가중치 합이 임계값보다 작으면 뉴런의 출력은 -1 혹은 0, 같거나 크면 +1 혹은 x의 값을 출력한다. 4.2.7.1.3 뉴런의 활성화 함수 시그모이드 함수 softmax 함수 Relu 함수 4.2.7.1.4 단일 뉴런의 학습 (단층 퍼셉트론) 퍼셉트론은 선형 결합기와 하드 리미터로 구성되며, 초평면은 n차원 공간을 두 개의 영역으로 나눈다. 초평면을 선형 분리함수로 정의한다. \\[\\sum_{i=1}^n x_iw_i-\\theta=0\\] 4.2.7.2 R을 이용한 인공신경망 분석 R에서 인공신경망 분석을 수행할 수 있는 패키지는 nnet와 neuralnet이 있으며 각각 nnet 함수와 neuralnet 함수를 제공한다. 4.2.7.2.1 nnet nnet 패키지는 전통적인 역전파를 가지고 feed-forward 신경망을 훈련하는 알고리즘을 제공한다. 그리고 신경망의 매개변수는 엔트로피와 SSE로 최적화되며, 출력결과를 softmax 함수를 사용해 확률 형태로 변환이 가능하고 과적합을 막기 위해 가중치 감소를 제공한다. nnet 함수는 size, maxit, decay 인자 외에도 가중치를 설정하는 weights, 초기 가중치 값을 설정하는 wts 등의 인자가 있다. nnet 함수로 생성된 모델의 변수 중요도를 파악하기 위해서는 NeuralNetTools 패키지의 garson 함수를 사용하여 확인한다. 함수사용법 nnet(formula, data, size, maxit, decay=5e-04 ...) 인자 설명 formula 수식(종속변수 ~ 독립변수) data 분석하고자 하는 데이터 size hidden node의 개수 maxit 학습 반복횟수, 반복 중 가장 좋은 모델을 선정함. decay 가중치 감소의 모수, 보통 5e-04 채택 함수사용법 garson(mod_in) 인자 설명 mod_in 생성된 인공신경망 모델 Q. 앞서 분할한 credit 데이터의 train 데이터로 nnet 함수를 활용한 인공신경망 모델을 만들어 보자. library(nnet) set.seed(1231) nn.model&lt;-nnet(credit.rating ~ ., data=train, size=2, maxit=200, decay=5e-04) ## # weights: 45 ## initial value 431.435559 ## iter 10 value 423.284097 ## iter 20 value 423.069450 ## iter 30 value 412.275788 ## iter 40 value 379.171871 ## iter 50 value 343.935787 ## iter 60 value 338.439394 ## iter 70 value 334.864491 ## iter 80 value 331.713287 ## iter 90 value 331.330445 ## iter 100 value 330.202215 ## iter 110 value 328.168809 ## iter 120 value 320.901862 ## iter 130 value 316.684662 ## iter 140 value 313.512983 ## iter 150 value 312.780486 ## iter 160 value 312.686319 ## iter 170 value 312.614992 ## iter 180 value 312.507704 ## iter 190 value 312.286285 ## iter 200 value 312.204529 ## final value 312.204529 ## stopped after 200 iterations summary(nn.model) ## a 20-2-1 network with 45 weights ## options were - entropy fitting decay=5e-04 ## b-&gt;h1 i1-&gt;h1 i2-&gt;h1 i3-&gt;h1 i4-&gt;h1 i5-&gt;h1 i6-&gt;h1 i7-&gt;h1 i8-&gt;h1 i9-&gt;h1 ## -11.03 35.33 -1.77 14.85 -6.62 0.00 26.54 -2.95 -1.53 4.76 ## i10-&gt;h1 i11-&gt;h1 i12-&gt;h1 i13-&gt;h1 i14-&gt;h1 i15-&gt;h1 i16-&gt;h1 i17-&gt;h1 i18-&gt;h1 i19-&gt;h1 ## -6.35 0.83 -23.34 1.32 3.49 8.71 -3.97 -24.77 -9.60 6.63 ## i20-&gt;h1 ## -5.47 ## b-&gt;h2 i1-&gt;h2 i2-&gt;h2 i3-&gt;h2 i4-&gt;h2 i5-&gt;h2 i6-&gt;h2 i7-&gt;h2 i8-&gt;h2 i9-&gt;h2 ## -3.17 10.72 -10.34 7.07 0.76 -0.02 1.81 7.05 -2.46 13.33 ## i10-&gt;h2 i11-&gt;h2 i12-&gt;h2 i13-&gt;h2 i14-&gt;h2 i15-&gt;h2 i16-&gt;h2 i17-&gt;h2 i18-&gt;h2 i19-&gt;h2 ## 1.54 -0.92 -7.43 1.62 1.62 1.12 -6.78 1.57 -11.20 11.62 ## i20-&gt;h2 ## -1.12 ## b-&gt;o h1-&gt;o h2-&gt;o ## -0.71 2.34 3.51 분석 결과를 확인하면 총 45개의 가중치가 주어졌음을 #weights: 45에서 확인할 수 있으며, iteration이 반복될수록 error이 줄어들고 있음을 확인할 수 있다. 그리고 200번째 반복 후에 학습을 멈췄으며, 최종 error값이 312.204529임을 final value를 보고 확인할 수 있다. summary 함수로 분석결과를 확인하면 “a 20-2-1 network with 45 weights”는 입력노드 20개, 은닉노드 2개, 출력노드 1개를 의미하고 가중치는 총 45개임을 알 수 있다. install.packages(setdiff(&quot;devtools&quot;, rownames(installed.packages()))) library(devtools) source_url(&#39;https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r&#39;) X11() plot.nnet(nn.model) - summary의 결과에서 나타난 것처럼 20개의 입력노드, 2개의 은닉노드, 1개의 출력노드, 2개의 상수항을 확인할 수 있다. 그림에서 선의 굵기는 연결선의 가중치에 비례한다. install.packages(setdiff(&quot;NeuralNetTools&quot;, rownames(installed.packages()))) library(NeuralNetTools) X11() garson(nn.model) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
