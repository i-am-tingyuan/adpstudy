<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 비정형 데이터마이닝 | ADPStudy</title>
  <meta name="description" content="5 비정형 데이터마이닝 | ADPStudy" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="5 비정형 데이터마이닝 | ADPStudy" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 비정형 데이터마이닝 | ADPStudy" />
  
  
  

<meta name="author" content="tingyuan" />


<meta name="date" content="2021-10-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="정형-데이터마이닝.html"/>

<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> R기초</a></li>
<li class="chapter" data-level="2" data-path="데이터-전처리.html"><a href="데이터-전처리.html"><i class="fa fa-check"></i><b>2</b> 데이터 전처리</a>
<ul>
<li class="chapter" data-level="2.1" data-path="데이터-전처리.html"><a href="데이터-전처리.html#제어문"><i class="fa fa-check"></i><b>2.1</b> 제어문</a></li>
<li class="chapter" data-level="2.2" data-path="데이터-전처리.html"><a href="데이터-전처리.html#데이터-변환"><i class="fa fa-check"></i><b>2.2</b> 데이터 변환</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="데이터-전처리.html"><a href="데이터-전처리.html#파생변수-생성"><i class="fa fa-check"></i><b>2.2.1</b> 파생변수 생성</a></li>
<li class="chapter" data-level="2.2.2" data-path="데이터-전처리.html"><a href="데이터-전처리.html#변수-축소"><i class="fa fa-check"></i><b>2.2.2</b> 변수 축소</a></li>
<li class="chapter" data-level="2.2.3" data-path="데이터-전처리.html"><a href="데이터-전처리.html#표준화와-정규화"><i class="fa fa-check"></i><b>2.2.3</b> 표준화와 정규화</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="데이터-전처리.html"><a href="데이터-전처리.html#데이터-결합-및-요약"><i class="fa fa-check"></i><b>2.3</b> 데이터 결합 및 요약</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="데이터-전처리.html"><a href="데이터-전처리.html#데이터-결합"><i class="fa fa-check"></i><b>2.3.1</b> 데이터 결합</a></li>
<li class="chapter" data-level="2.3.2" data-path="데이터-전처리.html"><a href="데이터-전처리.html#데이터-요약"><i class="fa fa-check"></i><b>2.3.2</b> 데이터 요약</a></li>
<li class="chapter" data-level="2.3.3" data-path="데이터-전처리.html"><a href="데이터-전처리.html#apply-계열-함수"><i class="fa fa-check"></i><b>2.3.3</b> apply 계열 함수</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="데이터-전처리.html"><a href="데이터-전처리.html#패키지를-활용한-데이터-전처리"><i class="fa fa-check"></i><b>2.4</b> 패키지를 활용한 데이터 전처리</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="데이터-전처리.html"><a href="데이터-전처리.html#plyr"><i class="fa fa-check"></i><b>2.4.1</b> plyr</a></li>
<li class="chapter" data-level="2.4.2" data-path="데이터-전처리.html"><a href="데이터-전처리.html#dplyr"><i class="fa fa-check"></i><b>2.4.2</b> dplyr</a></li>
<li class="chapter" data-level="2.4.3" data-path="데이터-전처리.html"><a href="데이터-전처리.html#reshape2"><i class="fa fa-check"></i><b>2.4.3</b> reshape2</a></li>
<li class="chapter" data-level="2.4.4" data-path="데이터-전처리.html"><a href="데이터-전처리.html#data.table"><i class="fa fa-check"></i><b>2.4.4</b> data.table</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="데이터-전처리.html"><a href="데이터-전처리.html#결측치"><i class="fa fa-check"></i><b>2.5</b> 결측치</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="데이터-전처리.html"><a href="데이터-전처리.html#결측치-인식"><i class="fa fa-check"></i><b>2.5.1</b> 결측치 인식</a></li>
<li class="chapter" data-level="2.5.2" data-path="데이터-전처리.html"><a href="데이터-전처리.html#결측치-처리"><i class="fa fa-check"></i><b>2.5.2</b> 결측치 처리</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="데이터-전처리.html"><a href="데이터-전처리.html#이상치-인식"><i class="fa fa-check"></i><b>2.6</b> 이상치 인식</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="데이터-전처리.html"><a href="데이터-전처리.html#이상치란"><i class="fa fa-check"></i><b>2.6.1</b> 이상치란?</a></li>
<li class="chapter" data-level="2.6.2" data-path="데이터-전처리.html"><a href="데이터-전처리.html#사분위수"><i class="fa fa-check"></i><b>2.6.2</b> 사분위수</a></li>
<li class="chapter" data-level="2.6.3" data-path="데이터-전처리.html"><a href="데이터-전처리.html#boxplot을-활용한-이상치-판별"><i class="fa fa-check"></i><b>2.6.3</b> boxplot을 활용한 이상치 판별</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="데이터-전처리.html"><a href="데이터-전처리.html#날짜-데이터-전처리"><i class="fa fa-check"></i><b>2.7</b> 날짜 데이터 전처리</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="데이터-전처리.html"><a href="데이터-전처리.html#날짜-데이터-다루기"><i class="fa fa-check"></i><b>2.7.1</b> 날짜 데이터 다루기</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="통계분석.html"><a href="통계분석.html"><i class="fa fa-check"></i><b>3</b> 통계분석</a>
<ul>
<li class="chapter" data-level="3.1" data-path="통계분석.html"><a href="통계분석.html#통계-자료의-획득방법"><i class="fa fa-check"></i><b>3.1</b> 통계 자료의 획득방법</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="통계분석.html"><a href="통계분석.html#총조사전수-조사census"><i class="fa fa-check"></i><b>3.1.1</b> 총조사/전수 조사(census)</a></li>
<li class="chapter" data-level="3.1.2" data-path="통계분석.html"><a href="통계분석.html#표본조사"><i class="fa fa-check"></i><b>3.1.2</b> 표본조사</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="통계분석.html"><a href="통계분석.html#t-검정-t-test"><i class="fa fa-check"></i><b>3.2</b> T-검정 (T-Test)</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="통계분석.html"><a href="통계분석.html#일표본-t-검정-one-sample-t-test"><i class="fa fa-check"></i><b>3.2.1</b> 일표본 T-검정 (One Sample T-Test)</a></li>
<li class="chapter" data-level="3.2.2" data-path="통계분석.html"><a href="통계분석.html#대응표본-t-검정-paired-sample-t-test"><i class="fa fa-check"></i><b>3.2.2</b> 대응표본 T-검정 (Paired Sample T-Test)</a></li>
<li class="chapter" data-level="3.2.3" data-path="통계분석.html"><a href="통계분석.html#독립표본-t-검정-independent-sample-t-test"><i class="fa fa-check"></i><b>3.2.3</b> 독립표본 T-검정 (Independent Sample T-Test)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="통계분석.html"><a href="통계분석.html#교차분석"><i class="fa fa-check"></i><b>3.3</b> 교차분석</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="통계분석.html"><a href="통계분석.html#교차분석-개념"><i class="fa fa-check"></i><b>3.3.1</b> 교차분석 개념</a></li>
<li class="chapter" data-level="3.3.2" data-path="통계분석.html"><a href="통계분석.html#적합성-검정"><i class="fa fa-check"></i><b>3.3.2</b> 적합성 검정</a></li>
<li class="chapter" data-level="3.3.3" data-path="통계분석.html"><a href="통계분석.html#독립성-검정"><i class="fa fa-check"></i><b>3.3.3</b> 독립성 검정</a></li>
<li class="chapter" data-level="3.3.4" data-path="통계분석.html"><a href="통계분석.html#동질성-검정"><i class="fa fa-check"></i><b>3.3.4</b> 동질성 검정</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="통계분석.html"><a href="통계분석.html#분산분석-anova"><i class="fa fa-check"></i><b>3.4</b> 분산분석 (ANOVA)</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="통계분석.html"><a href="통계분석.html#분산분석의-개념"><i class="fa fa-check"></i><b>3.4.1</b> 분산분석의 개념</a></li>
<li class="chapter" data-level="3.4.2" data-path="통계분석.html"><a href="통계분석.html#일원배치-분산분석-one-way-anova"><i class="fa fa-check"></i><b>3.4.2</b> 일원배치 분산분석 (One-way ANOVA)</a></li>
<li class="chapter" data-level="3.4.3" data-path="통계분석.html"><a href="통계분석.html#이원배치-분산분석-two-way-anova"><i class="fa fa-check"></i><b>3.4.3</b> 이원배치 분산분석 (Two-way ANOVA)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="통계분석.html"><a href="통계분석.html#상관분석"><i class="fa fa-check"></i><b>3.5</b> 상관분석</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="통계분석.html"><a href="통계분석.html#상관분석-개념"><i class="fa fa-check"></i><b>3.5.1</b> 상관분석 개념</a></li>
<li class="chapter" data-level="3.5.2" data-path="통계분석.html"><a href="통계분석.html#상관분석의-유형"><i class="fa fa-check"></i><b>3.5.2</b> 상관분석의 유형</a></li>
<li class="chapter" data-level="3.5.3" data-path="통계분석.html"><a href="통계분석.html#상관계수-검정"><i class="fa fa-check"></i><b>3.5.3</b> 상관계수 검정</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="통계분석.html"><a href="통계분석.html#회귀분석"><i class="fa fa-check"></i><b>3.6</b> 회귀분석</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="통계분석.html"><a href="통계분석.html#회귀분석의-개념"><i class="fa fa-check"></i><b>3.6.1</b> 회귀분석의 개념</a></li>
<li class="chapter" data-level="3.6.2" data-path="통계분석.html"><a href="통계분석.html#단순선형회귀분석"><i class="fa fa-check"></i><b>3.6.2</b> 단순선형회귀분석</a></li>
<li class="chapter" data-level="3.6.3" data-path="통계분석.html"><a href="통계분석.html#다중선형회귀분석-다변량-회귀분석"><i class="fa fa-check"></i><b>3.6.3</b> 다중선형회귀분석 (다변량 회귀분석)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html"><i class="fa fa-check"></i><b>4</b> 정형 데이터마이닝</a>
<ul>
<li class="chapter" data-level="4.1" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html#데이터-분할과-성과분석"><i class="fa fa-check"></i><b>4.1</b> 데이터 분할과 성과분석</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html#데이터-분할"><i class="fa fa-check"></i><b>4.1.1</b> 데이터 분할</a></li>
<li class="chapter" data-level="4.1.2" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html#성과분석"><i class="fa fa-check"></i><b>4.1.2</b> 성과분석</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html#분류-분석"><i class="fa fa-check"></i><b>4.2</b> 분류 분석</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html#로지스틱-회귀분석"><i class="fa fa-check"></i><b>4.2.1</b> 로지스틱 회귀분석</a></li>
<li class="chapter" data-level="4.2.2" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html#의사결정나무"><i class="fa fa-check"></i><b>4.2.2</b> 의사결정나무</a></li>
<li class="chapter" data-level="4.2.3" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html#앙상블-기법"><i class="fa fa-check"></i><b>4.2.3</b> 앙상블 기법</a></li>
<li class="chapter" data-level="4.2.4" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html#svm-support-vector-machine"><i class="fa fa-check"></i><b>4.2.4</b> SVM (Support Vector Machine)</a></li>
<li class="chapter" data-level="4.2.5" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html#나이브-베이즈-분류"><i class="fa fa-check"></i><b>4.2.5</b> 나이브 베이즈 분류</a></li>
<li class="chapter" data-level="4.2.6" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html#k-nn-k-nearest-neighbor"><i class="fa fa-check"></i><b>4.2.6</b> K-NN (K-Nearest Neighbor)</a></li>
<li class="chapter" data-level="4.2.7" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html#인공신경망-모형-artificial-neural-network"><i class="fa fa-check"></i><b>4.2.7</b> 인공신경망 모형 (Artificial Neural Network)</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html#군집분석"><i class="fa fa-check"></i><b>4.3</b> 군집분석</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html#군집분석-1"><i class="fa fa-check"></i><b>4.3.1</b> 군집분석</a></li>
<li class="chapter" data-level="4.3.2" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html#계층적-군집분석"><i class="fa fa-check"></i><b>4.3.2</b> 계층적 군집분석</a></li>
<li class="chapter" data-level="4.3.3" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html#비계층적-군집분석"><i class="fa fa-check"></i><b>4.3.3</b> 비계층적 군집분석</a></li>
<li class="chapter" data-level="4.3.4" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html#혼합-분포-군집"><i class="fa fa-check"></i><b>4.3.4</b> 혼합 분포 군집</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html#연관분석"><i class="fa fa-check"></i><b>4.4</b> 연관분석</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="정형-데이터마이닝.html"><a href="정형-데이터마이닝.html#연관규칙"><i class="fa fa-check"></i><b>4.4.1</b> 연관규칙</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="비정형-데이터마이닝.html"><a href="비정형-데이터마이닝.html"><i class="fa fa-check"></i><b>5</b> 비정형 데이터마이닝</a>
<ul>
<li class="chapter" data-level="5.1" data-path="비정형-데이터마이닝.html"><a href="비정형-데이터마이닝.html#텍스트마이닝"><i class="fa fa-check"></i><b>5.1</b> 텍스트마이닝</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="비정형-데이터마이닝.html"><a href="비정형-데이터마이닝.html#데이터-전처리-1"><i class="fa fa-check"></i><b>5.1.1</b> 데이터 전처리</a></li>
<li class="chapter" data-level="5.1.2" data-path="비정형-데이터마이닝.html"><a href="비정형-데이터마이닝.html#term-document-matrix"><i class="fa fa-check"></i><b>5.1.2</b> Term-Document Matrix</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">ADPStudy</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="비정형-데이터마이닝" class="section level1" number="5">
<h1><span class="header-section-number">5</span> 비정형 데이터마이닝</h1>
<div id="텍스트마이닝" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> 텍스트마이닝</h2>
<p>텍스트마이닝은 텍스트로부터 고품질의 정보를 도출하는 분석방법으로, 입력된 텍스트를 구조화해 그 데이터에서 패턴을 도출한 후 결과를 평가 및 해석하는 일련의 과정을 의미한다. 주로 구조화된 정형 데이터 속에서 정보나 패턴을 발견하는 데이터마이닝과는 달리 텍스트마이닝은 인터넷 데이터, 소셜 미디어 데이터 등과 같은 자연어로 구성된 비정형 텍스트 데이터 속에서 정보나 관계를 발견하는 분석 기법이다.</p>
<p>단어들 간의 관계를 이용해 감성분석, 워드 클라우드 분석 등을 수행할 후 이 정보를 클러스터링, 분류, 사회연결망 분석 등에 활용한다.</p>
<div id="데이터-전처리-1" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> 데이터 전처리</h3>
<div id="tm-패키지" class="section level4" number="5.1.1.1">
<h4><span class="header-section-number">5.1.1.1</span> tm 패키지</h4>
<ul>
<li>tm 패키지는 문서를 관리하는 기본 구조인 Corpus를 생성하여 tm_map 함수를 통해 데이터들을 전처리 및 가공한다.</li>
<li>Corpus와 VCorpus 중 VCorpus에서 에러가 적게 나타나므로 주로 VCorpus를 활용한다.</li>
</ul>
<div id="corpus" class="section level5" number="5.1.1.1.1">
<h5><span class="header-section-number">5.1.1.1.1</span> Corpus</h5>
<ul>
<li>Corpus는 데이터마이닝의 절차중 데이터의 정제, 통합, 선택, 변환의 과정을 거친 구조화된 단계로 더 이상 추가적인 절차없이 데이터마이닝 알고리즘 실험에 활용될 수 있는 상태이다.</li>
<li>R프로그램의 텍스트마이닝 패키지인 tm에서 문서를 관리하는 기본 구조이며, 텍스트 문서들의 집합을 의미한다.</li>
<li>VCorpus: 문서를 Corpus class로 만들어 주는 함수로, 결과는 메모리에 저장되어 현재 구동중인 R메모리에서만 유지된다.</li>
</ul>
</div>
<div id="tm-패키지를-활용한-corpus-만들기" class="section level5" number="5.1.1.1.2">
<h5><span class="header-section-number">5.1.1.1.2</span> tm 패키지를 활용한 Corpus 만들기</h5>
<ul>
<li>텍스트 마이닝을 수행하기 전에 tm패키지를 활용해 Corpus를 만들고 생성된 Corpus를 전처리하고 분석에 활용하여야 한다.</li>
<li>텍스트 데이터를 문서로 만들기 위해 VectorSource() 함수를 사용하고 문서로 완성된 데이터를 VCorpus()함수를 이용하여 Corpus로 만든다.</li>
</ul>
<p><b>함수 사용법</b></p>
<pre><code>VectorSource(text)</code></pre>
<pre><code>VCorpus(data)</code></pre>
<p><strong>Q. 아래의 내장 데이터로 Corpus를 살펴보자.</strong></p>
<div class="sourceCode" id="cb610"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb610-1"><a href="비정형-데이터마이닝.html#cb610-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="fu">setdiff</span>(<span class="st">&quot;tm&quot;</span>, <span class="fu">rownames</span>(<span class="fu">installed.packages</span>())))</span>
<span id="cb610-2"><a href="비정형-데이터마이닝.html#cb610-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tm)</span></code></pre></div>
<pre><code>## Loading required package: NLP</code></pre>
<pre><code>## 
## Attaching package: &#39;NLP&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     annotate</code></pre>
<pre><code>## 
## Attaching package: &#39;tm&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:arules&#39;:
## 
##     inspect</code></pre>
<div class="sourceCode" id="cb616"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb616-1"><a href="비정형-데이터마이닝.html#cb616-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(crude)</span>
<span id="cb616-2"><a href="비정형-데이터마이닝.html#cb616-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(crude)[<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>,]</span></code></pre></div>
<pre><code>##     Length Class             Mode
## 127 2      PlainTextDocument list
## 144 2      PlainTextDocument list
## 191 2      PlainTextDocument list
## 194 2      PlainTextDocument list
## 211 2      PlainTextDocument list
## 236 2      PlainTextDocument list</code></pre>
<ul>
<li>crude 데이터는 로이터 뉴스 기사 중 원유와 관련된 기사 20개가 저장된 데이터이다. summary 결과에서 class는 TextDocument 형태임을 알 수 있고, list 형태로 저장되어 있다.</li>
</ul>
<div class="sourceCode" id="cb618"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb618-1"><a href="비정형-데이터마이닝.html#cb618-1" aria-hidden="true" tabindex="-1"></a><span class="fu">inspect</span>(crude[<span class="dv">1</span>])</span></code></pre></div>
<pre><code>## &lt;&lt;VCorpus&gt;&gt;
## Metadata:  corpus specific: 0, document level (indexed): 0
## Content:  documents: 1
## 
## $`reut-00001.xml`
## &lt;&lt;PlainTextDocument&gt;&gt;
## Metadata:  15
## Content:  chars: 527</code></pre>
<ul>
<li>inspect 함수로 문서의 정보(파일형태, 글자수 등)를 파악할 수 있으며, 문서의 내용은 $content를 통해 확인이 가능하다.</li>
</ul>
</div>
<div id="tm-패키지를-활용한-데이터-전처리" class="section level5" number="5.1.1.1.3">
<h5><span class="header-section-number">5.1.1.1.3</span> tm 패키지를 활용한 데이터 전처리</h5>
<ul>
<li>tm_map 함수를 활용하여 코퍼스로 변환된 데이터에 텍스트 전처리를 수행할 수 있다.</li>
<li>공백 제거, 문장부호 제거, 숫자 제거, 불용어 제거 등의 전처리를 통해 텍스트마이닝을 수행할 수 있는 형태로 데이터를 전처리 한다.</li>
</ul>
<p><b>함수사용법</b></p>
<pre><code>tm_map(x, FUN, ...)</code></pre>
<table>
<thead>
<tr class="header">
<th>인자</th>
<th>설명</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>x</td>
<td>코퍼스로 변환된 데이터</td>
</tr>
<tr class="even">
<td>FUN</td>
<td>변환에 사용할 함수를 입력</td>
</tr>
</tbody>
</table>
<ul>
<li>tm_map의 function 종류는 아래와 같음</li>
</ul>
<table>
<thead>
<tr class="header">
<th>함수</th>
<th>기능</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>tm_map(x, tolower)</td>
<td>소문자로 만들기</td>
</tr>
<tr class="even">
<td>tm_map(x, stemDocument)</td>
<td>어근만 남기기</td>
</tr>
<tr class="odd">
<td>tm_map(x, stripWhitespace)</td>
<td>공백제거</td>
</tr>
<tr class="even">
<td>tm_map(x, removePunctuation)</td>
<td>문장부호 제거</td>
</tr>
<tr class="odd">
<td>tm_map(x, removeNumbers)</td>
<td>숫자 제거</td>
</tr>
<tr class="even">
<td>tm_map(x, removeWords, “word”)</td>
<td>단어 제거</td>
</tr>
<tr class="odd">
<td>tm_map(x, removeWords, stopWords(“english”))</td>
<td>불용어 제거</td>
</tr>
<tr class="even">
<td>tm_map(x, PlainTextDocument)</td>
<td>TextDocument로 변환</td>
</tr>
</tbody>
</table>
<p><strong>Q. 데이터 분석 전문가라는 키워드로 뉴스 기사를 검색하여 10개의 기사를 수집하였다. 수집 데이터를 Corpus로 만들고, tm_map 함수로 데이터를 전처리해 보자.</strong></p>
<div class="sourceCode" id="cb621"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb621-1"><a href="비정형-데이터마이닝.html#cb621-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="fu">setdiff</span>(<span class="st">&quot;tm&quot;</span>, <span class="fu">rownames</span>(<span class="fu">installed.packages</span>())))</span>
<span id="cb621-2"><a href="비정형-데이터마이닝.html#cb621-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tm)</span>
<span id="cb621-3"><a href="비정형-데이터마이닝.html#cb621-3" aria-hidden="true" tabindex="-1"></a>news<span class="ot">&lt;-</span><span class="fu">readLines</span>(<span class="fu">file</span>(<span class="st">&quot;./data/키워드_뉴스.txt&quot;</span>, <span class="at">encoding=</span><span class="st">&quot;EUC-KR&quot;</span>))</span></code></pre></div>
<details>
<summary>
Click for Result
</summary>
<div class="sourceCode" id="cb622"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb622-1"><a href="비정형-데이터마이닝.html#cb622-1" aria-hidden="true" tabindex="-1"></a>news</span></code></pre></div>
<pre><code>##  [1] &quot;동아대학교(총장 한석정)가 &#39;수요자 데이터기반 스마트헬스케어 서비스&#39;분야 ‘4차 산업혁명 혁신선도대학으로 최종선정됐습니다. 동아대가 혁신선도대학으로 펼치게 될 ‘수요자 데이터기반 스마트헬스케어 서비스’ 산업은 리빙데이터(운동·영양·약물)와 메디컬데이터(생체계측·진료기록)를 종합 분석, 다양한 헬스케어 서비스를 제공하는 것입니다. 동아대는 건강과학대학과 의료원, 재활요양병원 등 경쟁력 있는 인프라를 바탕으로 신뢰도 높은 정밀 분석을 실시, ‘헬스케어 기획 전문가’와 ‘헬스케어 데이터분석 전문가’ 등 수요자 맞춤형 헬스케어 서비스 분야를 선도하는 전문 인재를 키워나갈 계획입니다. ‘스마트헬스케어 융합전공’을 신설, 경영정보학과를 중심으로 한 빅데이터 분석, 식품영양학과·의약생명공학과·건강관리학과 중심의 헬스케어 등 학문 간 경계는 교육혁신도 이뤄나갈 방침입니다. &quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
##  [2] &quot;첨단 정보통신기술에 AI 등이 더해지면서 무기체계가 날로 지능화하는 가운데 &#39;국방개혁 2.0&#39; 추진과 4차 산업기술의 군사분야 접목 속도가 빨라지고 있다. 국방부가 2025년까지 군 내부의 데이터베이스를 기초로 전장의 모든 변수를 모아 최선의 작전계획을 영위하는 ‘AI지휘체계’를 구축한다고 발표한 이래 빅데이터 활용에 대한 관심은 날로 증가하고 있다. 15일 국회의원회관에서 열린 ‘미래국방과 4차산업혁명 교육혁신 포럼’에서 2miles 윤혜식 대표는 ‘빅데이터&amp;블록체인을 통한 스마트 국방’을 주제로 발표하면서 데이터, 클라우드, 인텔리전스를 강조했다. 윤 대표는 “빅데이터 시대의 핵심 기술인 모바일, 인공지능, 로봇, 사물인터넷, 클라우드, 블록체인을 새로운 도구로 활용해 서비스를 혁신하는 국방기술의 디지타이징이 필요하다”며 “국방 서비스의 각 영역들의 통합운영이 필요하고 언제 어디서나 정보에 안전하게 접근하는 모빌리티를 확보해야 한다. 분산되어 있는 정보를 한 곳에 모으고 운영상 발생하는 보안 이슈를 통합 모니터링을 통해 해결하는 클라우드 서비스가 중요하다”고 말한 미 육군 CIO Bruce Crawford 장군의 인터뷰 내용을 인용했다. 그는 미사일 대응체계를 사례로 들며 “적군의 미사일 발사를 감지해 아군에게 정보가 전달되는 과정에서 발생될 수 있는 해커의 방해작업을, 블록체인을 기반으로 한 탈 중앙화 미사일 대응체계를 갖추면 다면화된 내부 블록체인의 보안화로 막을수 있어 효과적으로 상대편 미사일에 대응할 수 있게 된다”고 설명했다. 그러면서 “빅테이터를 활용하게 되면 전략자산에 연결된 자동화된 사전 알람으로 원격 모니터링을 이용해 빠르게 공정설비의 이슈를 파악하고 적합한 엔지니어를 투입할 수 있다”며, “이로써 예방보전과 유지보수 속도가 빨라지고 전투력을 증강시킬 수 있다”고 말했다. 이어 윤 대표는 “가상/증강(VR/AR) 등 첨단기술을 활용한 실감형 과학화 훈련체계를 구축함은 물론 물론 내부망으로 연결된 실시간 화면 송수신으로 부상자의 응급처치도 가능하다”며 “신속하고 스마트한 의사결정으로 전투력을 극대화할 수 있다”고 전했다. 그는 이러한 빅데이터 시대의 성공요인으로 고품질의 데이터, 분석에 대한 전방위의 관점, 분석지향의 리더십, 전략적 타깃, 그리고 데이터 분석 전문가 양성이 무엇보다 중요하다고 덧붙였다. 한편 이 날 포럼을 주최한 백승주 국회 국방위원은 축사를 통해 “미래 대한민국의 국방과 안보를 위해 4차 산업혁명 도입의 필요성을 되짚어보면서 향후 4차 산업혁명 기술을 활용한 국방 및 안보 보안강화 방안을 살펴보고자 한다”며, “미래의 변화에 걸맞은 국방과 안보, 교육과 경제제도가 마련되는 본격적인 계기가 되기 바란다”고 말했다.(konas)&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          
##  [3] &quot;SK하이닉스는 김영한(사진) 미국 샌디에이고 캘리포니아대(UCSD) 종신 교수를 수석 연구위원(전무급)으로 영입했다고 10일 밝혔다. 미 스탠퍼드대에서 통계학 석사, 전기공학 석·박사 학위를 취득한 김 연구위원은 2015년 전자업계 최고 권위의 미국 전기전자공학회(IEEE) 석학회원(Fellow)에도 이름을 올린 세계적인 데이터 과학 전문가다. SK하이닉스는 김 수석 연구위원의 영입에 맞춰 2016년 데이터 분석을 전문적으로 담당하기 위해 설립한 데이터 사이언스 조직 산하에 데이터 리서치 조직을 만들었다. 또 데이터 리서치 하에 ‘MIDAS(Machine Intelligence and Data Analytics Solutions)랩’을 신설했다. MIDAS 랩은 반도체 제조·개발 미세공정뿐 아니라 인사·기업문화 등에까지 인공지능(AI) 기반 시스템을 구축하는 역할을 맡게 된다.  SK하이닉스 관계자는 \&quot;최근 반도체 산업은 제조·개발의 미세공정 난도 증가로 처리해야 하는 데이터의 양이 기하급수적으로 늘어났다\&quot;며 \&quot;이에 엔지니어가 직관적으로 판단하는 것이 아니라 AI를 통해 수율(투입량 대비 정상제품 생산 비율)을 올릴 수 있는 최적의 해결 방법을 찾는 것이 경쟁력의 핵심이 되고 있다\&quot;고 설명했다. SK하이닉스는 AI 경쟁력 확보의 일환으로 현지 시각 10일부터 15일까지 미국 캘리포니아에서 열리는 세계 최고 권위의 AI 학회 국제머신러닝학회(ICML)에도 처음으로 참가해 홍보부스를 열 예정이다. 이를 통해 국내·외 우수 빅데이터, AI 전문가를 적극적으로 유치한다는 계획이다. SK하이닉스 CIO(최고정보관리책임자) 송창록 전무는 \&quot;올해부터 반도체 제조·개발 현장에 엔지니어뿐 아니라 뛰어난 데이터 분석 전문가들을 투입할 계획\&quot;이라면서 \&quot;전문가들이 역량을 발휘할 수 있는AI 기반의 업무 시스템을 확장해나가겠다\&quot;고 말했다. &quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          
##  [4] &quot;삼성SDS(대표 홍원표)는 국내·외 대학(원)생을 대상으로 데이터 분석 경진대회인 ‘브라이틱스 아카데미’ 공모전을 개최한다고 28일 밝혔다. 브라이틱스 아카데미(Brightics Academy)는 데이터 분석 전문가 양성과 빅데이터 분석 플랫폼의 저변 확대를 위해 삼성SDS가 자사 데이터 분석 시스템 플랫폼인 ‘브라이틱스 스튜디오’를 활용하여 강의와 연구를 지원하는 산학 협력 프로그램이다. 이번 공모전은 학생들에게 비전공자라도 이용 가능한 브라이틱스 스튜디오(Brightics Studio)를 통해, 데이터 분석을 해볼 수 있는 기회를 제공하고자 마련됐다. 브라이틱스 스튜디오는 삼성SDS의 기업용 대용량 데이터 분석 플랫폼인 브라이틱스 AI의 오픈소스 버전으로, 지난해 11월 공개 이후 홈페이지 등에서 매주 1,000명 이상 다운로드 받고 있다. 공모 분야는 ▲데이터 사업기획 부문 ▲데이터 분석 부문 ▲연구 혁신 부문 등 3개 부문이며, 심사를 거쳐 각 부문별 대상 500만원(1팀), 최우수상 300만원(2팀), 우수상 200만원(3팀)의 상금이 수여된다. 이 중 대상 수상팀에게는 삼성SDS의 다양한 빅데이터 분석 사례를 경험해 볼 수 있는 4주간의 현장실습 기회가 주어진다. 이은주 삼성SDS 빅데이터분석팀장(상무)은 “이번 브라이틱스 아카데미 공모전을 통해 학생들이 브라이틱스 스튜디오를 활용한 데이터 분석을 경험해 보고, 분석 역량을 향상시키는 좋은 기회가 되기를 바란다”고 밝혔다. 참가 접수는 5월 27일부터 6월 21일까지 브라이틱스 AI 홈페이지에서 진행되며, 최종 과제는 8월 30일까지 제출하면 된다.&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
##  [5] &quot;푸드테크 스타트업들이 빅데이터 기반 시스템 배달 체계로 전환한다. 일선 상점과 배달기사 수익 향상에 초점을 맞춘 게 특징이다. 고객 입장에서도 도착 시간을 알 수 있다. 배달업체 간 경쟁이 &#39;속도&#39;에서 &#39;빅데이터&#39;로 옮겨 붙는 양상이다. 13일 업계에 따르면 배달 스타트업들이 빅데이터 기반 배송 효율화 작업에 속도를 낸다. 우아한형제들(대표 김봉진)이 운영하는 음식 배달 브랜드 배민라이더스는 고객 편의성을 높이기 위해 빅데이터를 도입했다. 배달 음식 도착 예상 시간을 알려준다. 주문 데이터를 축적하며 예측 오차를 줄이고 있다. 배달기사를 위한 추천 배차 시스템도 개발했다. 실시간 데이터 분석 기술이 적용됐다. 배달기사별 맞춤형 배달 주문이 연결되도록 돕는다. 이동 경로도 제시한다. 배달 시간을 줄이는 것은 물론 배달기사 간 경쟁을 완화한다. 우아한형제들은 지난해 기준 전년 대비 데이터서비스 팀 인력을 두 배 늘렸다. 메쉬코리아(대표 유정범)도 가맹점, 배달기사가 상생(윈윈)할 수 있는 방안을 빅데이터에서 찾는다. 빅데이터 분석 전문가인 데이터 사이언티스트를 영입했다. 배달 주문 접수에서 음식물을 운송, 소비자에게 전달하는 단계별 비효율을 없앤다. 회사 관계자는 “낭비되는 시간, 비효율을 최소화하는 데 연구개발(R&amp;D) 초점이 맞춰져 있다”고 설명했다. 최근 첫발을 뗀 사륜차 기반의 배송 사업 붐업도 빅데이터에 맡겼다. 거점 배송 효율을 높여 기존의 이륜차 물류망과 시너지를 극대화한다. 자동주문 추천, 배달 예상 시간 알림 서비스도 빅데이터를 활용해 고도화한다. 소상공인과의 상생·발전 해법으로 빅데이터를 제시한 업체도 있다. 스파이더크래프트(대표 유현철)는 소상공인 대상 경영 컨설팅 서비스를 개발하고 있다. 주문 내용, 고객 나이·성별, 매출 발생 시간·지역, 인기 메뉴 정보 등을 종합적으로 진단해 가게별 맞춤형 성공 전략을 세워 줄 계획이다. 안전한 배달 환경 조성에도 빅데이터를 접목한다. 내년도 예산은 올해보다 30% 넘게 증액할 방침이다. 신규 지점 개소, 메뉴 개발에 필요한 상권 분석 서비스도 추가한다. 바로고(대표 이태권)도 빅데이터와의 접점을 지속 확대한다. 주문 관련 데이터는 물론 가맹점 피드백까지 수집, 바로고만의 차별화한 서비스를 선보일 계획이다. 데이터 확보에 자신감이 충만하다. 업계 최대 규모의 고객사를 보유했기 때문이다. 공유주방 음식 배달도 업계에서 가장 먼저 시작했다. 바로고 관계자는 “빅데이터를 활용해 가맹점 매출 증대 방안을 찾고 있다”면서 “이 같은 시스템이 자리 잡히면 배달 기사별 예상 수익도 뽑아볼 수 있다”며 웃었다. 사륜차 기반 배송 업계의 최대 화두도 빅데이터다. 화물 운송 플랫폼 센디 운영사 벤디츠(대표 선현국·염상준)는 빅데이터로 차별화한 경쟁력을 키운다. 인공지능(AI) 상담 서비스 출시에 이어 카카오택시처럼 화물차를 부를 수 있는 &#39;물류 클라우드&#39;를 구축했다. 화물 차주가 출근해서 퇴근할 때까지 일정도 짜 준다. 물건을 싣지 않고 다니는 공차 시간을 줄이기 위해서다. 센디 기사들은 10시간 운행 시 최소 7시간 동안 짐을 싣고 다닌다. 업계 평균 대비 약 30% 높은 수치다. 투자도 확충한다. 빅데이터 개발자 인력을 올해 두 배로 늘릴 예정으로 있다. 업계 관계자는 “아직은 데이터베이스(DB)에서 일부 항목 값으로 통계를 내는 수준에 가깝다”면서 “배송 데이터가 쌓이면서 지금보다 더 편리하고 효율적인 빅데이터 기반 서비스가 빠르게 확산될 것”이라며 기대감을 내비쳤다.&quot;
##  [6] &quot;코오롱그룹 IT서비스업체 코오롱베니트(대표 이진용)는 정부 &#39;2019 데이터바우처 지원사업’ 참여사로서 머신러닝을 활용한 &#39;데이터 AI 가공 서비스&#39;를 제공한다고 17일 밝혔다. 서비스는 제조현장 시계열 데이터를 분석해 제조 생산성 향상을 돕는 데 쓰일 수 있다. 머신러닝 솔루션 &#39;팔콘리 LRS&#39;를 활용한 생산 및 설비 데이터 패턴 분석을 통해 공정 모니터링, 품질 개선, 설비 고장 예측 등을 수행할 수 있다. 데이터바우처 지원사업은 중소·벤처기업, 소상공인, 스타트업에 데이터 구매와 가공 바우처를 제공하는 사업이다. 지원 대상 기업에 데이터 활용 기회를 높인다는 취지다. 과학기술정보통신부 산하 한국데이터산업진흥원 주관으로 운영된다. 코오롱베니트 측은 팔콘리 LRS를 데이터 분석 전문가 없이 현장 작업자가 직접 쓸 수 있을 정도로 간단해 전문 인력이 부족한 중소기업도 지능형 제조공정을 구축하고 운영할 수 있다고 주장했다. 코오롱베니트 이종찬 본부장은 \&quot;대기업 제조현장에서는 이미 데이터 분석을 통해 품질, 수율, 가동률 등의 생산성 개선 효과를 체감하고 있다\&quot;며 \&quot;도입비용과 전문기술 등 문제로 부담을 느끼는 중소기업들도 데이터를 활용해 제조 경쟁력을 높일 수 있도록 지원하겠다\&quot;고 말했다. 코오롱베니트는 오는 21일까지 한국데이터산업진흥원 데이터스토어에서 신청을 받아 데이터 AI 가공 서비스를 제공한다.&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           
##  [7] &quot;대구광역시가 DGB대구은행과 공동으로 &#39;제1회 대구 빅데이터 분석 경진대회&#39;를 개최한다고 밝혔습니다. 대한민국 국민이면 누구나 자격 제한 없이 개인 또는 3인 이하의 팀을 구성하여 참여할 수 있으며, 분석 주제는 행정, 교통, 관광, 복지?의료, 금융에 대한 5개의 지정과제와 공공, 금융 부문의 자율과제로 선택할 수 있습니다. 지정과제는 ▲행정(시·도 공공데이터 분석에 따른 대구시 공공데이터 개발 및 행정혁신 방안), ▲교통(노선개편 등을 통한 대중교통 서비스 개선 방안), ▲관광(관광객 증대를 위한 관광자원 발굴 및 개선 방안), ▲복지의료(폭염, 미세먼지 등이 노인 건강에 미치는 영향 분석), ▲금융(상권분석을 통한 소상공인(예비창업자 포함) 지원방안)입니다. 대회 진행은 7월 22일부터 8월 2일까지 참가접수를 통해 1차 서류심사에서 20개 팀을 선발하여 10월 30일 본선 대회에서 최종 5개 팀을 선발해 시상합니다. 상금 규모는 총 3,000만원으로 대상 1팀 1,500만원(대구광역시장상), 최우수상 1팀 1,000만원(대구은행장상), 우수상 1팀 300만원(대구디지털산업진흥원장상) 등으로, 5개의 팀에게 시상할 예정입니다. 관계자는 “이번 경진대회는 대구 지역에서 발생하는 공공부문에 대한 현안 해결을 주제로, 참신한 아이디어를 제공 받아 정책에 적극 활용하고자 올해 처음으로 개최하는 것”이라며, “전국의 데이터 분석 전문가들의 참여를 유도하기 위해 상금을 전국 최고 수준으로 정하여 참여도 및 분석 결과물의 양적?질적인 수준을 높이고자 했다”고 덧붙였습니다. 대구디지털산업진흥원 이승협 원장은 “‘제1회 대구 빅데이터 경진 대회’를 통해 전국의 역량 있는 분석가들이 모여 대구 현안 해결을 위한 좋은 아이디어와 해결책을 제시해 줬으면 좋겠다”고 말했습니다. &quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               
##  [8] &quot;광주과학기술원(GIST·총장 김기선)은 김준하 지구·환경공학부 교수가 1일부터 4주간 매주 월요일마다 GIST 오룡관에서 열리는 환경분야 전문가 양성프로그램인 &#39;환경통계 및 데이터 분석 전문가 교육(E-DAP)&#39;에서 재능 기부로 강연한다고 밝혔다. E-DAP은 지난 2014년 7월부터 GIST 국제환경연구소가 전국 대학생과 공무원, 기업인, 연구원 등을 대상으로 실시해온 교육 프로그램이다. 지난해까지 매년 선착순 50여 명을 선발했으며 올해는 75명의 수강생이 참여한다. 김준하 교수는 지난 15여년간 강의하며 집필한 &#39;환경통계 및 데이터 분석(R과 SPSS를 활용한 자기주도 학습서)&#39;을 기반으로 강연한다. 환경공학 관련 실험과 현장연구를 통해 얻은 다양한 종류의 데이터를 논리에 맞게 해석하는 능력을 함양할 수 있도록 할 예정이다. 국산 시뮬레이션 소프트웨어인 &#39;에드슨(EDISON)&#39;과 빅 데이터 분석에 최적화된 &#39;R&#39;을 활용해 수강생들이 학습내용을 현장에서 쉽게 적용할 수 있도록 할 계획이다. 김 교수는 “4차 산업혁명 시대에서 빅데이터 분석과 최신 과학기술을 활용한 실용적 연구 능력을 갖추는 것은 더 이상 선택이 아닌 필수”라며 “차세대 환경 리더를 양성하고, 더 나아가 대한민국의 빅 데이터 및 인공지능 분야 경쟁력 제고에 기여하겠다”고 밝혔다.&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
##  [9] &quot;포스코 광양제철소(소장 이시우)가 지난 27일 광양제철소본부 대강당에서 ‘2019년 상반기 빅데이터 경진대회’를 개최하고 우수 과제를 공유했다. 빅데이터 경진대회는 4차 산업혁명이라는 시대의 변화를 반영해 엔지니어들이 가진 조업 현장의 기술과 데이터 활용기법을 결합해 스마트팩토리 구현을 가속화하고자 2017년부터 개최해오고 있다. 이번 경진대회에 참여한 광양제철소 엔지니어들은 올해 1월부터 대회 참가를 위한 인원 선발과 과제 선정을 시작으로 사내 교육 콘텐츠를 통해 데이터 통계 및 분석 기초 등 빅데이터 관련 교육을 받았다. 또한, 과제 수행 중에는 사내 데이터 분석 전문가를 투입해 과제를 수행하는 엔지니어들이 빅데이터에 대해 체계적으로 학습하고 데이터 분석 능력을 향상시킬 수 있도록 지원했다. 이날 총 6명이 경연을 펼쳐 ‘두께 불량 및 통판성 향상을 위한 마무리 압연 온도 모델 개발’을 주제로 △효과성 △활용성 △발표력 전 부분에서 가장 높은 점수를 받은 열연부 김경수 대리가 최우수상의 영광을 안았다. 우수상은 압연설비부 임용호 대리와 제강부 윤선혁 대리, 장려상은 선강설비부 황민수 사원, 도금부 윤지선 사원, 발전부 문현익 대리가 차지했다. 이시우 광양제철소장은 강평을 통해 “오늘 발표한 내용들이 실제로 현장에 적용된다면 가까운 미래에 스마트 제철소가 완성될 날도 머지않은 것 같다”며 “광양제철소가 다가올 미래에 경쟁력을 확보할 수 있도록 데이터 분석 능력과 함께 조업 현장의 지식 습득을 위한 학습에도 힘써달라”고 당부했다. 한편, 광양제철소는 매년 상반기와 하반기 빅데이터 경진대회를 개최해오고 있으며 올해 하반기에는 빅데이터를 활용한 조업 모델이 실제 현장에 적용한 사례를 중심으로 경진대회를 개최할 계획이다. &quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             
## [10] &quot;KB금융그룹(회장 윤종규)이 역량별 맞춤교육, 연수과정 확대 등 인력 육성 프로그램을 통해 그룹 내 &#39;데이터 분석&#39; 역량을 강화하고 있다. KB금융그룹에 따르면 그룹 내 데이터 분석 초급자들은 각 계열사에 도입된 초급분석 교육과정과 &#39;그룹 데이터분석 CoP(Community of Practice)&#39;를 통해 분석의 기초를 다질 수 있다. 또 빅데이터분석 플랫폼과 시각화툴을 접하도록 해 관련분야에 직원들이 보다 친근하게 다가가게 했다. 중·고급 분석인력에는 연세대 정보대학원과 개설한 &#39;KB데이터분석 아카데미&#39;, 카이스트 전자공학과와 개설한 &#39;AI 인텐시브 코스&#39;를 들을 수 있다. 이러한 외부교육을 통해 KB금융그룹이 지난 3년간 육성한 데이터 분석 전문가는 총 150여명에 달한다.  KB금융그룹은 교육에 그치지 않고, 직원별 데이터 분석 역량에 기반해 인사이동을 실시하고, 프로젝트에 투입시키는 등 인력 운용도 하고 있다. 이는 가시적인 성과로 이어졌다. 대표 사례가 위 교육과정을 수료한 직원이 참여해 개발한 KBotSAM(케이봇쌤)이다. 케이봇쌤은 전세계 수많은 금융 빅데이터를 AI가 매일 분석, 학습해 최적 포트폴리오를 제안하는 머신러닝 알고리즘 기반 로보어드바이저 자산관리서비스다. 또 KB금융그룹은 빅데이터를 활용해 모든 계열사가 신용평가모델을 고도화, 머신러닝과 강화학습 기반 신용평가모형도 개발중이다. 금융사기 방지를 위해서 딥러닝 기반 FDS시스템은 24시간 가동되고 있으며, 성능 향상을 위해 지속적으로 시스템도 고도화하고 있다. KB금융 관계자는 “내부직원 육성을 통해 역량 강화를 도모하고 기술을 내재화해 KB만의 차별적인 AI와 빅데이터 서비스를 통해 초격차 리딩 금융그룹을 만들어 갈 것”이라고 밝혔다. KB금융그룹은 인공지능 기반으로 더욱 정교하고 개인화된 고객 마케팅, 최적 금융상품 추천, 중고차 시세 예측, 아파트 가격 추정 모델 등을 개발해 조만간 서비스를 선보일 예정이다.&quot;</code></pre>
</details>
<div class="sourceCode" id="cb624"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb624-1"><a href="비정형-데이터마이닝.html#cb624-1" aria-hidden="true" tabindex="-1"></a>news.corpus<span class="ot">&lt;-</span><span class="fu">VCorpus</span>(<span class="fu">VectorSource</span>(news))</span>
<span id="cb624-2"><a href="비정형-데이터마이닝.html#cb624-2" aria-hidden="true" tabindex="-1"></a>news.corpus[[<span class="dv">1</span>]]<span class="sc">$</span>content</span></code></pre></div>
<pre><code>## [1] &quot;동아대학교(총장 한석정)가 &#39;수요자 데이터기반 스마트헬스케어 서비스&#39;분야 ‘4차 산업혁명 혁신선도대학으로 최종선정됐습니다. 동아대가 혁신선도대학으로 펼치게 될 ‘수요자 데이터기반 스마트헬스케어 서비스’ 산업은 리빙데이터(운동·영양·약물)와 메디컬데이터(생체계측·진료기록)를 종합 분석, 다양한 헬스케어 서비스를 제공하는 것입니다. 동아대는 건강과학대학과 의료원, 재활요양병원 등 경쟁력 있는 인프라를 바탕으로 신뢰도 높은 정밀 분석을 실시, ‘헬스케어 기획 전문가’와 ‘헬스케어 데이터분석 전문가’ 등 수요자 맞춤형 헬스케어 서비스 분야를 선도하는 전문 인재를 키워나갈 계획입니다. ‘스마트헬스케어 융합전공’을 신설, 경영정보학과를 중심으로 한 빅데이터 분석, 식품영양학과·의약생명공학과·건강관리학과 중심의 헬스케어 등 학문 간 경계는 교육혁신도 이뤄나갈 방침입니다. &quot;</code></pre>
<ul>
<li>예제 텍스트 파일을 readLines 함수로 news라는 변수에 저장한다. 텍스트 데이터를 VectorSource함수를 통해 문서를 만들고, VCorpus 함수로 Corpus 형태로 변환한다.</li>
<li>Corpus로 변환된 데이터는 리스트 형태이고 $content를 통해 내용을 확인할 수 있다. Corpus 데이터를 전처리하기 위해 사용자 지정함수를 제작하여 데이터 전처리를 수행한다.</li>
</ul>
<div class="sourceCode" id="cb626"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb626-1"><a href="비정형-데이터마이닝.html#cb626-1" aria-hidden="true" tabindex="-1"></a>clean_txt<span class="ot">&lt;-</span><span class="cf">function</span>(txt){</span>
<span id="cb626-2"><a href="비정형-데이터마이닝.html#cb626-2" aria-hidden="true" tabindex="-1"></a>  txt<span class="ot">&lt;-</span><span class="fu">tm_map</span>(txt, removeNumbers)</span>
<span id="cb626-3"><a href="비정형-데이터마이닝.html#cb626-3" aria-hidden="true" tabindex="-1"></a>  txt<span class="ot">&lt;-</span><span class="fu">tm_map</span>(txt, removePunctuation)</span>
<span id="cb626-4"><a href="비정형-데이터마이닝.html#cb626-4" aria-hidden="true" tabindex="-1"></a>  txt<span class="ot">&lt;-</span><span class="fu">tm_map</span>(txt, stripWhitespace)</span>
<span id="cb626-5"><a href="비정형-데이터마이닝.html#cb626-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(txt)</span>
<span id="cb626-6"><a href="비정형-데이터마이닝.html#cb626-6" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<ul>
<li>tm_map 함수를 통해 숫자 제거, 문장부호 제거, 공백 제거를 진행하고 txt에 데이터를 저장한다.</li>
</ul>
<div class="sourceCode" id="cb627"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb627-1"><a href="비정형-데이터마이닝.html#cb627-1" aria-hidden="true" tabindex="-1"></a>clean.news<span class="ot">&lt;-</span><span class="fu">clean_txt</span>(news.corpus)</span>
<span id="cb627-2"><a href="비정형-데이터마이닝.html#cb627-2" aria-hidden="true" tabindex="-1"></a>clean.news[[<span class="dv">1</span>]]<span class="sc">$</span>content</span></code></pre></div>
<pre><code>## [1] &quot;동아대학교총장 한석정가 수요자 데이터기반 스마트헬스케어 서비스분야 ‘차 산업혁명 혁신선도대학으로 최종선정됐습니다 동아대가 혁신선도대학으로 펼치게 될 ‘수요자 데이터기반 스마트헬스케어 서비스’ 산업은 리빙데이터운동·영양·약물와 메디컬데이터생체계측·진료기록를 종합 분석 다양한 헬스케어 서비스를 제공하는 것입니다 동아대는 건강과학대학과 의료원 재활요양병원 등 경쟁력 있는 인프라를 바탕으로 신뢰도 높은 정밀 분석을 실시 ‘헬스케어 기획 전문가’와 ‘헬스케어 데이터분석 전문가’ 등 수요자 맞춤형 헬스케어 서비스 분야를 선도하는 전문 인재를 키워나갈 계획입니다 ‘스마트헬스케어 융합전공’을 신설 경영정보학과를 중심으로 한 빅데이터 분석 식품영양학과·의약생명공학과·건강관리학과 중심의 헬스케어 등 학문 간 경계는 교육혁신도 이뤄나갈 방침입니다 &quot;</code></pre>
<div class="sourceCode" id="cb629"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb629-1"><a href="비정형-데이터마이닝.html#cb629-1" aria-hidden="true" tabindex="-1"></a>txt2<span class="ot">&lt;-</span><span class="fu">gsub</span>(<span class="st">&quot;[[:punct:]]&quot;</span>, <span class="st">&quot;&quot;</span>, clean.news[[<span class="dv">1</span>]])</span></code></pre></div>
<ul>
<li>전처리 결과에서 숫자와 구두점 등을 제거했으나, ’,. 와 같은 부호는 제거되지 않아 gsub 함수를 통해 제거할 수 있다. gsub에서 [[:punct:]]와 같은 용어를 통해 전체 대체가 가능하다.</li>
</ul>
</div>
</div>
<div id="자연어-처리" class="section level4" number="5.1.1.2">
<h4><span class="header-section-number">5.1.1.2</span> 자연어 처리</h4>
<ul>
<li>자연어 처리는 기본적으로 형태소 분석을 하는 과정을 포함하고 있다. 문장의 품사를 구분하여 분석에 필요한 품사만 추출하여 활용할 수 있다.</li>
<li>R에서 한글 자연어 분석을 하기 위해 KoNLP 패키지를 이용한다. 25개의 함수가 들어 있으며, 형태소 분석 등의 자연어 처리 및 텍스트 마이닝을 수행할 수 있다.</li>
</ul>
<div id="konlp-패키지를-활용한-한글처리" class="section level5" number="5.1.1.2.1">
<h5><span class="header-section-number">5.1.1.2.1</span> KoNLP 패키지를 활용한 한글처리</h5>
<p><b>함수 사용법</b></p>
<pre><code>buildDictionary(ext_dic, data)</code></pre>
<table>
<thead>
<tr class="header">
<th>인자</th>
<th>설명</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ext_dic</td>
<td>단어를 추가하고자 하는 사전을 선택. “woorimalsam”, “sejong”, “insighter”이 있음.</td>
</tr>
<tr class="even">
<td>data</td>
<td>추가하고자 하는 단어와 품사가 들어간 data frame 또는 txt 파일</td>
</tr>
</tbody>
</table>
<pre><code>extraNoun(text)</code></pre>
<table>
<thead>
<tr class="header">
<th>인자</th>
<th>설명</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>text</td>
<td>명사를 추출하고자 하는 문장 또는 문서</td>
</tr>
</tbody>
</table>
<pre><code>SimplePos22(text)</code></pre>
<table>
<thead>
<tr class="header">
<th>인자</th>
<th>설명</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>text</td>
<td>형태소 분석을 하고자 하는 문장 또는 문서</td>
</tr>
</tbody>
</table>
<p><strong>Q. 간단한 문장으로 명사추출, 사전 단어추가, 품사를 확인해 보자.</strong></p>
<div class="sourceCode" id="cb633"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb633-1"><a href="비정형-데이터마이닝.html#cb633-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="fu">setdiff</span>(<span class="st">&quot;KoNLP&quot;</span>, <span class="fu">rownames</span>(<span class="fu">installed.packages</span>())))</span>
<span id="cb633-2"><a href="비정형-데이터마이닝.html#cb633-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(KoNLP)</span>
<span id="cb633-3"><a href="비정형-데이터마이닝.html#cb633-3" aria-hidden="true" tabindex="-1"></a><span class="fu">useSejongDic</span>()</span></code></pre></div>
<ul>
<li>KoNLP 라이브러리를 활성화하고 useSejongDic() 함수를 실행하여 사용하고자 하는 사전을 설정한다.</li>
</ul>
<div class="sourceCode" id="cb634"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb634-1"><a href="비정형-데이터마이닝.html#cb634-1" aria-hidden="true" tabindex="-1"></a>setence<span class="ot">&lt;-</span><span class="st">&#39;아버지가 방에 스르륵 들어가신다&#39;</span></span>
<span id="cb634-2"><a href="비정형-데이터마이닝.html#cb634-2" aria-hidden="true" tabindex="-1"></a><span class="fu">extractNoun</span>(sentence)</span>
<span id="cb634-3"><a href="비정형-데이터마이닝.html#cb634-3" aria-hidden="true" tabindex="-1"></a><span class="fu">buildDictionary</span>(<span class="at">ext_dic=</span><span class="st">&quot;sejong&quot;</span>, <span class="at">user_dic=</span><span class="fu">data.frame</span>(<span class="fu">c</span>(<span class="st">&#39;스르륵&#39;</span>), <span class="fu">c</span>(<span class="st">&#39;mag&#39;</span>)))</span>
<span id="cb634-4"><a href="비정형-데이터마이닝.html#cb634-4" aria-hidden="true" tabindex="-1"></a><span class="fu">extractNoun</span>(sentence)</span></code></pre></div>
<ul>
<li>명사를 추출하기 위해 예제 문장을 setence라는 데이터에 저장하고 extractNoun 함수로 명사를 추출했다. 결과에서 ’스르륵’은 명사가 아니라 부사인데, ’스르륵’이라는 단어가 세종사전에 포함되어 있지 않으므로 ’스르륵’을 부사로 세종사전에 추가한다.</li>
<li>사전을 추가하고 다시 extractNoun함수를 사용해 결과를 확인하면 ’스르륵’이 제외된 것을 확인할 수 있다.</li>
</ul>
<div class="sourceCode" id="cb635"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb635-1"><a href="비정형-데이터마이닝.html#cb635-1" aria-hidden="true" tabindex="-1"></a><span class="fu">SimplePos22</span>(sentence)</span></code></pre></div>
<ul>
<li>SimplePos22 함수로 sentence의 문장을 형태소 분석을 하여 분리된 단어마다 품사를 확인할 수 있으며, NC은 명사, PV는 동사, PA는 형용사를 의미한다.</li>
</ul>
<p><strong>Q. 위의 new 데이터에서 corpus로 변환하지 않고 전처리 및 명사추출, 사전추가, 품사확인을 하고 형용사를 추출해 보자.</strong></p>
<div class="sourceCode" id="cb636"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb636-1"><a href="비정형-데이터마이닝.html#cb636-1" aria-hidden="true" tabindex="-1"></a>clean_txt2<span class="ot">&lt;-</span><span class="cf">function</span>(txt) {</span>
<span id="cb636-2"><a href="비정형-데이터마이닝.html#cb636-2" aria-hidden="true" tabindex="-1"></a>  txt<span class="ot">&lt;-</span><span class="fu">removeNumbers</span>(txt)</span>
<span id="cb636-3"><a href="비정형-데이터마이닝.html#cb636-3" aria-hidden="true" tabindex="-1"></a>  txt<span class="ot">&lt;-</span><span class="fu">removePunctuation</span>(txt)</span>
<span id="cb636-4"><a href="비정형-데이터마이닝.html#cb636-4" aria-hidden="true" tabindex="-1"></a>  txt<span class="ot">&lt;-</span><span class="fu">stripWhitespace</span>(txt)</span>
<span id="cb636-5"><a href="비정형-데이터마이닝.html#cb636-5" aria-hidden="true" tabindex="-1"></a>  txt<span class="ot">&lt;-</span><span class="fu">gsub</span>(<span class="st">&quot;[^[:alnum:]]&quot;</span>,<span class="st">&quot; &quot;</span>,txt)</span>
<span id="cb636-6"><a href="비정형-데이터마이닝.html#cb636-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(txt)</span>
<span id="cb636-7"><a href="비정형-데이터마이닝.html#cb636-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb636-8"><a href="비정형-데이터마이닝.html#cb636-8" aria-hidden="true" tabindex="-1"></a>clean.news2<span class="ot">&lt;-</span><span class="fu">clean_txt2</span>(news)</span>
<span id="cb636-9"><a href="비정형-데이터마이닝.html#cb636-9" aria-hidden="true" tabindex="-1"></a>Noun.news[<span class="dv">5</span>]</span></code></pre></div>
<ul>
<li>tm패키지의 tm_map 함수의 인자로 사용되는 FUN을 그대로 함수로 적용하여 사용이 가능하다. 사용자 정의함수에 숫자, 문장부호, 공백 제거 함수를 사용했고 gsub 함수를 활용해 영문자/숫자를 제외한 것들을 제거하는 전처리를 한다.</li>
<li>전처리를 마친 데이터를 Corpus로 반환하지 않고 데이터를 확인했을 때, ‘푸드테크, ’스타트업’ 등과 같은 복합명사가 분리되어 출력되는 것을 확인할 수 있다.</li>
</ul>
<div class="sourceCode" id="cb637"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb637-1"><a href="비정형-데이터마이닝.html#cb637-1" aria-hidden="true" tabindex="-1"></a><span class="fu">buildDictionary</span>(<span class="at">ext_dic=</span><span class="st">&quot;sejong&quot;</span>, <span class="at">user_dic=</span><span class="fu">data.frame</span>(<span class="fu">c</span>(<span class="fu">read.table</span>(<span class="st">&quot;food.txt&quot;</span>))))</span>
<span id="cb637-2"><a href="비정형-데이터마이닝.html#cb637-2" aria-hidden="true" tabindex="-1"></a><span class="fu">extractNoun</span>(clean.news2[<span class="dv">5</span>])</span></code></pre></div>
<ul>
<li>복합명사를 명사로 인식할 수 있도록 사전에 등록하고 다시 분석 결과를 확인하면 복합명사도 하나의 명사로 추가된 것으로 확인할 수 있다.</li>
<li>단어사전은 txt파일 형태로도 추가가 가능하며 형태는 ‘단어’, ’품사’로 저장하여 추가할 수 있다.</li>
</ul>
<div class="sourceCode" id="cb638"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb638-1"><a href="비정형-데이터마이닝.html#cb638-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;stringr&quot;</span>)</span>
<span id="cb638-2"><a href="비정형-데이터마이닝.html#cb638-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stringr)</span>
<span id="cb638-3"><a href="비정형-데이터마이닝.html#cb638-3" aria-hidden="true" tabindex="-1"></a>doc1<span class="ot">&lt;-</span><span class="fu">paste</span>(<span class="fu">SimplePos22</span>(clean.news2[[<span class="dv">2</span>]]))</span>
<span id="cb638-4"><a href="비정형-데이터마이닝.html#cb638-4" aria-hidden="true" tabindex="-1"></a>doc1</span></code></pre></div>
<ul>
<li>stringr 패키지는 R에서 문자열을 처리할 수 있는 패키지로, str_match 함수로 문자열 중 특정 부분이 해당하는 데이터를 선별할 수 있다.</li>
<li>SimplePos22 함수를 실행한 결과가 리스트 형태로 나타나므로, 이를 paste 함수를 사용해 character형 벡터로 변형하여 doc1에 저장한다. str_match 함수를 활용해 품사 중 PA(형용사)인 데이터만 뽑아낸다.</li>
</ul>
<div class="sourceCode" id="cb639"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb639-1"><a href="비정형-데이터마이닝.html#cb639-1" aria-hidden="true" tabindex="-1"></a>doc2<span class="ot">&lt;-</span><span class="fu">str_match</span>(doc1, <span class="st">&quot;([가-힣]+)/PA&quot;</span>)</span>
<span id="cb639-2"><a href="비정형-데이터마이닝.html#cb639-2" aria-hidden="true" tabindex="-1"></a>doc2</span>
<span id="cb639-3"><a href="비정형-데이터마이닝.html#cb639-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb639-4"><a href="비정형-데이터마이닝.html#cb639-4" aria-hidden="true" tabindex="-1"></a>doc3<span class="ot">&lt;-</span>doc2[,<span class="dv">2</span>]</span>
<span id="cb639-5"><a href="비정형-데이터마이닝.html#cb639-5" aria-hidden="true" tabindex="-1"></a>doc3[<span class="sc">!</span><span class="fu">is.na</span>(doc3)]</span></code></pre></div>
<ul>
<li>doc2 데이터에서 1열은 PA를 포함한 단어가 있는 열이며, 2열은 PA를 제외한 단어만 있는 열이 생성되고 PA가 없는 행은 NA로 채워진다. doc2의 2열의 데이터를 doc3에 저장하고 is.na 함수로 NA를 제외한 데이터만 추출한다.</li>
</ul>
</div>
<div id="stemming" class="section level5" number="5.1.1.2.2">
<h5><span class="header-section-number">5.1.1.2.2</span> Stemming</h5>
<ul>
<li>어간 추출(Stemming)은 형태학적 분석을 단순화한 버전이라고 할 수 있으며, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고 할 수 있다. 즉 공통 어간을 가지는 단어를 묶는 작업을 Stemming이라고 한다.</li>
<li>R프로그램에서는 tm패키지에서 stemDocument() 함수를 통해 공통으로 들어가지 않은 부분을 제외하고 stemCompletion() 함수를 통해 stemming된 단어와 완성을 위한 dictionary를 함께 넣으면 가장 기본적인 어휘로 완성해주는 역할을 한다.</li>
</ul>
<pre><code>stemDocument(text)</code></pre>
<pre><code>stemCompletion(text, dictionary)</code></pre>
<p><strong>Q. analyze, analyzed, analyzing 단어의 어간을 추출하고 가장 기본단어로 만들어 보자.</strong></p>
<div class="sourceCode" id="cb642"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb642-1"><a href="비정형-데이터마이닝.html#cb642-1" aria-hidden="true" tabindex="-1"></a>test<span class="ot">&lt;-</span><span class="fu">stemDocument</span>(<span class="fu">c</span>(<span class="st">&#39;analyze&#39;</span>, <span class="st">&#39;analyzed&#39;</span>, <span class="st">&#39;analyzing&#39;</span>))</span>
<span id="cb642-2"><a href="비정형-데이터마이닝.html#cb642-2" aria-hidden="true" tabindex="-1"></a>test</span>
<span id="cb642-3"><a href="비정형-데이터마이닝.html#cb642-3" aria-hidden="true" tabindex="-1"></a>completion<span class="ot">&lt;-</span><span class="fu">stemCompletion</span>(test, <span class="at">dictionary=</span><span class="fu">c</span>(<span class="st">&#39;analyze&#39;</span>, <span class="st">&#39;analyzed&#39;</span>, <span class="st">&#39;analyzing&#39;</span>))</span>
<span id="cb642-4"><a href="비정형-데이터마이닝.html#cb642-4" aria-hidden="true" tabindex="-1"></a>completion</span></code></pre></div>
<ul>
<li>stemDocument 함수를 통해 앞 어간을 제외한 나머지 부분을 잘려 나가게 만들어 각 단어가 서로 다르지만 사실 모두 analyz-라는 어간을 가지므로 위와 같이 도출된다.</li>
<li>stemCompletion 함수를 통해 analyz로 stemming 되었던 단어들이 dictionary에 포함된 단어중 가장 기본 어휘로 완성된 것을 확인할 수 있다. 가장 중요한 것은 stemCompletion을 할 때는 단어의 완성을 위해 반드시 dictionary가 필요하다.</li>
</ul>
</div>
</div>
</div>
<div id="term-document-matrix" class="section level3" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Term-Document Matrix</h3>
<ul>
<li>앞선 과정을 통해 읽어 들인 문서의 빈 공간을 제거하고, 대문자를 소문자로 변환, 문장부호 제거, 불용어 처리 등의 과정을 수행했다. 이렇게 전처리된 데이터에서 각 문서와 단어 간의 사용 여부를 이용해 만들어진 matrix가 바로 TDM(Term-Document Matrix)이다.</li>
<li>TDM을 보면 문서마다 등장한 단어의 빈도수를 쉽게 파악할 수 있다는 장점이 있다.</li>
</ul>
<div id="r을-활용한-tdm-구축하기" class="section level4" number="5.1.2.1">
<h4><span class="header-section-number">5.1.2.1</span> R을 활용한 TDM 구축하기</h4>
<p><b>함수사용법</b></p>
<pre><code>TermDocumentMatrix(data, control)</code></pre>
<table>
<thead>
<tr class="header">
<th>인자</th>
<th>설명</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>data</td>
<td>Corpus 형태의 데이터</td>
</tr>
<tr class="even">
<td>control</td>
<td>사전 변경, 가중치 부여 등의 옵션 추가기능 지원</td>
</tr>
</tbody>
</table>
<p><strong>Q. 앞서 전처리가 완료된 clean.news2 데이터를 Vcorpus로 변환하여 TDM을 생성해 보자.</strong></p>
<div class="sourceCode" id="cb644"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb644-1"><a href="비정형-데이터마이닝.html#cb644-1" aria-hidden="true" tabindex="-1"></a>VC.news<span class="ot">&lt;-</span><span class="fu">VCorpus</span>(<span class="fu">VectorSource</span>(clean.news2))</span>
<span id="cb644-2"><a href="비정형-데이터마이닝.html#cb644-2" aria-hidden="true" tabindex="-1"></a>VC.news[[<span class="dv">1</span>]]<span class="sc">$</span>content</span>
<span id="cb644-3"><a href="비정형-데이터마이닝.html#cb644-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb644-4"><a href="비정형-데이터마이닝.html#cb644-4" aria-hidden="true" tabindex="-1"></a>TDM.news<span class="ot">&lt;-</span><span class="fu">TermDocumentMatrix</span>(VC.news)</span>
<span id="cb644-5"><a href="비정형-데이터마이닝.html#cb644-5" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(TDM.news)</span>
<span id="cb644-6"><a href="비정형-데이터마이닝.html#cb644-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb644-7"><a href="비정형-데이터마이닝.html#cb644-7" aria-hidden="true" tabindex="-1"></a><span class="fu">inspect</span>(TDM.news[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, ])</span></code></pre></div>
<ul>
<li>전처리가 완료된 clean.news2 데이터를 VC.news에 Corpus 형태로 저장하고 VC.news 데이터를 TermDocumentMatrix() 함수를 활용하여 TDM을 구축하였다.</li>
<li>dim 함수를 통해 10개의 기사에서 1011개의 단어가 추출되었다는 것을 확인할 수 있으며, inspect 함수로 TDM 구축 결과를 확인할 수 있다.</li>
<li>TDM 결과를 확인하면 10개의 문서에서 1~5번째 단어의 분포를 확인할 수 있다. 여기서 ’academy는’이 4번 문서에서 1번 사용됐음을 확인할 수 있다. 대부분의 단어가 모든 문서에서 이용되지 않기 때문에 조회한 내용의 10개 문서와 5개 단어에 대해 사용된 단어는 0 이상의 숫자로 빈도를 나타내고 사용되지 않은 단어는 0으로 표시된다.</li>
<li>sparsity는 전체 행렬에서 0이 차지하는 비중을 의미하므로, 45/50로 90%에 해당한다.</li>
</ul>
<div class="sourceCode" id="cb645"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb645-1"><a href="비정형-데이터마이닝.html#cb645-1" aria-hidden="true" tabindex="-1"></a>words<span class="ot">&lt;-</span><span class="cf">function</span>(doc) {</span>
<span id="cb645-2"><a href="비정형-데이터마이닝.html#cb645-2" aria-hidden="true" tabindex="-1"></a>  doc<span class="ot">&lt;-</span><span class="fu">as.character</span>(doc)</span>
<span id="cb645-3"><a href="비정형-데이터마이닝.html#cb645-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extractNoun</span>(doc)</span>
<span id="cb645-4"><a href="비정형-데이터마이닝.html#cb645-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb645-5"><a href="비정형-데이터마이닝.html#cb645-5" aria-hidden="true" tabindex="-1"></a>TDM.news2<span class="ot">&lt;-</span><span class="fu">TermDocumentMatrix</span>(VC.news, <span class="at">control=</span><span class="fu">list</span>(<span class="at">tokenize=</span>words))</span>
<span id="cb645-6"><a href="비정형-데이터마이닝.html#cb645-6" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(TDM.news2)</span>
<span id="cb645-7"><a href="비정형-데이터마이닝.html#cb645-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb645-8"><a href="비정형-데이터마이닝.html#cb645-8" aria-hidden="true" tabindex="-1"></a>tdm2<span class="ot">&lt;-</span><span class="fu">as.matrix</span>(TDM.news2)</span>
<span id="cb645-9"><a href="비정형-데이터마이닝.html#cb645-9" aria-hidden="true" tabindex="-1"></a>tdm3<span class="ot">&lt;-</span><span class="fu">rowSums</span>(tdm2)</span>
<span id="cb645-10"><a href="비정형-데이터마이닝.html#cb645-10" aria-hidden="true" tabindex="-1"></a>tdm4<span class="ot">&lt;-</span>tdm3[<span class="fu">order</span>(tdm3, <span class="at">decreasing=</span><span class="cn">TRUE</span>)]</span>
<span id="cb645-11"><a href="비정형-데이터마이닝.html#cb645-11" aria-hidden="true" tabindex="-1"></a>tdm4[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span></code></pre></div>
<ul>
<li>‘academy는’과 같이 명사 뒤에 조사가 붙는 경우가 있다. extractNoun 함수를 통해 명사만 추출하여 TDM을 다시 구축하면 위와 같은 결과가 나타나게 된다. 모든 문서의 단어 빈도를 분석하여 상위 10개를 추출하면 ’데이터’, ‘빅데이터’ 등 순서로 단어의 빈도를 확인할 수 있다.</li>
</ul>
<p><strong>Q. 단어 사전을 정의하여 해당 단어들에 대해서만 분석 결과를 확인해 보자.</strong></p>
<div class="sourceCode" id="cb646"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb646-1"><a href="비정형-데이터마이닝.html#cb646-1" aria-hidden="true" tabindex="-1"></a>mydict<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="st">&quot;빅데이터&quot;</span>, <span class="st">&quot;스마트&quot;</span>, <span class="st">&quot;산업혁명&quot;</span>, <span class="st">&quot;인공지능&quot;</span>, <span class="st">&quot;사물인터넷&quot;</span>, <span class="st">&quot;AI&quot;</span>, <span class="st">&quot;스타트업&quot;</span>, <span class="st">&quot;머신러닝&quot;</span>)</span>
<span id="cb646-2"><a href="비정형-데이터마이닝.html#cb646-2" aria-hidden="true" tabindex="-1"></a>my.news<span class="ot">&lt;-</span><span class="fu">TermDocumentMatrix</span>(VC.news, <span class="at">control=</span><span class="fu">list</span>(<span class="at">tokenize=</span>words, <span class="at">dictionary=</span>mydict))</span>
<span id="cb646-3"><a href="비정형-데이터마이닝.html#cb646-3" aria-hidden="true" tabindex="-1"></a><span class="fu">inspect</span>(my.news)</span></code></pre></div>
<ul>
<li>빅데이터와 관련된 단어를 mydict에 저장하여 TermDocumentMatrix함수의 control 인자에 적용하여 해당 단어들에 대해서만 분석 결과를 확인할 수 있다.</li>
</ul>
</div>
<div id="tdm을-활용한-분석-및-시각화" class="section level4" number="5.1.2.2">
<h4><span class="header-section-number">5.1.2.2</span> TDM을 활용한 분석 및 시각화</h4>
<div id="연관성-분석" class="section level5" number="5.1.2.2.1">
<h5><span class="header-section-number">5.1.2.2.1</span> 연관성 분석</h5>
<ul>
<li>작성된 TDM에서 특정 단어와의 연관성에 따라 단어를 조회할 수 있다. findAssocs 함수를 통해 TDM과 연관된 단어와의 연관성이 일정 수치 이상인 단어들만 표시할 수 있다.</li>
</ul>
<p><b>함수사용법</b></p>
<pre><code>findAssocs(data, terms, corlimit)</code></pre>
<p>|인자|설명|
|data|TDM 형태의 데이터|
|terms|연관성을 확인할 단어|
|corlimit|최소 연관성|</p>
<p><strong>Q. VC.news 데이터를 명사만 추출하는 TDM으로 변경하여 TDM에서 ’빅데이터’라는 단어와의 연관성이 0.9 이상인 단어들만 추출해 보자.</strong></p>
<div class="sourceCode" id="cb648"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb648-1"><a href="비정형-데이터마이닝.html#cb648-1" aria-hidden="true" tabindex="-1"></a>words<span class="ot">&lt;-</span><span class="cf">function</span>(doc) {</span>
<span id="cb648-2"><a href="비정형-데이터마이닝.html#cb648-2" aria-hidden="true" tabindex="-1"></a>  doc<span class="ot">&lt;-</span><span class="fu">as.character</span>(doc)</span>
<span id="cb648-3"><a href="비정형-데이터마이닝.html#cb648-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">extractNoun</span>(doc)</span>
<span id="cb648-4"><a href="비정형-데이터마이닝.html#cb648-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb648-5"><a href="비정형-데이터마이닝.html#cb648-5" aria-hidden="true" tabindex="-1"></a>TDM.news2<span class="ot">&lt;-</span><span class="fu">TermDocumentMatrix</span>(VC.news, <span class="at">control=</span><span class="fu">list</span>(<span class="at">tokenize=</span>words))</span>
<span id="cb648-6"><a href="비정형-데이터마이닝.html#cb648-6" aria-hidden="true" tabindex="-1"></a><span class="fu">findAssocs</span>(TDM.news2, <span class="st">&#39;빅데이터&#39;</span>, <span class="fl">0.9</span>)</span></code></pre></div>
<ul>
<li>구축된 TDM과 ‘빅데이터’라는 단어와의 연관성을 파악한 결과, ’가맹점’, ‘개발자’ 등의 단어들이 연관된 단어로 나타나며, 연관성에 대한 수치도 해당 단어 아래에 같이 표시됨을 확인할 수 있다.</li>
</ul>
</div>
<div id="워드-클라우드" class="section level5" number="5.1.2.2.2">
<h5><span class="header-section-number">5.1.2.2.2</span> 워드 클라우드</h5>
<ul>
<li>문서에 포함되는 단어의 사용 빈도를 효과적으로 보여주기 위한 막대그래프 등의 시각화 도구가 있지만, 워드 클라우드를 이용하면 더 효과적으로 표시할 수 있다.</li>
</ul>
<p><b>함수사용법</b></p>
<pre><code>wordcloud(words, freq, min.freq, random.order, colors, ...)</code></pre>
<p>|인자|설명|
|words|워드클라우드를 만들고자하는 단어|
|freq|단어의 빈도|
|min.freq|시각화하려는 단어의 최소 빈도|
|random.order|단어의 배치를 랜덤으로 할지 정함. F일때, 빈도순으로 그려짐.|
|colors|빈도에 따라 단어의 색을 지정|</p>
<p><strong>Q. TDM.news2 데이터를 워드 클라우드로 만들어보자.</strong></p>
<div class="sourceCode" id="cb650"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb650-1"><a href="비정형-데이터마이닝.html#cb650-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(wordcloud)</span>
<span id="cb650-2"><a href="비정형-데이터마이닝.html#cb650-2" aria-hidden="true" tabindex="-1"></a>tdm2<span class="ot">&lt;-</span><span class="fu">as.matrix</span>(TDM.news2)</span>
<span id="cb650-3"><a href="비정형-데이터마이닝.html#cb650-3" aria-hidden="true" tabindex="-1"></a>term.freq<span class="ot">&lt;-</span><span class="fu">sort</span>(<span class="fu">rowSums</span>(tdm2), <span class="at">decreasing=</span><span class="cn">TRUE</span>)</span>
<span id="cb650-4"><a href="비정형-데이터마이닝.html#cb650-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(term.freq, <span class="dv">15</span>)</span>
<span id="cb650-5"><a href="비정형-데이터마이닝.html#cb650-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb650-6"><a href="비정형-데이터마이닝.html#cb650-6" aria-hidden="true" tabindex="-1"></a><span class="fu">wordcloud</span>(<span class="at">words=</span><span class="fu">names</span>(term.freq), <span class="at">freq=</span>term.freq, <span class="at">min.freq=</span><span class="dv">5</span>, </span>
<span id="cb650-7"><a href="비정형-데이터마이닝.html#cb650-7" aria-hidden="true" tabindex="-1"></a>          <span class="at">random.order=</span><span class="cn">FALSE</span>, <span class="at">colors=</span><span class="fu">brewer.pal</span>(<span class="dv">8</span>, <span class="st">&#39;Dark2&#39;</span>))</span></code></pre></div>
<ul>
<li>TDM을 Matrix 형태로 변환하여 행 결합을 통해 각 단어마다 빈도를 합쳐 내림차순으로 정렬하여 term.freq 데이터에 저장하고 head 함수를 통해 확인이 가능하다.</li>
<li>wordcloud 함수로 term.freq 데이터에서 단어만 가져오고, 빈도는 term.freq의 빈도로 지정하고, 최소 빈도 5로 지정하여 워드클라우드를 그린다.</li>
</ul>

</div>
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="정형-데이터마이닝.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsubsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
