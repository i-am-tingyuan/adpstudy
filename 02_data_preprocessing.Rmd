---
title: "ADPStudy"
author: "tingyuan"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
output:
  bookdown::gitbook: default
  #bookdown::pdf_book: default
---
# 데이터 전처리

## 제어문

## 데이터 변환

### 파생변수 생성

### 변수 축소

#### 주성분분석

##### 개념

- 주성분분석이란 데이터에 여러 변수들이 있을 때 서로 상관성이 높은 변수들의 선형결합으로 이루어진 '주성분' 이라는 새로운 변수를 만들어 변수들을 요약하고 축소하는 기법이다.
- 예를 들어, 변수 x와 z로 y를 예측하고자 할 때 x=3a, z=a+1, y=2x+z와 같은 관계가 성립된다면 굳이 x와 z라는 두 변수를 사용하지 않고, 변수 a로만 y를 예측하는 것이 더 좋을 것이다. 이와 같이 여러 변수의 선형조합으로 만들어진 주성분을 통해 변수들을 축소할 수 있다. 
- 주성분분석을 할 때, 첫 번째 주성분으로 전체 변동을 가장 많이 설명할 수 있도록 하고, 두 번째 주성분으로는 첫 번째 주성분이 설명하지 못하는 나머지 변동을 정보의 손실 없이 가장 많이 설명할 수 있도록 변수들의 선형조합을 만든다. 각 주성분은 서로 독립인 것(상관계수 = 0)을 원칙으로 한다. 

##### 목적

- 소수의 주성분으로 차원을 축소함. 
- 다중공선성이 존재하는 경우, 상관성이 없는 (적은) 주성분으로 변수들을 축소
- 주성분분석을 통해 변수 차원을 축소한 후 군집분석을 수행하면 군집화 결과와 연산속도를 개선할 수 있다. 

##### 주성분의 선택

(1) 기여율
  - '주성분 기여율'을 사용하여 주성분이 데이터를 얼마나 잘 설명하는지 평가함. 
  - 주성분 기여율 : 원변수의 총변동 (각 변수들의 분산값 총합) 분의 주성분 변수의 분산으로, 총변동에 대한 주성분의 설명력을 의미한다. 
  - 기여율은 1에 가까울수록 적절하고 0에 가까울수록 데이터에 대한 설명력이 떨어진다고 판단한다. 
  - 첫번째 주성분부터 차례대로 기여율을 합한 누적 기여율(cumulative proportion)이 85% 이상이 되면 해당 지점까지를 주성분의 수로 결정한다. 아래의 사진에서는 두 번째 주성분까지의 누적 기여율이 약 87%이므로, 주성분의 수를 두 개로 결정한다. 

(2) 스크리 산점도 (Scree Plot) 
  - 주성분을 x축, 각 주성분의 고유값(주성분의 분산)을 y축에 둔 그래프이다. 고유치가 급격히 완만해지는 지점의 바로 전 단계로 주성분의 수를 선택한다. 

##### R을 이용한 주성분 분석

```{}
prcomp(data, center=TRUE, scale.=FALSE, ...)
```

```{}
princomp(data, cor=FALSE, scores=TRUE, ...)
```

__Q. R에 내장된 USArrests 데이터는 1973년 미국 50개주 100,000명의 인구 당 체포된 세가지 강력범죄수(assault, murder, rape)와 각 주마다 도시에 거주하는 인구의 비율(%)로 구성되어 있다. 주성분 분석을 수행하여 해당 데이터이ㅡ 변수들을 가장 잘 요약하는 주성분을 구하고 해석해 보자.__  

- USArrests 데이터는 변수들 간의 척도 차이가 상당히 크기 때문에 상관행렬을 사용하여 분석한다. 
- 특이치 분해를 사용하는 경우 자료 행렬 각 변수의 평균과 제곱의 합이 1로 표준화되었다고 가정할 수 있다. 

__(1) 데이터 확인 및 산점도를 통한 변수 간 상관관계 파악__

```{r}
library(datasets)
data(USArrests)
head(USArrests)
```

```{r}
pairs(USArrests, panel=panel.smooth, main="USArrests data")
```

__(2) 주성분분석 수행__

```{r}
US.prin <- princomp(USArrests, cor=TRUE)
summary(US.prin)
```
- 주성분 분석 결과에 summary함수를 적용하면 결과에 대한 요약 설명이 나온다. summary 결과로 나오는 Standard deviation은 주성분의 표준편차, Proportion of Variance는 주성분의 기여율, Cumulative Proportion은 누적기여율을 의미한다. 
- 제1주성분과 제2주성분까지의 누적 기여율은 대략 86.8%로 2개의 주성분변수를 활용하여 전체 데이터의 약 86.8%를 설명할 수 있다. 

```{r}
plot(US.prin, type='l')
```
- 주성분들에 의해 설명되는 변동의 비율은 Screeplot을 통해 시각적으로도 확인이 가능하다. 그래프의 3번재 주성분에서 기울기가 급격하게 줄어드는 형태를 보이므로, 그 이전 주성분인 2번째 주성분까지 선택하는 것이 적절하다. 

__(3) Loading__

```{r}
US.prin$loadings
```
- 주성분분석 결과의 loading을 통해 주성분계수 즉, 네 개의 변수가 각 주성분에 기여하는 가중치가 제시된다. 제 1주성분은 0.536 * Murder + 0.583 * Assault + 0.278 * UrbanPop + 0.543 * Rape의 선형결합식으로 이루어져 있음을 파악할 수 있다. 

__(4) Scores__

주성분분석 결과, 차원축소로 얻어지는 주성분점수는 scores 인자를 통해 확인할 수 있다. 주성분 점수는 주성분들의 선형식을 통해 새롭게 계산된 각 행별 좌표를 나타낸다. 

```{r}
head(US.prin$scores)
```

__(5) 제 1-2 주성분에 의한 행렬도__

```{r}
biplot(US.prin, scale=0)
```

- biplot 함수는 제1주성분과 제2주성분으로 이루어진 좌표평면상에 원데이터 행들의 주성분점수를 산점도의 형태로 나타내고, 각 변수에 대한 주성분계수를 화살표로 시각화하여 그래프로 표현해 준다. 
- 제1주성분의 모든 주성분계수는 양수이므로 가로축(PC1)을 기준으로 모든 변수가 0 이상의 값을 가리키는 축을 나타내고 있다. PC1을 이루는 선형 결합식에서 상대적 부하량의 절대값이 가장 큰 Assault 변수는 가장 수평의 형태를 나타내고 있으며, 상대적 부하량의 절대값이 가장 작은 UrbanPop 변수는 가장 수직에 가까운 형태를 나타내고 있다. 

#### 요인분석

##### 개념

- 여러개의 변수들로 이루어진 데이터에서 변수들 간의 상관관계를 고려하여 서로 유사한 변수들을 묶어 새로운 잠재요인들을 추출해내는 분석방법으로, 변수를 축소하고 데이터를 요약하는데 사용
- 예를 들어, 시험성적에 대한 데이터가 '국어, 영어, 중국어, 수학, 물리, 음악, 미술'에 해당하는 7개의 변수로 이루어져 있다고 하자. 이 7개가 아닌 공통의 변수들을 파악해 국어, 영어, 중국어를 (언어능력), 수락, 물리를 (수리능력), 음악, 미술을 (예술적 재능) 등과 같이 새로운 요인들로 구성해낼 때 요인분석을 사용한다. 
- 요인분석을 수행하기 위해서는 변수가 간격척도 혹은 비율척도로 측정되어야 하며, 표본(관측치)의 크기는 100개 이상이 바람직하며 최소 50개 이상이 되어야 한다. 

##### 주성분분석 .vs. 요인분석

|  | 주성분분석| 요인분석 |
|--| :- | :- |
|공통점 | * 원데이터를 활용하여 몇개의 새로운 변수를 생성<br />* 변수축소 및 데이터 요약에 사용됨||
|생성되는 변수의 수 | 통상적으로 2개 | 지정된 개수 없음 |
|생성되는 변수의 이름 | 제1, 2주성분과 같이 표현됨 | 분석가가 변수의 이름을 지정함 |
|생성되는 변수들의 관계 | 제1주성분이 가장중요. 2주성분이 그다음 | 대등한 관계 |
|분석방법의 의미 | 목표변수를 잘 예측/분류하기 위해 기존 변수들의 선형결합으로 이루어진 몇 개의 주성분을 찾아냄 | 목표변수를 고려하지 않고 주어진 변수들을 비슷한 성격으로 묶어서 새로운 (잠재)변수를 생성 |

##### 요인추출방법

- __주성분분석__ : 변수들로부터 요인을 추출하는 방식으로, 전체분산을 토대로 요인을 추출. 가장 많이 사용
- __공통요인분석__ : 잠재요인으로부터 변수들이 산출된 것으로 보는 방식, 공통분산만을 토대로 요인을 추출. 

##### 요인의 수 결정

- 고유값을 기준으로 할 때는, 고유값이 1이상인 요인들을 추출한다. 
- 스크리 도표에서 요인의 설명력이 하락하다가 완만한 하락으로 추세가 바뀌기 직전의 요인수를 기준으로 요인을 추출한다. 
- 경우에 따라 추출할 요인의 수를 사전에 정의한 후 요인분석을 수행할 수도 있다. 

##### R을 이용한 요인분석

- R에서 요인추출법으로 주성분분석을 사용할 때는 __prcomp__ 혹은 __principal__ 함수를 활용하며, 요인추출법으로 공통요인분석을 사용할 때는 __factanal__ 함수를 사용한다. 

```
factanal(data, factors=n, rotation="varimax", scores="regression", ...)
```
| 인자 | 설명 |
|---:|---|
|data|요인분석을 수행할 숫자형 행렬 혹은 데이터프레임|
|factors|요인의 개수 지정|
|rotation|요인 회전방법을 선택 ("varimax", "promax", "none"이 있음)|
|scores|요인점수 계산방법을 선택 ("regression", "Bartlett"가 있음)|

__Q. R의 내장 데이터 swiss는 1888년 경 스위스 내 47개주의 사회 경제적 지표(교육, 농업 종사자 비율 등)와 출산율에 대한 데이터이다. 원활한 분석을 위해 먼저 해당 데이터의 6가지 변수들을 min-max 정규화한 뒤 (2장 3절 표준화와 정규화 참고), 요인분석을 실시하여 변수들을 3개의 요인으로 축소해 보자. (factanal 함수 사용)__

__(1) swiss 데이터 확인__

```{r}
data(swiss)
str(swiss)
```

__(2) 정규화 수행 및 실습용 데이터 생성__

```{r}
Min <- apply(swiss, 2, min)
Max <- apply(swiss, 2, max)
swiss_fa <- scale(swiss, center=Min, scale=(Max-Min))
head(swiss_fa)
```

__(3) 요인분석 수행__

```{r}
factanal(x = swiss_fa, factors=3)
```
- 요인분석 결과의 Proportion Var는 각 요인이 설명하는 분산의 비율이며 Cumulative Var는 요인별 해당값의 누적치이다. 세번째 요인에 대한 Cumulative Var 값이 0.759이므로 세 요인은 전체 데이터 분산의 약 76%를 설명할 수 있다고 해석이 가능한다. 

### 표준화와 정규화

#### 표준화 (standardization)
- 각 개체들이 평균을 기준으로 얼마나 떨어져 있는지를 나타내는 값으로 변환하는 과정을 의미하여, 표준화한 후 특정 범위를 벗어난 데이터를 확인하여 이상치 판별에 활용할 수도 있다. 
- Z-Score 표준화는 각 요소의 값에서 평균을 뺀 후 표준편차로 나누어 수행한다. 변환 후 데이터의 평균은 0, 표준편차는 1의 값을 갖게 된다. scale 함수 혹은 사용자 정의 함수를 이용하여 R에서 구현할 수 있다. 

```{}
scale(data, center=TRUE, scale=TRUE)
```

| 인자|설명 |
|---|---|
|data|숫자형벡터|
|center|TRUE이면 데이터에서 해당 벡터의 평균을 뺌|

__Q. R의 내장 데이터 'mtcars'의 mpg(마일)변수와 hp(총 마력)변수로만 이루어진 데이터프레임(test, cars)을 생성하고, 각 변수를 표준화한 새로운 변수를 추가해 보자. (mpg를 표준화한 변수의 이름은 mpg_scale, hp를 표준화한 변수의 이름은 hp_scale로 지정)__

```{r}
data("mtcars")
str(mtcars)
test.cars <- mtcars[,c("mpg", "hp")]
head(test.cars)
test.cars <- transform(test.cars, mpg_scale=scale(test.cars$mpg), hp_scale=scale(test.cars$hp))
head(test.cars)
```

#### 정규화 (Normalization)

- 정규화란 데이터의 범위를 0과 1사이로 변환하여 데이터의 분포를 조정하는 방법으로, 데이터군 내에서 특정 개체가 가지는 위치를 파악하고 비교할 때 유용하게 사용할 수 있다. 
- 일반적으로 많이 사용되는 min-max 정규화는 '(x~i~ - min~x~)/(max~i~-min~x~)'의 공식을 이용하며, scale함수 혹은 사용자 정의 함수를 이용하는 등 다양한 방법을 사용할 수 있다. 

#### scale 함수 이용

```{r}
Min <- min(iris$Sepal.Length)
Max <- max(iris$Sepal.Length)
iris$SL_new <- scale(iris$Sepal.Length, center=Min, scale=Max-Min)
head(iris)
```

## 데이터 결합 및 요약
### 데이터 결합
#### rbind
#### cbind
#### merge
- merge는 두 데이터프레임에서 기준이 되는 특정 칼럼의 값이 같은 행끼리 묶어 병합하는 함수이다. 이는 데이터베이스에서 join과 같은 역할을 한다. 
```{}
merge(x, y, by, by.x, by.y, all=FALSE, all.x,)
```

```{r}
(id_name <- data.frame(id=c("c01", "c02", "c03", "c04", "c05", "c06", "c07"), 
                      last_name=c("Lee", "Kim", "Choi", "park", "Lim", "Bae", "Kim")))
(id_number <- data.frame(id=c("c03", "c04", "c05", "c06", "c07", "c08", "c09"), 
                        number=c(3, 1, 0, 7, 3, 4, 1)))
```

__Q-1. Inner Join__
```{r}
merge(id_name, id_number, by="id")
```
__Q-2. Outer Join__
```{r}
merge(id_name, id_number, by="id", all=TRUE)
```
__Q-3. Left Outer Join__
```{r}
merge(id_name, id_number, by="id", all.x=TRUE)
```
__Q-4. Right Outer Join__
```{r}
merge(id_name, id_number, by="id", all.y=TRUE)
```

### 데이터 요약
#### aggregate
```{}
aggregate(x, by, FUN)
aggregate(formula, data, FUN)
```

__Q1. iris 데이터에서 종별 Sepal.width의 평균을 구해보자.__

```{r}
aggregate(Sepal.Width~Species, iris, mean)
```

__Q2. iris 데이터에서 종별 Sepal.Width와 Petal.Width의 평균을 구해보자.__

```{r}
aggregate(cbind(Sepal.Width, Petal.Width)~Species, iris, mean)
```


#### table
__Q1. 내장 데이터 Titanic은 타이타닉호 탑승자들의 특성에 따른 생존여부를 기록해놓은 데이터이다. Titanic 데이터에서 좌석등급을 의미하는 Class 변수에 대해서 도수분포표를 생성해 보자.__

__내장데이터 Titanic의 구조 확인__
```{r}
str(Titanic)
```

__데이터프레임으로 변환한 뒤 다시 구조를 확인__
```{r}
Titanic<-as.data.frame(Titanic)
str(Titanic)
```

__table 함수를 이용하여 범주형 변수 Class에 대한 도수분포표를 생성__
```{r}
table(Titanic$Class)
```
__Q2. 내장데이터 Titanic에서 Survived 변수는 승객의 생존여부를 의미한다. 좌석등급과 생존여부의 관계를 살펴보기 위해 Class 변수에 따른 Survived 변수의 도수를 표 형태로 나타내 보자. __

```{r}
table(Titanic$Class, Titanic$Survived)
```


#### prop.table
```{}
prop.table(table)
prop.table(table, 1)
prop.table(table, 2)
```
__Q. Titanic 데이터에서 Age 변수는 해당 승객이 어른인지 아이인지의 여부를 나타낸다. Age 변수에 따른 생존여부의 관계를 전체에 대한 비율, 행별 비율, 열별 비율로 살펴보자.__

```{r}
prop.table(table(Titanic$Age, Titanic$Survived))
prop.table(table(Titanic$Age, Titanic$Survived), 1)
prop.table(table(Titanic$Age, Titanic$Survived), 2)
```


### apply 계열 함수
#### apply

apply는 데이터의 행 혹은 열 방향으로 주어진 함수를 한번에 적용한 뒤 그 결과를 벡터, 배열, 리스트로 반환하는 함수이다. 

```{}
apply(X, MARGIN, FUN)
```

```{r}
a <- matrix(1:12, nrow=4, ncol=3)
apply(a, 1, max)
```

```{r}
apply(iris[, 1:4], 2, mean)
```

#### lapply
```{}
lapply(X, FUN, ...)
```

```{r}
a <- c(1, 2, 3)
lapply(a, FUN=function(x){x ^ 2})
class(lapply(a, FUN=function(x){x^2}))
b<- lapply(a, FUN=function(x){x^2})
unlist(b)
```

#### sapply
```{}
sapply(X, FUN, simplify=TRUE, ...)
```

__Q1. iris 데이터에서 각 칼럼별 데이터 타입을 구해보자. __
```{r}
sapply(iris, class)
```

__Q2. iris 데이터에서 각 칼럼에 summary 함수를 적용해 보자.__
```{r}
sapply(iris, summary)
```

#### vapply
```{}
vapply(X, FUN, FUN.VALUE, ...)
```

__Q. 1~100까지의 숫자가 저장된 리스트에 fivenum 함수를 적용한 후, 각 값에 이름을 부여하여 리스트 형태로 출력해 보자.__
```{r}
test <- c(1:100)
fivenum(test)
test <- list(test)
(test2 <- vapply(test, fivenum, c("Min"=0, "Q1"=0, "Median"=0, "Q3"=0, "Max"=0)))
```


#### mapply
```{}
mapply(FUN, arg1, arg2, ..., argn, ...)
```

__Q. 1을 4번, 2를 3번, 3을 2번, 4를 1번 반복하는 4개의 수열을 구해보자. 이 때 rep 함수를 이용할 때와 mapply 함수를 이용 할 때를 비교해 보자.__
```{r}
mapply(rep, c(1:4), c(4:1))
```


#### tapply
tapply 함수를 이용하면 데이터를 특정 기준에 따라 그룹으로 나눈 뒤 각 그룹별로 함수를 적용하여 그 결과를 반환할 수 있다. 

```{}
tapply(DATA, INDEX, FUN, ...)
```

__Q1. R의 googleVis 패키지에 있는 Fruits 데이터에서 과일종류별 판매량의 평균을 구해보자.__

```{r}
install.packages(setdiff("googleVis", rownames(installed.packages())))
library(googleVis)
head(Fruits)
tapply(Fruits$Sales, Fruits$Fruit, mean)
```

__Q2. Fruits 데이터에서 Location이 West인 것과 아닌 것으로 그룹을 지정하여 Profit의 평균을 구해보자.__

```{r}
tapply(Fruits$Profit, Fruits$Location=="West", mean)
```

## 패키지를 활용한 데이터 전처리
### plyr
#### plyr 패키지
plyr 패키지의 함수들은 데이터를 분할(split)한 뒤 원하는 방향으로 특정 함수를 적용하고 (apply), 그 결과를 재조합 (combine) 하여 반환해 준다. 

#### adply
```{}
adply(data, margins, fun)
```
__Q. R의 iris 데이터에서 Petal.Length 변수가 1.5 미만이면서 Species 변수 값이 'setosa'인 조건을 만족하는 경우 '1'을 그렇지 않은 경우 '0'을 부여한 칼럼을 생성하여, 원래의 iris 데이터와 함께 데이터프레임 형태로 출력해 보자. __

```{r}
install.packages(setdiff("plyr", rownames(installed.packages())))
library(plyr)
head(adply(iris, 1, 
      function(row) {ifelse(row$Petal.Length < 1.5 & row$Species == "setosa", "1", "0")}))
```

#### ddply
```{}
ddply(data, .variables, ddply-func, fun)
```

__Q1. iris 데이터에서 Species별로 나머지 네 개 변수의 평균을 출력해 보자. __
```{r}
ddply(iris, .(Species), function(sub) {
  data.frame(
    mean_SL = mean(sub$Sepal.Length), mean_SW = mean(sub$Sepal.Width), 
    mean_PL = mean(sub$Petal.Length), mean_PW = mean(sub$Petal.Width)
  )
})
```

__Q2. iris 데이터에서 Species와 Petal.Length가 1.5 미만인지 여부로 데이터를 그룹지어 네 개 변수의 평균을 출력해 보자. __

```{r}
ddply(iris, .(Species, Petal.Length<1.5), function(sub) {
  data.frame( 
    mean_SL = mean(sub$Sepal.Length), mean_SW = mean(sub$Sepal.Width), 
    mean_PL = mean(sub$Petal.Length), mean_PW = mean(sub$Petal.Width))
})
```

##### transform
```{r}
head(ddply(baseball, .(id), transform, avgG=sum(g)/length(year)))
```

##### mutate
```{r}
# avgG 칼럼과 avgG_RND 칼럼을 한번에 추가하여 출력. 
# 이 경우, mutate가 아닌 tansform을 사용하면 에러가 발생함. 
head(ddply(baseball, .(id), mutate, avgG=sum(g)/length(year), avgG_RND=round(avgG)))
```

##### summarise
```{r}
head(ddply(baseball, .(id), summarise, year_fin=max(year)))
head(ddply(baseball, .(team), summarise, hr_sum=sum(hr)))
```

### dplyr
#### dplyr 패키지
#### filter
```
dataframe name %>% filter(condition)
```

__Q. Cars93 데이터에서 제조사가 "Audi" 혹은 "BMW"이면서, 엔진크기가 2.4 이상인 행들만 추출해 보자.__

```{r}
install.packages(setdiff("dplyr", rownames(installed.packages())))
library(dplyr)
library(MASS)
Cars93 %>% filter((Manufacturer=="Audi"|Manufacturer=="BMW") & EngineSize >= 2.4)
```

#### select

__Q1. Cars93 데이터의 모델번호, 종류, 가격 변수들만 추출해 보자.__

```{r}
# Cars93 %>% select(Model, Type, Price)
# Error in select(., Model, Type, Price) : unused arguments (Model, Type, Price) Calls: <Anomymous> ...
# withCallingHandlers -> withVisible -> eval -> eval -> %>% Execution halted
# 에러 발생 이유 : MASS 패키지의 select()와 dplyr의 select()가 충돌하기 때문

head(Cars93 %>% dplyr::select(Model, Type, Price))
```


__Q2. 제조사가 "Chevrolet" 혹은 "Volkswagen" 이면서, 가격이 10 이상인 행들의 제조사, 모델, 종류, 가격 변수들만 추출해 보자.__

```{r}
Cars93 %>%
  filter((Manufacturer=="Chevrolet"|Manufacturer=="Volkswagen") & Price >= 10) %>%
  dplyr::select(Manufacturer, Model, Type, Price)
```

#### group_by 와 summarise

__Q1. Cars93 데이터의 제조사별 가격의 평균과 무게의 최댓값을 산출한 뒤 변수명을 각각 mean_Price, max_Weight로 지정하여 출력해 보자.__

```{r}
Cars93 %>% group_by(Manufacturer) %>% 
  summarise(mean_Price=mean(Price), max_Weight=max(Weight))
```

__Q2. 종류와 에어백을 기준으로 데이터를 그룹화한 뒤, 자동차 평균 무게를 구해 보자.__

```{r}
Cars93 %>% group_by(Type, AirBags) %>% summarise(mean_Weight=mean(Weight))
```

#### mutate

mutate는 데이터에 새로운 파생변수를 추가해 주는 함수이며, 사용법은 아래와 같다. 

__Q1. Cars93 데이터에서 가격이 12미만이면 "low", 12 이상 23 미만이면 "middle", 23 이상이면 "high" 값을 가지는 Pr_level 변수를 생성한 뒤, 모델, 가격, 새로운 파생변수 Pr_level만 출력해 보자.__

```{r}
head(
Cars93 %>% 
  mutate(Pr_level=ifelse(Price < 12, "low", ifelse(Price >= 12 & Price < 23, "middle", "high"))) %>%
  dplyr::select(Model, Price, Pr_level)
)
```

#### arrange

arrange는 특정열 기준으로 데이터를 정렬해주는 함수이며, 사용법은 아래와 같다. 

__Q. Cars93 데이터에서 종류가 "Midsize" 혹은 "Small"인 데이터의 Model, Type, Weight, Price 변수들만 추출한 뒤, 종류별로 Weight 변수값들이 Weight의 중앙값보다 작은 경우는 "low", 중앙값 이상인 경우 "high" 값을 갖는 Weight_lv 변수를 생성하라. 그리고 Price 변수를 기준으로 데이터를 오름차순으로 정렬하여 출력하라.__

```{r}
Cars93 %>%
  filter(Type %in% c("Midsize", "Small")) %>%
  dplyr::select(Model, Type, Weight, Price) %>%
  group_by(Type) %>%
  mutate(Weight_lv = ifelse(Weight < median(Weight), "low", "high")) %>%
  arrange(Price)
```

#### {left, right, inner, full}_join

__Q. 카페에서 판매하는 메뉴 코드, 이름을 담은 데이터 'NAME'과 메뉴코드 해당 메뉴의 가격을 담은 데이터 'PRICE'를 생성해보자. 그 후 각 메뉴의 고유코드를 의미하는 code 변수를 기준으로 left, right, inner, full_join을 수행하여 결과를 확인해 보자.__

```{r}
NAME<-data.frame(code=c("A01", "A02", "A03"), name=c("coffee", "cake", "cookie"))
PRICE<-data.frame(code=c("A01", "A02", "A04"), price=c(3000, 4000, 3000))
```
```{r}
(cafe_left <- left_join(NAME, PRICE, by="code"))
(cafe_right <- right_join(NAME, PRICE, by="code"))
(cafe_inner <- inner_join(NAME, PRICE, by="code"))
(cafe_full <- full_join(NAME, PRICE, by="code"))

```

#### bind_rows과 bind_cols
```{r}
# rbind(NAME, PRICE)
# 변수명이 다르므로 에러가 발생함
bind_rows(NAME,PRICE)
```
### reshape2
#### reshape2 패키지
#### melt
__Q. R의 airquality는 1973년 5월~9월 동안 뉴욕이ㅡ 일일 대기 질 측정량에 대한 데이터로, 153개의 행과 6개의 변수로 이루어져 있다. 6개의 변수 중 Month와 Day을 식별자로 두고, 나머지 변수와 변수값은 모두 데이터 내에 포함되는 형태로 변환해 보자.__
```{r}
install.packages(setdiff("reshape2", rownames(installed.packages())))
library(reshape2)
head(airquality)
head(melt(airquality, id.vars=c("Month", "Day"), na.rm=TRUE))
```
#### dcast
```{r}
air_melt<-melt(airquality, id.vars=c("Month", "Day"), na.rm=TRUE)
head(air_melt)
air_dcast<-dcast(air_melt, Month + Day ~ ...)
head(air_dcast)
```

### data.table
#### data.table 패키지
#### 데이터 테이블 생성
__Q1. __
```{r}
install.packages(setdiff("data.table", rownames(installed.packages())))
library(data.table)

(mydata<-data.table(x=c(1:3), y=c("가", "나", "다")))
class(mydata)
```
#### 데이터 접근
```{r}
iris_dt<-as.data.table(iris)
iris_dt[,mean(Petal.Length), by=Species]

# Petal.Length값이 1이상인 행들을 Species로 그룹화한 뒤, 
# Sepal.Length와 Sepal.Width의 평균을 각각 mean.SL과 mean.SW를 변수명으로 하여 출력
iris_dt[Petal.Length>=1, .(mean.SL=mean(Sepal.Length), mean.SW=mean(Sepal.Width)), by=Species]
```
#### 새로운 변수 생성
```{}
#하나의 변수 추가
데이터테이블[행, 새로운 칼럼명:=값, by="그룹화 기준 변수"]

#여러개의 변수 추가
데이터테이블[행, c("칼럼명1", ..., "칼럼명n"):=list(값1, ..., 값n), by="그룹화 기준 변수"]
```

```{r}
air<-as.data.table(airquality)
air[, Wind_class:=ifelse(Wind>=mean(Wind), "U", "D")]
head(air)
```

#### 데이터 정렬
```{r}
air[, season:=ifelse(Month %in% c(12,1,2), "winter", ifelse(Month %in% c(3:5), "spring", ifelse(Month %in% c(6:8), "summer", "fall")))]
head(air)
air[, .(Ozone_mean=mean(Ozone, na.rm=TRUE), Solar.R_mean=mean(Solar.R, na.rm=TRUE)), 
    by=.(season)][order(Ozone_mean, decreasing=TRUE)]
```
#### key를 활용하여 데이터 다루기

```{r}
baseball<-as.data.table(baseball)
setkey(baseball, year)
baseball[J(1960)]
```

#### key를 활용한 데이터 병합

## 결측치
### 결측치 인식
#### is.na(x)
#### complete.cases(x)
__Q1. airquality 데이터의 Ozone 변수에 대한 na값 존재 여부를 파악하고, 만약 na가 존재한다면 그 개수를 확인해 보자.__
```{r}
is.na(airquality$Ozone)
sum(is.na(airquality$Ozone))
table(is.na(airquality$Ozone))
```

__Q2. apply 함수는 행 혹은 열별로 함수를 적용하여 한 번에 결과를 산출해 주는 함수이다. apply 함수와 사용자 정의 함수를 활용하여 airquality 데이터의 모든 변수에 대해 각각 결측치(na값)가 몇개씩 존재하는지 확인해 보자.__
```{r}
apply(airquality, 2, function(x) sum(is.na(x)))
```

__Q3. complete.case 함수를 이용하여 airquality 데이터에서 na값이 하나라도 존재하는 행들을 air_na 변수에 저장하고, na값을 하나도 가지지 않는 행들을 air_com 변수에 저장하여라.__

```{r}
# na값이 하나라도 존재하는 행들을 air_na 변수에 저장
# complete.cases 함수를 적용했을 때 FALSE를 반환하는 행들만 저장하면 됨.
air_na <- airquality[!complete.cases(airquality),]
head(air_na)

# na값이 하나도 없는 행들은 air_com 변수에 저장
# complete.cases 함수를 적용했을 때 TRUE를 반환하는 행들만 저장하면 됨.
air_com <- airquality[complete.cases(airquality),]
head(air_com)

```

### 결측치 처리
#### 단순 대치법 (Single Imputation)

| 분류|설명 |
|---|---|
|completes analysis|결측값이 존재하는 행을 삭제|
|평균 대치법|관측 또는 실험을 통해 얻어진 데이터의 평균으로 결측치를 대치 (비조건부/조건부 평균대치법)|
|단순확률 대치법|평균대치법에서 추정량 표준 오차이ㅡ 과소 추정문제를 보완하고자 고안된 방법으로 Hot-deck 방법, nearest neightbor 방법 등이 있다.|
| | |

##### 결측치 제거

```{}
# 결측치가 존재하는 행 제거
데이터명[!is.na(데이터명)]
데이터명[complete.cases(데이터명),]
데이터명 %>% filter(!is.na(데이터명))

# na.omit 함수 활용
na.omit(데이터명)
```

##### 평균 대치법
__Q. airquality의 Ozone 변수값이 존재하지 않는 경우, Ozone 변수값들의 평균으로 대치해 보자.__
```{r}
airquality$Ozone<-ifelse(is.na(airquality$Ozone), mean(airquality$Ozone, na.rm=TRUE), airquality$Ozone)
table(is.na(airquality$Ozone))
```

#### 다중 대치법 (Multiple Imputation)
#### 패키지 활용
R에서 결측치를 대치하기 위해 사용될 수 있는 함수에는 DMwR 패키지의 함수들이 있다. 
__Q. DMwR의 함수들을 이용하여 NA값을 해당 변수의 중위수로 대치하는 전처리를 수행해 보자. NA값이 대치되는 ㅗ가정을 확인하기 위해 airquality 데이터에서 NA값을 하나라도 가지고 있는 행번호들을 미리 봅아놓고, 전처리 전의 원본 데이터와 전처리 후의 데이터를 비교하여 전처리가 잘 진행되었는지 확인해 보자.__
```{r}
install.packages(setdiff("DMwR2", rownames(installed.packages())))
library(DMwR2)

#airquality에서 na값을 가진 행들의 번호 추출
na_idx<-which(!complete.cases(airquality))

air_before<-airquality

#na값을 제거한 데이터를 air_after에 저장(centralImputation 함수 활용)
air_after<-centralImputation(airquality)

# 두 데이터에서 na_idx에 저장된 행번호에 해당하는 데이터들을 추출하여 na값이 잘 대체되었는지 비교
head(air_before[na_idx,])

head(air_after[na_idx,])

median(airquality$Ozone, na.rm=TRUE)

median(airquality$Solar.R, na.rm=TRUE)
```



## 이상치 인식
### 이상치란?
### 사분위수
### boxplot을 활용한 이상치 판별
__Q1. 내장데이터 airquality의 Ozone 변수에 대한 boxplot을 그려보자. 또한 이를 OzoneBP 이라는 변수에 저장하여 lower whisker와 upper whisker 밖에 있는 이상치가 존재하는지를 확인해 보자.__

```{r}
(OzoneBP<-boxplot(airquality$Ozone))
```

데이터는 전체적으로 작은 값들에 많이 분포한 것을 확인할 수 있으며, upper whisker 위로 15개의 이상치 값 ($out)이 있음을 알 수 있다. 

__Q2. lower whisker(Q1-1.5*IQR) 보다 작거나 upper whisker(Q3+1.5*IQR) 보다 큰값들을 이상치로 간주하고, 해당 값들이 저장된 행번호를 각각 upperOutlier, lowerOutlier 변수에 저장하자. 그리고 해당 행을 출력하여 데이터를 확인해 보자.__

```{r}
(LowerQ<-fivenum(airquality$Ozone)[2])
(UpperQ<-fivenum(airquality$Ozone)[4])
(IQR<-IQR(airquality$Ozone, na.rm=TRUE))

# 조건을 만족하는 행번호 추출
(upperOutlier<-which(airquality$Ozone > UpperQ+IQR*1.5))

# 조건을 만족하는 행번호 추출
(lowerOutlier<-which(airquality$Ozone < UpperQ-IQR*1.5))
airquality[upperOutlier, "Ozone"]
airquality[lowerOutlier, "Ozone"]
```

## 날짜 데이터 전처리
### 날짜 데이터 다루기
#### R의 날짜 데이터 형식

| 분류|설명 |
|---|---|
| Date|날짜만을 나타내는 클래스 이며, 내부적으로 1970년 1월1일 이후 경과된 날 수를 저장함. |
|POSIXct|날짜와 시간을 나타내며, 일초 간격의 정확도로 시간을 표현함. </br >내부적으로 1970년 1월1일에서 경과된 초 수와 시간대를 저장함. |
|POSIXlt|날짜와 시간을 나타내며 1900년에서부터 경과된 연도, 월, 일, 시, 분, 초를 포함하는 9개의 정보를 리스트에 저장|

```{r}
(today<-Sys.Date())
class(today)
(time<-Sys.time())
class(time)
```

```{r}
# today 내부의 값 확인: 1970년 1월 1일 이후로 경과한 일 수를 의미함. 
unclass(today)

# time 내부의 값 확인: 1970년 1월 1일 00:00:00 이후로 경과한 초 수를 의미함.
unclass(time)

# unclass를 적용한 time값을 다시 원래 날짜 형식으로 변환하기
# unclass(time)은 1970년 1월 1일 이후로 경과한 초수를 의미하므로, 
# origin 인자값으로 '1970-01-01'을 지정
as.POSIXct(unclass(time), origin="1970-01-01")
```

#### 날짜 표시형식 변경

__Q1. Sys.time 함수를 이용해 현재 날짜와 시간을 now 변수에 저장한 후, 이를 "네자리 년도수-두자리 월-일 시:분:초"의 형태를 가진 문자형 데이터로 변환해 보자.__
```{r}
now<-Sys.time()
class(now)
format(now, "%Y-%m-%d %H:%M:%S")
class(format(now, "%y-%m-%d %H:%M:%S"))
```

__Q2. "20200101"이라는 문자열을 Date 형식으로 변환한 후, date 변수에 저장하여 class를 확인해 보자.__
```{r}
(date<-as.Date("20200101", format="%Y%m%d"))
class(date)
```

#### 날짜 데이터의 연산

```{r}
Sys.Date() + 100
as.Date("2021-01-01", format="%Y-%m-%d") + 365
```
